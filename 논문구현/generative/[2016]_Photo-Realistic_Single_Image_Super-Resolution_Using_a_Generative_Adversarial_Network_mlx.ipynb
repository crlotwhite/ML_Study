{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "7ea04f66-a4a0-4a4d-9911-1fa17fefa52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tqdm\n",
    "import mlx.data as dx\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f67f88-4b54-4b9a-9823-47cc252d22bd",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "dbf68b93-2f90-42f2-82e6-af5224a42b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    train_data_root: str = '/Users/noel/dataset/VOCdevKit/VOC2012/JPEGImages'\n",
    "    crop_size: int = 96\n",
    "    upscale_factor: int = 4\n",
    "    learning_rate: float = 0.001\n",
    "    batch_size: int = 64\n",
    "    num_epochs: int = 10\n",
    "    dropout_rate: float = 0.5\n",
    "    num_workers: int = 4\n",
    "\n",
    "hp = Hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "8adc3bab-4133-4e95-ad7b-26efc6c0dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_files(root: Path):\n",
    "    images = []\n",
    "    for ext in ['*.png', '*.jpg', '*.jpeg', '*.PNG', '*.JPG', '*.JPEG']:\n",
    "        images += list(root.rglob(ext))\n",
    "\n",
    "    return [{\"image\": str(img).encode(\"ascii\")} for img in images]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "0bc9b09b-8369-461a-b4f0-6cadbcaab61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    dx.buffer_from_vector(get_img_files(root=Path(hp.train_data_root)))\n",
    "    .to_stream()\n",
    "    .load_image('image')\n",
    "    .image_random_crop('image', hp.crop_size, hp.crop_size, 'hr_image')\n",
    "    .image_resize('image', hp.crop_size // hp.upscale_factor, hp.crop_size // hp.upscale_factor, 'lr_image')\n",
    "    .key_transform(\"hr_image\", lambda x: x.astype(\"float32\"))\n",
    "    .key_transform(\"lr_image\", lambda x: x.astype(\"float32\"))\n",
    "    .batch(hp.batch_size)\n",
    "    .prefetch(4, 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "073d9eea-7e63-4a7d-8e55-57a2e6873ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hr_image', 'image', 'lr_image'])"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(dataset).keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbb647a-0218-4b48-82b1-f00ddbaad1d4",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f979885f-16b5-402a-a315-320ed9ee18ce",
   "metadata": {},
   "source": [
    "![image.png](https://raw.githubusercontent.com/crlotwhite/ML_Study/main/%EB%85%BC%EB%AC%B8%EA%B5%AC%ED%98%84/generative/assets/Cap%202024-08-15%2001-58-41-395.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "e5b2b347-abc5-4055-bce1-50daedfd7c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=in_channels,\n",
    "                      kernel_size=3, padding=1),\n",
    "            nn.BatchNorm(num_features=in_channels),\n",
    "            nn.PReLU(),\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=in_channels,\n",
    "                      kernel_size=3, padding=1),\n",
    "            nn.BatchNorm(num_features=in_channels)\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.model(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "fd0e3b39-bd8a-462f-ac64-c96762acd378",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelShuffle(nn.Module):\n",
    "    def __init__(self, upscale_factor):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.upscale_factor = upscale_factor\n",
    "\n",
    "    def __call__(self, x):\n",
    "        batch, H_in, W_in, C_in = x.shape[:-3], x.shape[-3], x.shape[-2], x.shape[-1]\n",
    "        C_out = C_in // self.upscale_factor**2 \n",
    "        H_out = H_in * self.upscale_factor\n",
    "        W_out = W_in * self.upscale_factor\n",
    "        \n",
    "        x = mx.reshape(x, shape= (*batch, self.upscale_factor, self.upscale_factor, H_in, W_in, C_out))\n",
    "        x = mx.einsum('b c u v h w -> b c h u w v', x)\n",
    "        \n",
    "        return mx.reshape(x, shape=(*batch, H_out, W_out, C_out))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "a3a40fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 24, 24, 64)\n",
      "(64, 96, 96, 4)\n"
     ]
    }
   ],
   "source": [
    "# pixelshuffle test\n",
    "model = PixelShuffle(4)\n",
    "\n",
    "input_image = mx.random.randint(0, 1, shape=(64, 24, 24, 64))\n",
    "output_image = model(input_image)\n",
    "\n",
    "print(input_image.shape)\n",
    "print(output_image.shape) # expect: (1, 1, 12, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "6416dde9-18f4-421d-9e10-a26c3de2ec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 4, 9)\n",
      "(1, 12, 12, 1)\n"
     ]
    }
   ],
   "source": [
    "# pixelshuffle test\n",
    "upscale_factor = 3\n",
    "model = PixelShuffle(upscale_factor)\n",
    "\n",
    "input_image = mx.random.randint(0, 1, shape=(1, 4, 4, 9))\n",
    "output_image = model(input_image)\n",
    "\n",
    "print(input_image.shape)\n",
    "print(output_image.shape) # expect: (1, 12, 12, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "01bf4544-de96-474a-ad86-45aceca40bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, up_scale):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, \n",
    "                      out_channels=in_channels * up_scale**2,\n",
    "                      kernel_size=3, padding=1),\n",
    "            PixelShuffle(up_scale),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "d2cda70d-4d8b-419c-9559-8ccc49095edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, scale_factor):\n",
    "        super().__init__()\n",
    "\n",
    "        num_upsamples = int(math.log(scale_factor, 2))\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=9, padding=4),\n",
    "            nn.PReLU(),\n",
    "            *[ResidualBlock(64) for _ in range(5)],\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm(num_features=64),\n",
    "            *[UpsampleBlock(64, 2) for _ in range(num_upsamples)],\n",
    "            nn.Conv2d(in_channels=64, out_channels=3, kernel_size=9, padding=4),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return (self.model(x) + 1) / 2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "2de4c806-9e6d-4b98-92ae-a124f0d38b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
    "                      kernel_size=3, padding=1),\n",
    "            nn.BatchNorm(num_features=out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, \n",
    "                      kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm(num_features=out_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "f5d34214-4d44-4edb-89d2-1eaf89ffffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm(num_features=64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            *[ConvBlock(in_channels, out_channels) \n",
    "              for in_channels, out_channels in [(64, 128), (128, 256), (256, 512)]],\n",
    "            nn.AvgPool2d(kernel_size=12),\n",
    "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(in_channels=1024, out_channels=1, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        return nn.sigmoid(self.model(x).reshape(batch_size, -1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "04f463a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1, 1, 512)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = nn.AvgPool2d(kernel_size=12)\n",
    "x = mx.random.randint(0, 1, shape=(64, 12, 12, 512))# (64, 12, 12, 512)\n",
    "pool(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6c24ea-9e2c-42e1-9053-ef31059efe35",
   "metadata": {},
   "source": [
    "# Loss\n",
    "![](https://raw.githubusercontent.com/crlotwhite/ML_Study/main/%EB%85%BC%EB%AC%B8%EA%B5%AC%ED%98%84/generative/assets/Cap%202024-08-15%2002-45-29-962.png)\n",
    "![](https://raw.githubusercontent.com/crlotwhite/ML_Study/main/%EB%85%BC%EB%AC%B8%EA%B5%AC%ED%98%84/generative/assets/Cap%202024-08-15%2002-43-50-138.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "671f9353-3bf1-410e-b616-5784cc630cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TVLoss(nn.Module):\n",
    "    def __init__(self, tv_loss_weight=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tv_loss_weight = tv_loss_weight\n",
    "\n",
    "    def __call__(self, x):\n",
    "        batch_size, h_x, w_x = x.shape[0], x.shape[2], x.shape[3]\n",
    "        count_h = self.tensor_size(x[:, :, 1:, :])\n",
    "        count_w = self.tensor_size(x[:, :, :, 1:])\n",
    "        h_tv = mx.power((x[:, :, 1:, :] - x[:, :, :h_x - 1, :]), 2).sum()\n",
    "        w_tv = mx.power((x[:, :, :, 1:] - x[:, :, :, :w_x - 1]), 2).sum()\n",
    "        return self.tv_loss_weight * 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n",
    "\n",
    "    def tensor_size(self, t):\n",
    "        return t.shape[1] * t.shape[2] * t.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "58408d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py, requests, os\n",
    "\n",
    "VGG16_WEIGHTS_LINK = (\n",
    "    \"https://storage.googleapis.com/tensorflow/keras-applications/\"\n",
    "    \"vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\"\n",
    ")\n",
    "\n",
    "class VGG16(nn.Module):     \n",
    "    def __init__(self, weight_path=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(25088, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "        \n",
    "        if weight_path is not None:\n",
    "            self.load_weights()\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.classifier(self.model(x))\n",
    "        \n",
    "\n",
    "    def load_weights(self, weights_path=None) -> None:\n",
    "        if weights_path is None:\n",
    "            weights_path = 'resources/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "            if not os.path.exists(weights_path):\n",
    "                os.makedirs('resources', exist_ok=True)\n",
    "                r = requests.get(VGG16_WEIGHTS_LINK, allow_redirects=True)\n",
    "                open(weights_path, 'wb').write(r.content)\n",
    "                \n",
    "        weight = h5py.File(weights_path, 'r')\n",
    "        for key in weight.keys():\n",
    "            params = weight[key]\n",
    "            params_keys = list(params.keys())\n",
    "            try:\n",
    "                layer = getattr(self, key)\n",
    "                if len(layer.weight.shape) == 4:\n",
    "                    layer.weight = mx.array(params[params_keys[0]][...]).transpose((3, 0, 1, 2))\n",
    "                elif len(layer.weight.shape) == 2:\n",
    "                    layer.weight = mx.array(params[params_keys[0]][...]).transpose((1, 0))\n",
    "                layer.bias = mx.array(params[params_keys[1]][...])\n",
    "            except:\n",
    "                pass\n",
    "        weight.close()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "72335bf4-b3b0-4b32-b779-8de01f248cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        vgg = VGG16()\n",
    "        loss_net = vgg.model\n",
    "        loss_net.eval()\n",
    "        loss_net.freeze()\n",
    "        self.loss_net = loss_net\n",
    "        self.tv_loss = TVLoss()\n",
    "    \n",
    "    def __call__(self, D_out, fake_img, real_img):\n",
    "        content_loss = nn.losses.mse_loss(self.loss_net(fake_img), \n",
    "                                          self.loss_net(real_img))\n",
    "        adversarial_loss = mx.mean(1-D_out)\n",
    "        perceptual_loss = content_loss * 0.006 + adversarial_loss * 0.001\n",
    "        tv_loss = self.tv_loss(fake_img)\n",
    "        \n",
    "        return perceptual_loss + tv_loss * 2e-8\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "8cc33445",
   "metadata": {},
   "outputs": [],
   "source": [
    "mx.set_default_device(mx.gpu)\n",
    "\n",
    "G = Generator(4)\n",
    "mx.eval(G.parameters())\n",
    "D = Discriminator()\n",
    "mx.eval(D.parameters())\n",
    "\n",
    "G_loss = GeneratorLoss()\n",
    "\n",
    "G_opt = optim.Adam(0.001)\n",
    "D_opt = optim.Adam(0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "e91b727c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/17123 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                     | 32/17123 [02:51<25:23:21,  5.35s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[291], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m lr_img \u001b[38;5;241m=\u001b[39m mx\u001b[38;5;241m.\u001b[39marray(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr_image\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     36\u001b[0m loss_g, loss_d \u001b[38;5;241m=\u001b[39m step(lr_img, hr_img)\n\u001b[0;32m---> 37\u001b[0m \u001b[43mmx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m G_losses\u001b[38;5;241m.\u001b[39mappend(loss_g)\n\u001b[1;32m     40\u001b[0m D_losses\u001b[38;5;241m.\u001b[39mappend(loss_d)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def discriminator_loss(hr_img, fake_img):\n",
    "    return hr_img.mean() - 1 + fake_img.mean()\n",
    "\n",
    "def d_forward(D_model, hr_img, fake_img):\n",
    "    D_real = D_model(hr_img)\n",
    "    D_fake = D_model(fake_img) \n",
    "    loss = discriminator_loss(D_real, D_fake)\n",
    "    return loss, (D_real, D_fake)\n",
    "\n",
    "def g_forward(D_model, hr_img, fake_img):\n",
    "    D_out = D_model(fake_img).mean()\n",
    "    loss = G_loss(D_out, fake_img, hr_img)\n",
    "    return loss, D_out\n",
    "\n",
    "g_loss_and_grad_fn = nn.value_and_grad(G, g_forward)\n",
    "d_loss_and_grad_fn = nn.value_and_grad(D, d_forward)\n",
    "    \n",
    "state = [G.state, D.state, G_opt.state, D_opt.state]\n",
    "\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "@partial(mx.compile, inputs=state, outputs=state)\n",
    "def step(lr_img, hr_img):\n",
    "    fake_img = G(lr_img)\n",
    "    (loss_g, _), grad = d_loss_and_grad_fn(D, hr_img, fake_img)\n",
    "    D_opt.update(D, grad)\n",
    "    (loss_d, _), grad = g_loss_and_grad_fn(D, hr_img, fake_img)\n",
    "    G_opt.update(G, grad)\n",
    "    return loss_g, loss_d\n",
    "\n",
    "for batch in tqdm.tqdm(dataset, total=17123):\n",
    "    hr_img = mx.array(batch['hr_image'])\n",
    "    lr_img = mx.array(batch['lr_image'])\n",
    "\n",
    "    loss_g, loss_d = step(lr_img, hr_img)\n",
    "    mx.eval(state)\n",
    "    \n",
    "    G_losses.append(loss_g)\n",
    "    D_losses.append(loss_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1190c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(D_losses, label='Discriminator Loss')\n",
    "plt.plot(G_losses, label='Generator Loss')\n",
    "plt.title('Training Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
