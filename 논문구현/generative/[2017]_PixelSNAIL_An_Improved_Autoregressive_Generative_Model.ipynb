{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:02:14.716447Z",
     "start_time": "2024-08-18T19:02:13.024209Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import dataclasses\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Hyperparameter:\n",
    "    n_channels: int = 256\n",
    "    n_res_layers: int = 5\n",
    "    n_attn_layers: int = 12\n",
    "    attn_n_hidden: int = 1\n",
    "    attn_d_query: int = 16\n",
    "    attn_d_value: int = 128\n",
    "    attn_drop_rate: float = 0\n",
    "    n_logistic_mix: int = 10\n",
    "    seed: int = 0\n",
    "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    n_cond_classes: int = 10\n",
    "    n_bits: int = 4\n",
    "    image_dims: tuple = (1, 28, 28)\n",
    "    lr: float = 5e-4\n",
    "    lr_decay: float = 0.999995\n",
    "    polyak: float = 0.9995\n",
    "    batch_size: int = 4\n",
    "    n_epochs: int = 1\n",
    "    step: int = 0\n",
    "    start_epoch: int = 0\n",
    "    log_interval: int = 50\n",
    "    eval_interval: int = 10\n",
    "    n_samples: int = 8\n",
    "    num_workers: int = 0\n",
    "    dataset_root: str = 'C:\\\\Users\\\\tama0\\\\.data'\n",
    "    is_mini_dataset: bool = True\n",
    "    \n",
    "    \n",
    "hp = Hyperparameter"
   ],
   "id": "637df7c4444f06f2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-18T19:02:16.417365Z",
     "start_time": "2024-08-18T19:02:14.719453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def preprocess(x, n_bits):\n",
    "    return x.float().div(2**n_bits - 1).mul(2).add(-1)\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(), \n",
    "    lambda x: x.mul(255).div(2**(8-hp.n_bits)).floor(), \n",
    "    partial(preprocess, n_bits=hp.n_bits)\n",
    "])  \n",
    "target_transform = \\\n",
    "    (lambda y: torch.eye(hp.n_cond_classes)[y]) if hp.n_cond_classes else None\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(hp.dataset_root, train=True, download=True,\n",
    "                                           transform=transform, target_transform=target_transform)\n",
    "test_dataset = torchvision.datasets.MNIST(hp.dataset_root, train=False, download=True,\n",
    "                                          transform=transform, target_transform=target_transform)\n",
    "\n",
    "if hp.is_mini_dataset:\n",
    "    train_dataset.data = train_dataset.data[:hp.batch_size*4]\n",
    "    train_dataset.targets = train_dataset.targets[:hp.batch_size*4]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=hp.batch_size, shuffle=True, num_workers=hp.num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=hp.batch_size, shuffle=True, num_workers=hp.num_workers)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:02:16.612651Z",
     "start_time": "2024-08-18T19:02:16.609242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Conv2d(nn.Conv2d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        nn.utils.weight_norm(self)\n",
    "        \n",
    "class DownShiftedConv2d(Conv2d):\n",
    "    def forward(self, x):\n",
    "        hk, wk = self.kernel_size\n",
    "        x = F.pad(x, ((wk-1)//2, (wk-1)//2, hk-1, 0))\n",
    "        return super().forward(x)\n",
    "    \n",
    "class DownRightShiftedConv2d(Conv2d):\n",
    "    def forward(self, x):\n",
    "        hk, wk = self.kernel_size\n",
    "        x = F.pad(x, (wk-1, 0, hk-1, 0))\n",
    "        return super().forward(x)\n",
    "        "
   ],
   "id": "bb24fa33341885e6",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](https://github.com/crlotwhite/ML_Study/blob/pixelsnail/%EB%85%BC%EB%AC%B8%EA%B5%AC%ED%98%84/generative/assets/Cap%202024-08-19%2004-10-42-669.png?raw=true)",
   "id": "4e0d278f6945fa84"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:02:16.622062Z",
     "start_time": "2024-08-18T19:02:16.617754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GatedResidualBlock(nn.Module):\n",
    "    def __init__(self, conv, n_channels, kernel_size, \n",
    "                 drop_rate=0, shortcut_channels=None, n_cond_classes=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv_1 = conv(2 * n_channels, n_channels, kernel_size)\n",
    "        if shortcut_channels:\n",
    "            self.conv_1_sc = Conv2d(2 * shortcut_channels, n_channels, kernel_size=1)\n",
    "        if drop_rate > 0:\n",
    "            self.drop_1 = nn.Dropout(drop_rate)\n",
    "        self.conv_2 = conv(2 * n_channels, 2 * n_channels, kernel_size)\n",
    "        \n",
    "        if n_cond_classes:\n",
    "            self.proj_h = nn.Linear(n_cond_classes, 2 * n_channels)\n",
    "            \n",
    "    def forward(self, x, a=None, h=None):\n",
    "        c1 = self.conv_1(F.elu(torch.cat([x, -x], dim=1)))\n",
    "        if a is not None:\n",
    "            c1 = c1 + self.conv_1_sc(F.elu(torch.cat([a, -a], dim=1)))\n",
    "        c1 = F.elu(torch.cat([c1, -c1], dim=1))\n",
    "        if hasattr(self, 'drop_1'):\n",
    "            c1 = self.drop_1(c1)\n",
    "        c2 = self.conv_2(c1)\n",
    "        if h is not None:\n",
    "            c2 += self.proj_h(h)[:, :, None, None]\n",
    "        a, b = c2.chunk(2, 1)\n",
    "        out = x + a * torch.sigmoid(b)\n",
    "        \n",
    "        return out"
   ],
   "id": "3ff672cf54ef292b",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](https://github.com/crlotwhite/ML_Study/blob/pixelsnail/%EB%85%BC%EB%AC%B8%EA%B5%AC%ED%98%84/generative/assets/Cap%202024-08-19%2004-10-47-981.png?raw=true)",
   "id": "7f9071558a724879"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:02:16.632343Z",
     "start_time": "2024-08-18T19:02:16.626750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AttentionGatedResidualBlock(nn.Module):\n",
    "    def __init__(self, n_channels, n_background_ch, n_res_layers, n_cond_classes,\n",
    "                 drop_rate, n_hidden, d_query, d_value, attn_drop_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_hidden = n_hidden\n",
    "        self.d_query = d_query\n",
    "        self.d_value = d_value\n",
    "        self.attn_drop_rate = attn_drop_rate\n",
    "        \n",
    "        self.input_gated_resnet = nn.ModuleList([\n",
    "            GatedResidualBlock(DownRightShiftedConv2d, n_channels, (2, 2), \n",
    "                               drop_rate, None, n_cond_classes) \n",
    "            for _ in range(n_res_layers)])\n",
    "        \n",
    "        self.in_proj_kv = nn.Sequential(\n",
    "            GatedResidualBlock(Conv2d, 2 * n_channels + n_background_ch, 1, \n",
    "                               drop_rate, None, n_cond_classes), \n",
    "            Conv2d(2 * n_channels + n_background_ch, d_query + d_value, 1)\n",
    "        )\n",
    "        self.in_proj_q = nn.Sequential(\n",
    "            GatedResidualBlock(Conv2d, n_channels + n_background_ch, 1, \n",
    "                               drop_rate, None, n_cond_classes),\n",
    "            Conv2d(n_channels + n_background_ch, d_query, 1)\n",
    "        )\n",
    "        self.out_proj = GatedResidualBlock(Conv2d, n_channels, 1, drop_rate, \n",
    "                                           d_value, n_cond_classes)\n",
    "    \n",
    "    def forward(self, x, background, attn_mask, h=None):\n",
    "        ul = x\n",
    "        for layer in self.input_gated_resnet:\n",
    "            ul = layer(ul, h=h)\n",
    "        \n",
    "        kv = self.in_proj_kv(torch.cat([x, ul, background], dim=1))\n",
    "        k, v = kv.split([self.d_query, self.d_value], dim=1)\n",
    "        q = self.in_proj_q(torch.cat([ul, background], dim=1))\n",
    "        \n",
    "        B, dq, H, W = q.shape\n",
    "        _, dv, _, _ = v.shape\n",
    "        \n",
    "        flat_q = q.reshape(B, self.n_hidden,  dq//self.n_hidden, H, W).flatten(3) \\\n",
    "                 * (dq//self.n_hidden)**-0.5\n",
    "        flat_k = k.reshape(B, self.n_hidden,  dq//self.n_hidden, H, W).flatten(3)\n",
    "        flat_v = v.reshape(B, self.n_hidden,  dv//self.n_hidden, H, W).flatten(3)\n",
    "        \n",
    "        logits = flat_q.transpose(2, 3) @ flat_k\n",
    "        logits = F.dropout(logits, p=self.attn_drop_rate, \n",
    "                           training=self.training, inplace=True)\n",
    "        logits = logits.masked_fill(attn_mask == 0, -1e10)\n",
    "        weights = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        attn_out = weights @ flat_v.transpose(2, 3)\n",
    "        attn_out = attn_out.transpose(2, 3)\n",
    "        attn_out = attn_out.reshape(B, -1, H, W)\n",
    "        return self.out_proj(ul, attn_out)\n",
    "        "
   ],
   "id": "299a307f665fc33b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](https://github.com/crlotwhite/ML_Study/blob/pixelsnail/%EB%85%BC%EB%AC%B8%EA%B5%AC%ED%98%84/generative/assets/Cap%202024-08-19%2004-11-03-466.png?raw=true)",
   "id": "1571aba191191763"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:02:16.642630Z",
     "start_time": "2024-08-18T19:02:16.637103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def down_shift(x):\n",
    "    return F.pad(x, (0,0,1,0))[:,:,:-1,:]\n",
    "\n",
    "\n",
    "def right_shift(x):\n",
    "    return F.pad(x, (1,0))[:,:,:,:-1]\n",
    "\n",
    "\n",
    "class PixelSNAIL(nn.Module):\n",
    "    def __init__(self, image_dims=(3, 32, 32), n_channels=128, n_res_layers=5, \n",
    "                 attn_n_layers=12, attn_n_hidden=1, attn_d_query=16, \n",
    "                 attn_d_value=128, attn_drop_rate=0, n_logistic_mix=10, \n",
    "                 n_cond_classes=None, drop_rate=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        C, H, W = image_dims\n",
    "        background_v = (((torch.arange(H, dtype=torch.float) - H / 2) / 2)\n",
    "                        .view(1, 1, -1, 1).expand(1, C, H, W))\n",
    "        background_h = (((torch.arange(W, dtype=torch.float) - W / 2) / 2)\n",
    "                        .view(1, 1, 1, -1).expand(1, C, H, W))\n",
    "        self.register_buffer('background', torch.cat([background_v, background_h], 1))\n",
    "        attn_mask = torch.tril(torch.ones(1, 1, H*W, H*W), diagonal=-1).byte()\n",
    "        self.register_buffer('attn_mask', attn_mask)\n",
    "        \n",
    "        self.ul_input_d = DownShiftedConv2d(C+1, n_channels, kernel_size=(1, 3))\n",
    "        self.ul_input_dr = DownRightShiftedConv2d(C+1, n_channels, kernel_size=(2, 1))\n",
    "        \n",
    "        self.ul_modules = nn.ModuleList([\n",
    "            AttentionGatedResidualBlock(\n",
    "                n_channels, self.background.shape[1], n_res_layers, n_cond_classes, \n",
    "                drop_rate, attn_n_hidden, attn_d_query, attn_d_value, attn_drop_rate) \n",
    "            for _ in range(attn_n_layers)])\n",
    "        self.output_conv = Conv2d(n_channels, (3 * C + 1) * n_logistic_mix, 1)\n",
    "        \n",
    "    def forward(self, x, h=None):\n",
    "        x = F.pad(x, (0, 0, 0, 0, 0, 1), value=1)\n",
    "        \n",
    "        ul = down_shift(self.ul_input_d(x)) + right_shift(self.ul_input_dr(x))\n",
    "        \n",
    "        for module in self.ul_modules:\n",
    "            ul = module(ul, self.background.expand(x.shape[0],-1,-1,-1), \n",
    "                        self.attn_mask, h)\n",
    "            \n",
    "        return self.output_conv(F.elu(ul))"
   ],
   "id": "f3a1effbd44ddef0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Polyak 평균**  \n",
    "훈련 파라미터에 대한 Polyak 평균을 사용합니다.  \n",
    "이는 파라미터의 변화가 부드럽게 진행되도록 도와줍니다.    \n",
    "CIFAR-10의 경우, 0.9995의 지수 이동 평균(Exponential Moving Average) 가중치를 사용하고, ImageNet의 경우 0.9997를 사용합니다."
   ],
   "id": "9ab64162bb67cb90"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:02:16.651078Z",
     "start_time": "2024-08-18T19:02:16.646341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Adam(torch.optim.Adam):\n",
    "    def __init__(self, *args, polyak=0.0, **kwargs):\n",
    "        if not 0.0 <= polyak <= 1.0:\n",
    "            raise ValueError('polyak value must be in [0, 1]')\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.defaults['polyak'] = polyak\n",
    "        \n",
    "    def step(self, closure=None):\n",
    "        super().step(closure)\n",
    "        \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                \n",
    "                if 'ema' not in state:\n",
    "                    state['ema'] = torch.zeros_like(p.data)\n",
    "                    \n",
    "                state['ema'] -= (1 - self.defaults['polyak']) * (state['ema'] - p.data)\n",
    "                \n",
    "    def swap_ema(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                data = p.data\n",
    "                state = self.state[p]\n",
    "                p.data = state['ema']\n",
    "                state['ema'] = data\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = super().__repr__()\n",
    "        return self.__class__.__mro__[1].__name__ \\\n",
    "            + ' (\\npolyak: {}\\n'.format(self.defaults['polyak']) \\\n",
    "            + s.partition('\\n')[2]"
   ],
   "id": "7174e47317c2dbf0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Loss Function**\n",
    "\n",
    "- **모델 개요**:\n",
    "  - 이 모델은 **변분 오토인코더(Variational Autoencoder, VAE)**의 아이디어를 활용하여 색상 강도인 **ν**를 모델링합니다.\n",
    "  - ν는 **연속 분포**를 가정하며, 관측된 서브 픽셀 값 **x**는 ν를 가장 가까운 8비트 표현으로 반올림하여 얻습니다.\n",
    "- **연속 분포 선택**:\n",
    "  - ν를 모델링하기 위해 **로지스틱 분포(Logistic distribution)** 같은 단순한 연속 분포를 선택합니다.\n",
    "  - 이를 통해 **x**에 대한 부드럽고 메모리 효율적인 예측 분포를 생성합니다.\n",
    "- **로그 가능도 및 혼합 모델**:\n",
    "  - ν는 여러 로지스틱 분포의 혼합으로 표현됩니다:\n",
    "    $$\n",
    "    \\nu \\sim \\sum_{i=1}^{K} \\pi_i \\text{logistic}(\\mu_i, s_i) \\quad (1)\n",
    "    $$\n",
    "    - 여기서 **K**는 혼합 성분의 수입니다.\n",
    "    - **πᵢ**는 각 성분의 혼합 가중치입니다.\n",
    "    - **μᵢ**와 **sᵢ**는 각각 성분의 평균과 확산을 나타냅니다.\n",
    "- **관측된 값 x의 가능도**:\n",
    "  - 관측된 값 **x**의 확률은 다음과 같이 주어집니다:\n",
    "    $$\n",
    "    P(x | \\pi, \\mu, s) = \\sum_{i=1}^{K} \\pi_i [\\sigma((x + 0.5 - \\mu_i) / s_i) - \\sigma((x - 0.5 - \\mu_i) / s_i)] \\quad (2)\n",
    "    $$\n",
    "    - **σ()**는 로지스틱 시그모이드 함수입니다.\n",
    "    - 앞의 식에서 **σ(·)**는 특정 값을 0과 1 사이로 변환합니다, 이는 누적 분포 함수의 역할을 합니다.\n",
    "  - 경계 값 처리:\n",
    "    - 0의 경우: \\( x - 0.5 \\)를 \\( -\\infty \\)로 대체합니다.\n",
    "    - 255의 경우: \\( x + 0.5 \\)를 \\( +\\infty \\)로 대체합니다.\n"
   ],
   "id": "37818c2c3b85a937"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:02:16.660250Z",
     "start_time": "2024-08-18T19:02:16.655747Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 8,
   "source": [
    "def discretized_mix_logistic_loss(l, x, n_bits):\n",
    "    B, C, H, W = x.shape\n",
    "    n_mix = l.shape[1] // (1 + 3 * C)\n",
    "\n",
    "    logits = l[:, :n_mix, :, :]\n",
    "    l = l[:, n_mix:, :, :].reshape(B, 3 * n_mix, C, H, W)\n",
    "    means, logscales, coeffs = l.split(n_mix, 1)\n",
    "    logscales = logscales.clamp(min=-7)\n",
    "    coeffs = coeffs.tanh()\n",
    "\n",
    "    x = x.unsqueeze(1).expand_as(means)\n",
    "    if C != 1:\n",
    "        m1 = means[:, :, 0, :, :]\n",
    "        m2 = means[:, :, 1, :, :] + coeffs[:, :, 0, :, :] * x[:, :, 0, :, :]\n",
    "        m3 = means[:, :, 2, :, :] + coeffs[:, :, 1, :, :] * x[:, :, 0, :, :] + coeffs[:, :, 2, :, :] * x[:, :, 1, :, :]\n",
    "        means = torch.stack([m1, m2, m3], 2)\n",
    "\n",
    "    scales = torch.exp(-logscales)\n",
    "    plus = scales * (x - means + 1 / (2 ** n_bits - 1))\n",
    "    minus = scales * (x - means - 1 / (2 ** n_bits - 1))\n",
    "\n",
    "    cdf_minus = torch.sigmoid(minus)\n",
    "    log_one_minus_cdf_minus = -F.softplus(minus)\n",
    "    cdf_plus = torch.sigmoid(plus)\n",
    "    log_cdf_plus = plus - F.softplus(plus)\n",
    "\n",
    "    log_probs = torch.where(x < -0.999, log_cdf_plus,\n",
    "                            torch.where(x > 0.999, log_one_minus_cdf_minus,\n",
    "                                        torch.log((cdf_plus - cdf_minus).clamp(min=1e-12))))\n",
    "    log_probs = log_probs.sum(2) + F.log_softmax(logits, 1)\n",
    "\n",
    "    return -log_probs.logsumexp(1).sum([1, 2])\n"
   ],
   "id": "326987d4edf4687e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:02:16.671032Z",
     "start_time": "2024-08-18T19:02:16.663973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def sample_from_discretized_mix_logistic(l, image_dims):\n",
    "    B, _, H, W = l.shape\n",
    "    C = image_dims[0]\n",
    "    n_mix = l.shape[1] // (1 + 3 * C)\n",
    "\n",
    "    logits = l[:, :n_mix, :, :]\n",
    "    l = l[:, n_mix:, :, :].reshape(B, 3 * n_mix, C, H, W)\n",
    "    means, logscales, coeffs = l.split(n_mix, 1)\n",
    "    logscales = logscales.clamp(min=-7)\n",
    "    coeffs = coeffs.tanh()\n",
    "\n",
    "    argmax = torch.argmax(logits - torch.log(-torch.log(torch.rand_like(logits).uniform_(1e-5, 1 - 1e-5))), dim=1)\n",
    "    sel = torch.eye(n_mix, device=logits.device)[argmax].permute(0, 3, 1, 2).unsqueeze(2)\n",
    "\n",
    "    means = means.mul(sel).sum(1)\n",
    "    logscales = logscales.mul(sel).sum(1)\n",
    "    coeffs = coeffs.mul(sel).sum(1)\n",
    "\n",
    "    u = torch.rand_like(means).uniform_(1e-5, 1 - 1e-5)\n",
    "    x = means + logscales.exp() * (torch.log(u) - torch.log1p(-u))\n",
    "\n",
    "    if C == 1:\n",
    "        return x.clamp(-1, 1)\n",
    "    \n",
    "    x0 = torch.clamp(x[:, 0, :, :], -1, 1)\n",
    "    x1 = torch.clamp(x[:, 1, :, :] + coeffs[:, 0, :, :] * x0, -1, 1)\n",
    "    x2 = torch.clamp(x[:, 2, :, :] + coeffs[:, 1, :, :] * x0 + coeffs[:, 2, :, :] * x1, -1, 1)\n",
    "    return torch.stack([x0, x1, x2], 1)\n",
    "\n",
    "def generate_fn(model, n_samples, image_dims, device, h=None):\n",
    "    out = torch.zeros(n_samples, *image_dims, device=device)\n",
    "    with tqdm(total=(image_dims[1] * image_dims[2]), desc='Generating {} images'.format(n_samples)) as pbar:\n",
    "        for yi in range(image_dims[1]):\n",
    "            for xi in range(image_dims[2]):\n",
    "                l = model(out, h)\n",
    "                out[:, :, yi, xi] = sample_from_discretized_mix_logistic(l, image_dims)[:, :, yi, xi]\n",
    "                pbar.update()\n",
    "    return out\n"
   ],
   "id": "62d11536e6edb248",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:02:18.651565Z",
     "start_time": "2024-08-18T19:02:16.677765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = (PixelSNAIL(hp.image_dims, hp.n_channels, hp.n_res_layers, hp.n_attn_layers, \n",
    "                   hp.attn_n_hidden, hp.attn_d_query, hp.attn_d_value, \n",
    "                   hp.attn_drop_rate, hp.n_logistic_mix, hp.n_cond_classes)\n",
    "         .to(hp.device))\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=hp.lr, betas=(0.95, 0.9995), \n",
    "                       polyak=hp.polyak, eps=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, hp.lr_decay)"
   ],
   "id": "d0f44617304559af",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tama0\\miniconda3\\envs\\dl\\Lib\\site-packages\\torch\\nn\\utils\\weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:02:18.664777Z",
     "start_time": "2024-08-18T19:02:18.660411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, loss_fn, epoch, hp):\n",
    "    model.train()\n",
    "\n",
    "    with tqdm(total=len(dataloader), desc='epoch {}/{}'.format(epoch, hp.start_epoch + hp.n_epochs)) as pbar:\n",
    "        for x,y in dataloader:\n",
    "            hp.step += 1\n",
    "\n",
    "            x = x.to(hp.device)\n",
    "            logits = model(x, y.to(hp.device) if hp.n_cond_classes else None)\n",
    "            loss = loss_fn(logits, x, hp.n_bits).mean(0)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if scheduler: \n",
    "                scheduler.step()\n",
    "\n",
    "            pbar.set_postfix(bits_per_dim='{:.4f}'.format(loss.item() / (np.log(2) * np.prod(hp.image_dims))))\n",
    "            pbar.update()"
   ],
   "id": "1fcdaf6b73d29c2b",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:02:18.673828Z",
     "start_time": "2024-08-18T19:02:18.671370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, loss_fn, hp):\n",
    "    model.eval()\n",
    "\n",
    "    losses = 0\n",
    "    for x,y in tqdm(dataloader, desc='Evaluate'):\n",
    "        x = x.to(hp.device)\n",
    "        logits = model(x, y.to(hp.device) if hp.n_cond_classes else None)\n",
    "        losses += loss_fn(logits, x, hp.n_bits).mean(0).item()\n",
    "    return losses / len(dataloader)"
   ],
   "id": "c6d0d335aeefb727",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:02:18.683360Z",
     "start_time": "2024-08-18T19:02:18.680446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "@torch.no_grad()\n",
    "def generate(model, generate_fn, hp):\n",
    "    model.eval()\n",
    "    if hp.n_cond_classes:\n",
    "        samples = []\n",
    "        for h in range(hp.n_cond_classes):\n",
    "            h = torch.eye(hp.n_cond_classes)[h,None].to(hp.device)\n",
    "            samples += [generate_fn(model, hp.n_samples, hp.image_dims, hp.device, h=h)]\n",
    "        samples = torch.cat(samples)\n",
    "    else:\n",
    "        samples = generate_fn(model, hp.n_samples, hp.image_dims, hp.device)\n",
    "    return torchvision.utils.make_grid(samples.cpu(), normalize=True, scale_each=True, nrow=hp.n_samples)"
   ],
   "id": "a60c4856599b70ad",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:02:18.694046Z",
     "start_time": "2024-08-18T19:02:18.690314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, train_dataloader, test_dataloader, \n",
    "                       optimizer, scheduler, loss_fn, generate_fn, hp):\n",
    "    for epoch in range(hp.start_epoch, hp.start_epoch + hp.n_epochs):\n",
    "        # train\n",
    "        train_epoch(model, train_dataloader, \n",
    "                    optimizer, scheduler, loss_fn, epoch, hp)\n",
    "\n",
    "        if (epoch+1) % hp.eval_interval == 0:\n",
    "            # save model\n",
    "            torch.save({'epoch': epoch,\n",
    "                        'global_step': hp.step,\n",
    "                        'state_dict': model.state_dict()},\n",
    "                        os.path.join(hp.output_dir, 'checkpoint.pt'))\n",
    "            torch.save(optimizer.state_dict(), os.path.join(hp.output_dir, 'optim_checkpoint.pt'))\n",
    "            if scheduler: torch.save(scheduler.state_dict(), os.path.join(hp.output_dir, 'sched_checkpoint.pt'))\n",
    "\n",
    "            # swap params to ema values\n",
    "            optimizer.swap_ema()\n",
    "\n",
    "            # evaluate\n",
    "            eval_loss = evaluate(model, test_dataloader, loss_fn, hp)\n",
    "            print('Evaluate bits per dim: {:.3f}'.format(eval_loss.item() / (np.log(2) * np.prod(hp.image_dims))))\n",
    "\n",
    "            # generate\n",
    "            samples = generate(model, generate_fn, hp)\n",
    "            torchvision.utils.save_image(samples, os.path.join(hp.output_dir, 'generation_sample_step_{}.png'.format(hp.step)))\n",
    "\n",
    "            # restore params to gradient optimized\n",
    "            optimizer.swap_ema()"
   ],
   "id": "a50c002a719311a5",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:02:21.693285Z",
     "start_time": "2024-08-18T19:02:18.701163Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hp.output_dir = os.path.join(os.getcwd(), 'assets')\n",
    "train_and_evaluate(model, train_loader, test_loader, optimizer, scheduler,\n",
    "                   discretized_mix_logistic_loss, generate_fn, hp)"
   ],
   "id": "623d6f6f2f4e0443",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0/1: 100%|██████████| 4/4 [00:02<00:00,  1.34it/s, bits_per_dim=3.6236]\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:07:16.112163Z",
     "start_time": "2024-08-18T19:07:13.044581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "x = torch.zeros(1, 1, 28, 28).to(hp.device)\n",
    "(make_dot(model(x), params=dict(list(model.named_parameters())), show_attrs=True, show_saved=True)\n",
    " .render('pixelsnail', format='svg'))\n"
   ],
   "id": "6dd29dacff6abf66",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pixelsnail.svg'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "레이어 구조: https://github.com/crlotwhite/ML_Study/blob/pixelsnail/%EB%85%BC%EB%AC%B8%EA%B5%AC%ED%98%84/generative/assets/pixelsnail.svg",
   "id": "e4746d053fe3423b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-18T19:44:07.106598Z",
     "start_time": "2024-08-18T19:09:27.561283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "samples = generate(model, generate_fn, hp)\n",
    "torchvision.utils.save_image(samples, \n",
    "                             os.path.join(hp.output_dir, \n",
    "                                          'generation_sample_step_{}.png'.format(hp.step)))"
   ],
   "id": "89fae451454df6a5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating 8 images: 100%|██████████| 784/784 [03:29<00:00,  3.74it/s]\n",
      "Generating 8 images: 100%|██████████| 784/784 [03:28<00:00,  3.76it/s]\n",
      "Generating 8 images: 100%|██████████| 784/784 [03:29<00:00,  3.74it/s]\n",
      "Generating 8 images: 100%|██████████| 784/784 [03:27<00:00,  3.78it/s]\n",
      "Generating 8 images: 100%|██████████| 784/784 [03:26<00:00,  3.80it/s]\n",
      "Generating 8 images: 100%|██████████| 784/784 [03:28<00:00,  3.75it/s]\n",
      "Generating 8 images: 100%|██████████| 784/784 [03:28<00:00,  3.75it/s]\n",
      "Generating 8 images: 100%|██████████| 784/784 [03:25<00:00,  3.81it/s]\n",
      "Generating 8 images: 100%|██████████| 784/784 [03:26<00:00,  3.80it/s]\n",
      "Generating 8 images: 100%|██████████| 784/784 [03:26<00:00,  3.79it/s]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**출력물**\n",
    "\n",
    "![](https://github.com/crlotwhite/ML_Study/blob/pixelsnail/%EB%85%BC%EB%AC%B8%EA%B5%AC%ED%98%84/generative/assets/generation_sample_step_4.png?raw=true)\n",
    "\n",
    "학습률이 매우 낮아서 확실히 결과가 나오지 않았습니다."
   ],
   "id": "e5db48998ee23fcf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8265b052975037e7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
