digraph {
	graph [size="1641.1499999999999,1641.1499999999999"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	1410754033712 [label="
 (1, 40, 28, 28)" fillcolor=darkolivegreen1]
	1401571653904 -> 1410754019056 [dir=none]
	1410754019056 [label="input
 (1, 256, 28, 28)" fillcolor=orange]
	1401571653904 -> 1410754027088 [dir=none]
	1410754027088 [label="weight
 (40, 256, 1, 1)" fillcolor=orange]
	1401571653904 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (40,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401709433072 -> 1401571653904
	1401709433072 -> 1410754035056 [dir=none]
	1410754035056 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1401709433072 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1401709432592 -> 1401709433072
	1401709432592 [label="AddBackward0
------------
alpha: 1"]
	1410539321424 -> 1401709432592
	1410539321424 [label="AddBackward0
------------
alpha: 1"]
	1410539321712 -> 1410539321424
	1410539321712 [label="AddBackward0
------------
alpha: 1"]
	1410539319312 -> 1410539321712
	1410539319312 [label="AddBackward0
------------
alpha: 1"]
	1410753893744 -> 1410539319312
	1410753893744 [label="AddBackward0
------------
alpha: 1"]
	1410753890816 -> 1410753893744
	1410753890816 [label="AddBackward0
------------
alpha: 1"]
	1410753890672 -> 1410753890816
	1410753890672 [label="AddBackward0
------------
alpha: 1"]
	1410753895424 -> 1410753890672
	1410753895424 [label="AddBackward0
------------
alpha: 1"]
	1410753894464 -> 1410753895424
	1410753894464 [label="AddBackward0
------------
alpha: 1"]
	1410753894320 -> 1410753894464
	1410753894320 [label="AddBackward0
------------
alpha: 1"]
	1410753890384 -> 1410753894320
	1410753890384 [label="AddBackward0
------------
alpha: 1"]
	1410753890576 -> 1410753890384
	1410753890576 [label="AddBackward0
------------
alpha: 1"]
	1410753889568 -> 1410753890576
	1410753889568 [label="AddBackward0
------------
alpha: 1"]
	1410753895760 -> 1410753889568
	1410753895760 [label="AddBackward0
------------
alpha: 1"]
	1410753891104 -> 1410753895760
	1410753891104 [label="AddBackward0
------------
alpha: 1"]
	1410753892256 -> 1410753891104
	1410753892256 [label="AddBackward0
------------
alpha: 1"]
	1410753892400 -> 1410753892256
	1410753892400 [label="AddBackward0
------------
alpha: 1"]
	1410753892544 -> 1410753892400
	1410753892544 [label="AddBackward0
------------
alpha: 1"]
	1410753892688 -> 1410753892544
	1410753892688 [label="AddBackward0
------------
alpha: 1"]
	1410753892832 -> 1410753892688
	1410753892832 [label="AddBackward0
------------
alpha: 1"]
	1410753892976 -> 1410753892832
	1410753892976 [label="AddBackward0
------------
alpha: 1"]
	1410753893120 -> 1410753892976
	1410753893120 [label="AddBackward0
------------
alpha: 1"]
	1410753893264 -> 1410753893120
	1410753893264 [label="AddBackward0
------------
alpha: 1"]
	1410753893408 -> 1410753893264
	1410753893408 [label="AddBackward0
------------
alpha: 1"]
	1410753893552 -> 1410753893408
	1410753893552 [label="AddBackward0
------------
alpha: 1"]
	1410753895088 -> 1410753893552
	1410753895088 [label="AddBackward0
------------
alpha: 1"]
	1410753890912 -> 1410753895088
	1410753890912 [label="AddBackward0
------------
alpha: 1"]
	1410753893936 -> 1410753890912
	1410753893936 [label="AddBackward0
------------
alpha: 1"]
	1410753894128 -> 1410753893936
	1410753894128 [label="AddBackward0
------------
alpha: 1"]
	1410753894704 -> 1410753894128
	1410753894704 [label="AddBackward0
------------
alpha: 1"]
	1410753894800 -> 1410753894704
	1410753894800 [label="AddBackward0
------------
alpha: 1"]
	1410753894944 -> 1410753894800
	1410753894944 [label="AddBackward0
------------
alpha: 1"]
	1410753892016 -> 1410753894944
	1410753892016 [label="AddBackward0
------------
alpha: 1"]
	1410753892112 -> 1410753892016
	1410753892112 [label="AddBackward0
------------
alpha: 1"]
	1410753891488 -> 1410753892112
	1410753891488 [label="AddBackward0
------------
alpha: 1"]
	1410753891344 -> 1410753891488
	1410753891344 [label="AddBackward0
------------
alpha: 1"]
	1410753891200 -> 1410753891344
	1410753891200 [label="AddBackward0
------------
alpha: 1"]
	1410753895472 -> 1410753891200
	1410753895472 [label="AddBackward0
------------
alpha: 1"]
	1410753891872 -> 1410753895472
	1410753891872 [label="AddBackward0
------------
alpha: 1"]
	1410753891728 -> 1410753891872
	1410753891728 [label="AddBackward0
------------
alpha: 1"]
	1410753891584 -> 1410753891728
	1410753891584 [label="AddBackward0
------------
alpha: 1"]
	1410753896048 -> 1410753891584
	1410753896048 [label="AddBackward0
------------
alpha: 1"]
	1410753896192 -> 1410753896048
	1410753896192 [label="AddBackward0
------------
alpha: 1"]
	1410753896336 -> 1410753896192
	1410753896336 [label="AddBackward0
------------
alpha: 1"]
	1410753896480 -> 1410753896336
	1410753896480 [label="AddBackward0
------------
alpha: 1"]
	1410753896624 -> 1410753896480
	1410753896624 [label="AddBackward0
------------
alpha: 1"]
	1410753896768 -> 1410753896624
	1410753896768 [label="AddBackward0
------------
alpha: 1"]
	1410753896912 -> 1410753896768
	1410753896912 [label="AddBackward0
------------
alpha: 1"]
	1410753897056 -> 1410753896912
	1410753897056 [label="AddBackward0
------------
alpha: 1"]
	1410753897200 -> 1410753897056
	1410753897200 [label="AddBackward0
------------
alpha: 1"]
	1410753897344 -> 1410753897200
	1410753897344 [label="AddBackward0
------------
alpha: 1"]
	1410753897488 -> 1410753897344
	1410753897488 [label="AddBackward0
------------
alpha: 1"]
	1410753897632 -> 1410753897488
	1410753897632 [label="AddBackward0
------------
alpha: 1"]
	1410753897776 -> 1410753897632
	1410753897776 [label="AddBackward0
------------
alpha: 1"]
	1410753897920 -> 1410753897776
	1410753897920 [label="AddBackward0
------------
alpha: 1"]
	1410753898064 -> 1410753897920
	1410753898064 [label="AddBackward0
------------
alpha: 1"]
	1410753898208 -> 1410753898064
	1410753898208 [label="AddBackward0
------------
alpha: 1"]
	1410753898352 -> 1410753898208
	1410753898352 [label="AddBackward0
------------
alpha: 1"]
	1410753898496 -> 1410753898352
	1410753898496 [label="AddBackward0
------------
alpha: 1"]
	1410753898640 -> 1410753898496
	1410753898640 [label="AddBackward0
------------
alpha: 1"]
	1410753898784 -> 1410753898640
	1410753898784 [label="AddBackward0
------------
alpha: 1"]
	1410753898928 -> 1410753898784
	1410753898928 [label="AddBackward0
------------
alpha: 1"]
	1410753899072 -> 1410753898928
	1410753899072 [label="AddBackward0
------------
alpha: 1"]
	1410753899216 -> 1410753899072
	1410753899216 [label="AddBackward0
------------
alpha: 1"]
	1410753899360 -> 1410753899216
	1410753899360 [label="AddBackward0
------------
alpha: 1"]
	1410753899504 -> 1410753899360
	1410753899504 [label="AddBackward0
------------
alpha: 1"]
	1410753899648 -> 1410753899504
	1410753899648 [label="AddBackward0
------------
alpha: 1"]
	1410753899792 -> 1410753899648
	1410753899792 [label="AddBackward0
------------
alpha: 1"]
	1410753899936 -> 1410753899792
	1410753899936 [label="AddBackward0
------------
alpha: 1"]
	1410753900080 -> 1410753899936
	1410753900080 [label="AddBackward0
------------
alpha: 1"]
	1410753900224 -> 1410753900080
	1410753900224 [label="AddBackward0
------------
alpha: 1"]
	1410753900368 -> 1410753900224
	1410753900368 [label="AddBackward0
------------
alpha: 1"]
	1410753900512 -> 1410753900368
	1410753900512 [label="AddBackward0
------------
alpha: 1"]
	1410753900656 -> 1410753900512
	1410753900656 [label="SliceBackward0
--------------------------------
dim           :                3
end           :       4294967295
self_sym_sizes: (1, 256, 28, 28)
start         :                0
step          :                1"]
	1410753900800 -> 1410753900656
	1410753900800 [label="SliceBackward0
--------------------------------
dim           :                2
end           :       4294967295
self_sym_sizes: (1, 256, 29, 28)
start         :                0
step          :                1"]
	1410753900896 -> 1410753900800
	1410753900896 [label="SliceBackward0
--------------------------------
dim           :                1
end           :       4294967295
self_sym_sizes: (1, 256, 29, 28)
start         :                0
step          :                1"]
	1410753900992 -> 1410753900896
	1410753900992 [label="SliceBackward0
--------------------------------
dim           :                0
end           :       4294967295
self_sym_sizes: (1, 256, 29, 28)
start         :                0
step          :                1"]
	1410753901088 -> 1410753900992
	1410753901088 [label="ConstantPadNdBackward0
----------------------
pad: (0, 0, 1, 0)"]
	1410753901184 -> 1410753901088
	1410753901184 -> 1410539646960 [dir=none]
	1410539646960 [label="input
 (1, 2, 28, 30)" fillcolor=orange]
	1410753901184 -> 1410539647440 [dir=none]
	1410539647440 [label="weight
 (256, 2, 1, 3)" fillcolor=orange]
	1410753901184 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410753901280 -> 1410753901184
	1410753901280 -> 1401320430128 [dir=none]
	1401320430128 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410753901280 -> 1410754133360 [dir=none]
	1410754133360 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410753901280 -> 1401320437808 [dir=none]
	1401320437808 [label="v
 (256, 2, 1, 3)" fillcolor=orange]
	1410753901280 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724828528 -> 1410753901280
	1401320437808 [label="ul_input_d.weight_v
 (256, 2, 1, 3)" fillcolor=lightblue]
	1401320437808 -> 1410724828528
	1410724828528 [label=AccumulateGrad]
	1410724828624 -> 1410753901280
	1401320430128 [label="ul_input_d.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320430128 -> 1410724828624
	1410724828624 [label=AccumulateGrad]
	1410724828768 -> 1410753901184
	1401320432048 [label="ul_input_d.bias
 (256)" fillcolor=lightblue]
	1401320432048 -> 1410724828768
	1410724828768 [label=AccumulateGrad]
	1410753900608 -> 1410753900512
	1410753900608 [label="SliceBackward0
--------------------------------
dim           :                3
end           :       4294967295
self_sym_sizes: (1, 256, 28, 29)
start         :                0
step          :                1"]
	1410753900944 -> 1410753900608
	1410753900944 [label="SliceBackward0
--------------------------------
dim           :                2
end           :       4294967295
self_sym_sizes: (1, 256, 28, 29)
start         :                0
step          :                1"]
	1410753901136 -> 1410753900944
	1410753901136 [label="SliceBackward0
--------------------------------
dim           :                1
end           :       4294967295
self_sym_sizes: (1, 256, 28, 29)
start         :                0
step          :                1"]
	1410753900704 -> 1410753901136
	1410753900704 [label="SliceBackward0
--------------------------------
dim           :                0
end           :       4294967295
self_sym_sizes: (1, 256, 28, 29)
start         :                0
step          :                1"]
	1410753901376 -> 1410753900704
	1410753901376 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0)"]
	1410753901472 -> 1410753901376
	1410753901472 -> 1410753954544 [dir=none]
	1410753954544 [label="input
 (1, 2, 29, 28)" fillcolor=orange]
	1410753901472 -> 1410539646000 [dir=none]
	1410539646000 [label="weight
 (256, 2, 2, 1)" fillcolor=orange]
	1410753901472 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410753901568 -> 1410753901472
	1410753901568 -> 1401320431760 [dir=none]
	1401320431760 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410753901568 -> 1410754151920 [dir=none]
	1410754151920 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410753901568 -> 1401320438192 [dir=none]
	1401320438192 [label="v
 (256, 2, 2, 1)" fillcolor=orange]
	1410753901568 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724828096 -> 1410753901568
	1401320438192 [label="ul_input_dr.weight_v
 (256, 2, 2, 1)" fillcolor=lightblue]
	1401320438192 -> 1410724828096
	1410724828096 [label=AccumulateGrad]
	1410724828144 -> 1410753901568
	1401320431760 [label="ul_input_dr.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320431760 -> 1410724828144
	1410724828144 [label=AccumulateGrad]
	1410724828288 -> 1410753901472
	1401320438000 [label="ul_input_dr.bias
 (256)" fillcolor=lightblue]
	1401320438000 -> 1410724828288
	1410724828288 [label=AccumulateGrad]
	1410753900464 -> 1410753900368
	1410753900464 -> 1410753955600 [dir=none]
	1410753955600 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753900464 -> 1410753955408 [dir=none]
	1410753955408 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753900464 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753901040 -> 1410753900464
	1410753901040 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410753901424 -> 1410753901040
	1410753901424 -> 1410753955216 [dir=none]
	1410753955216 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410753901424 -> 1410539646288 [dir=none]
	1410539646288 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410753901424 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410753900752 -> 1410753901424
	1410753900752 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410753901712 -> 1410753900752
	1410753901712 -> 1410754153744 [dir=none]
	1410754153744 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410753901712 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410753901808 -> 1410753901712
	1410753901808 -> 1410753955120 [dir=none]
	1410753955120 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410753901808 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410753901904 -> 1410753901808
	1410753901904 [label="CatBackward0
------------
dim: 1"]
	1410753902000 -> 1410753901904
	1410753902000 -> 1410753954928 [dir=none]
	1410753954928 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410753902000 -> 1410753954736 [dir=none]
	1410753954736 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410753902000 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410753902144 -> 1410753902000
	1410753902144 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410753902288 -> 1410753902144
	1410753902288 -> 1410753954832 [dir=none]
	1410753954832 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410753902288 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410753902384 -> 1410753902288
	1410753902384 [label="CatBackward0
------------
dim: 1"]
	1410753900512 -> 1410753902384
	1410753902480 -> 1410753902384
	1410753902480 [label=NegBackward0]
	1410753900512 -> 1410753902480
	1410753902096 -> 1410753902000
	1410753902096 -> 1401320437904 [dir=none]
	1401320437904 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410753902096 -> 1410754155664 [dir=none]
	1410754155664 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410753902096 -> 1401320438576 [dir=none]
	1401320438576 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410753902096 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724827136 -> 1410753902096
	1401320438576 [label="ul_modules.0.input_gated_resnet.0.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401320438576 -> 1410724827136
	1410724827136 [label=AccumulateGrad]
	1410724827232 -> 1410753902096
	1401320437904 [label="ul_modules.0.input_gated_resnet.0.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320437904 -> 1410724827232
	1410724827232 [label=AccumulateGrad]
	1410724827568 -> 1410753902000
	1401320438384 [label="ul_modules.0.input_gated_resnet.0.conv_1.bias
 (256)" fillcolor=lightblue]
	1401320438384 -> 1410724827568
	1410724827568 [label=AccumulateGrad]
	1410753901952 -> 1410753901904
	1410753901952 [label=NegBackward0]
	1410753902000 -> 1410753901952
	1410753901520 -> 1410753901424
	1410753901520 -> 1401320438288 [dir=none]
	1401320438288 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410753901520 -> 1410754157296 [dir=none]
	1410754157296 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410753901520 -> 1401320438960 [dir=none]
	1401320438960 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410753901520 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724827760 -> 1410753901520
	1401320438960 [label="ul_modules.0.input_gated_resnet.0.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401320438960 -> 1410724827760
	1410724827760 [label=AccumulateGrad]
	1410724827856 -> 1410753901520
	1401320438288 [label="ul_modules.0.input_gated_resnet.0.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401320438288 -> 1410724827856
	1410724827856 [label=AccumulateGrad]
	1410724829296 -> 1410753901424
	1401320438768 [label="ul_modules.0.input_gated_resnet.0.conv_2.bias
 (512)" fillcolor=lightblue]
	1401320438768 -> 1410724829296
	1410724829296 [label=AccumulateGrad]
	1410753900848 -> 1410753900464
	1410753900848 -> 1410754158448 [dir=none]
	1410754158448 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410753900848 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753901040 -> 1410753900848
	1410753900320 -> 1410753900224
	1410753900320 -> 1410753956560 [dir=none]
	1410753956560 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753900320 -> 1410753956368 [dir=none]
	1410753956368 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753900320 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753901664 -> 1410753900320
	1410753901664 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410753901760 -> 1410753901664
	1410753901760 -> 1410753956176 [dir=none]
	1410753956176 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410753901760 -> 1410753955504 [dir=none]
	1410753955504 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410753901760 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410753901616 -> 1410753901760
	1410753901616 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410753902048 -> 1410753901616
	1410753902048 -> 1410754159504 [dir=none]
	1410754159504 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410753902048 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410753902576 -> 1410753902048
	1410753902576 -> 1410753956080 [dir=none]
	1410753956080 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410753902576 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410753902192 -> 1410753902576
	1410753902192 [label="CatBackward0
------------
dim: 1"]
	1410753902672 -> 1410753902192
	1410753902672 -> 1410753955888 [dir=none]
	1410753955888 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410753902672 -> 1410753955696 [dir=none]
	1410753955696 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410753902672 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410753902816 -> 1410753902672
	1410753902816 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410753902960 -> 1410753902816
	1410753902960 -> 1410753955024 [dir=none]
	1410753955024 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410753902960 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410753903056 -> 1410753902960
	1410753903056 [label="CatBackward0
------------
dim: 1"]
	1410753900368 -> 1410753903056
	1410753903152 -> 1410753903056
	1410753903152 [label=NegBackward0]
	1410753900368 -> 1410753903152
	1410753902768 -> 1410753902672
	1410753902768 -> 1401320438672 [dir=none]
	1401320438672 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410753902768 -> 1410754161424 [dir=none]
	1410754161424 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410753902768 -> 1401320439536 [dir=none]
	1401320439536 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410753902768 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724826176 -> 1410753902768
	1401320439536 [label="ul_modules.0.input_gated_resnet.1.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401320439536 -> 1410724826176
	1410724826176 [label=AccumulateGrad]
	1410724826272 -> 1410753902768
	1401320438672 [label="ul_modules.0.input_gated_resnet.1.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320438672 -> 1410724826272
	1410724826272 [label=AccumulateGrad]
	1410724826608 -> 1410753902672
	1401320439344 [label="ul_modules.0.input_gated_resnet.1.conv_1.bias
 (256)" fillcolor=lightblue]
	1401320439344 -> 1410724826608
	1410724826608 [label=AccumulateGrad]
	1410753902624 -> 1410753902192
	1410753902624 [label=NegBackward0]
	1410753902672 -> 1410753902624
	1410753901856 -> 1410753901760
	1410753901856 -> 1401320439248 [dir=none]
	1401320439248 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410753901856 -> 1410754163056 [dir=none]
	1410754163056 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410753901856 -> 1401320489136 [dir=none]
	1401320489136 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410753901856 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724826800 -> 1410753901856
	1401320489136 [label="ul_modules.0.input_gated_resnet.1.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401320489136 -> 1410724826800
	1410724826800 [label=AccumulateGrad]
	1410724826944 -> 1410753901856
	1401320439248 [label="ul_modules.0.input_gated_resnet.1.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401320439248 -> 1410724826944
	1410724826944 [label=AccumulateGrad]
	1410724828048 -> 1410753901760
	1401320439728 [label="ul_modules.0.input_gated_resnet.1.conv_2.bias
 (512)" fillcolor=lightblue]
	1401320439728 -> 1410724828048
	1410724828048 [label=AccumulateGrad]
	1410753900560 -> 1410753900320
	1410753900560 -> 1410754164208 [dir=none]
	1410754164208 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410753900560 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753901664 -> 1410753900560
	1410753900176 -> 1410753900080
	1410753900176 -> 1410753957424 [dir=none]
	1410753957424 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753900176 -> 1410753957232 [dir=none]
	1410753957232 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753900176 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753902240 -> 1410753900176
	1410753902240 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410753902432 -> 1410753902240
	1410753902432 -> 1410753957040 [dir=none]
	1410753957040 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410753902432 -> 1410753955792 [dir=none]
	1410753955792 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410753902432 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410753902336 -> 1410753902432
	1410753902336 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410753902720 -> 1410753902336
	1410753902720 -> 1410754165264 [dir=none]
	1410754165264 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410753902720 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410753903248 -> 1410753902720
	1410753903248 -> 1410753956944 [dir=none]
	1410753956944 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410753903248 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410753902864 -> 1410753903248
	1410753902864 [label="CatBackward0
------------
dim: 1"]
	1410753903344 -> 1410753902864
	1410753903344 -> 1410753956656 [dir=none]
	1410753956656 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410753903344 -> 1410753955984 [dir=none]
	1410753955984 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410753903344 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410753903488 -> 1410753903344
	1410753903488 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410753903632 -> 1410753903488
	1410753903632 -> 1410753956464 [dir=none]
	1410753956464 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410753903632 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410753903728 -> 1410753903632
	1410753903728 [label="CatBackward0
------------
dim: 1"]
	1410753900224 -> 1410753903728
	1410753903824 -> 1410753903728
	1410753903824 [label=NegBackward0]
	1410753900224 -> 1410753903824
	1410753903440 -> 1410753903344
	1410753903440 -> 1401320439632 [dir=none]
	1401320439632 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410753903440 -> 1410754183632 [dir=none]
	1410754183632 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410753903440 -> 1401320489712 [dir=none]
	1401320489712 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410753903440 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724825216 -> 1410753903440
	1401320489712 [label="ul_modules.0.input_gated_resnet.2.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401320489712 -> 1410724825216
	1410724825216 [label=AccumulateGrad]
	1410724825312 -> 1410753903440
	1401320439632 [label="ul_modules.0.input_gated_resnet.2.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320439632 -> 1410724825312
	1410724825312 [label=AccumulateGrad]
	1410724825648 -> 1410753903344
	1401320489520 [label="ul_modules.0.input_gated_resnet.2.conv_1.bias
 (256)" fillcolor=lightblue]
	1401320489520 -> 1410724825648
	1410724825648 [label=AccumulateGrad]
	1410753903296 -> 1410753902864
	1410753903296 [label=NegBackward0]
	1410753903344 -> 1410753903296
	1410753902528 -> 1410753902432
	1410753902528 -> 1401320489424 [dir=none]
	1401320489424 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410753902528 -> 1410754185264 [dir=none]
	1410754185264 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410753902528 -> 1401320490096 [dir=none]
	1401320490096 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410753902528 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724825840 -> 1410753902528
	1401320490096 [label="ul_modules.0.input_gated_resnet.2.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401320490096 -> 1410724825840
	1410724825840 [label=AccumulateGrad]
	1410724825936 -> 1410753902528
	1401320489424 [label="ul_modules.0.input_gated_resnet.2.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401320489424 -> 1410724825936
	1410724825936 [label=AccumulateGrad]
	1410724827424 -> 1410753902432
	1401320489904 [label="ul_modules.0.input_gated_resnet.2.conv_2.bias
 (512)" fillcolor=lightblue]
	1401320489904 -> 1410724827424
	1410724827424 [label=AccumulateGrad]
	1410753900416 -> 1410753900176
	1410753900416 -> 1410754186416 [dir=none]
	1410754186416 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410753900416 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753902240 -> 1410753900416
	1410753900032 -> 1410753899936
	1410753900032 -> 1410753958288 [dir=none]
	1410753958288 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753900032 -> 1410753958096 [dir=none]
	1410753958096 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753900032 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753902912 -> 1410753900032
	1410753902912 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410753903104 -> 1410753902912
	1410753903104 -> 1410753957904 [dir=none]
	1410753957904 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410753903104 -> 1410753956752 [dir=none]
	1410753956752 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410753903104 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410753903008 -> 1410753903104
	1410753903008 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410753903392 -> 1410753903008
	1410753903392 -> 1410754187472 [dir=none]
	1410754187472 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410753903392 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410753903920 -> 1410753903392
	1410753903920 -> 1410753957808 [dir=none]
	1410753957808 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410753903920 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410753903536 -> 1410753903920
	1410753903536 [label="CatBackward0
------------
dim: 1"]
	1410753904016 -> 1410753903536
	1410753904016 -> 1410753957520 [dir=none]
	1410753957520 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410753904016 -> 1410753956848 [dir=none]
	1410753956848 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410753904016 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410753904160 -> 1410753904016
	1410753904160 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410753904304 -> 1410753904160
	1410753904304 -> 1410753957328 [dir=none]
	1410753957328 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410753904304 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410753904400 -> 1410753904304
	1410753904400 [label="CatBackward0
------------
dim: 1"]
	1410753900080 -> 1410753904400
	1410753904496 -> 1410753904400
	1410753904496 [label=NegBackward0]
	1410753900080 -> 1410753904496
	1410753904112 -> 1410753904016
	1410753904112 -> 1401320489808 [dir=none]
	1401320489808 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410753904112 -> 1410754189392 [dir=none]
	1410754189392 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410753904112 -> 1401320490672 [dir=none]
	1401320490672 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410753904112 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724824256 -> 1410753904112
	1401320490672 [label="ul_modules.0.input_gated_resnet.3.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401320490672 -> 1410724824256
	1410724824256 [label=AccumulateGrad]
	1410724824352 -> 1410753904112
	1401320489808 [label="ul_modules.0.input_gated_resnet.3.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320489808 -> 1410724824352
	1410724824352 [label=AccumulateGrad]
	1410724824688 -> 1410753904016
	1401320490480 [label="ul_modules.0.input_gated_resnet.3.conv_1.bias
 (256)" fillcolor=lightblue]
	1401320490480 -> 1410724824688
	1410724824688 [label=AccumulateGrad]
	1410753903968 -> 1410753903536
	1410753903968 [label=NegBackward0]
	1410753904016 -> 1410753903968
	1410753903200 -> 1410753903104
	1410753903200 -> 1401320490384 [dir=none]
	1401320490384 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410753903200 -> 1410754191024 [dir=none]
	1410754191024 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410753903200 -> 1401320491056 [dir=none]
	1401320491056 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410753903200 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724824880 -> 1410753903200
	1401320491056 [label="ul_modules.0.input_gated_resnet.3.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401320491056 -> 1410724824880
	1410724824880 [label=AccumulateGrad]
	1410724824976 -> 1410753903200
	1401320490384 [label="ul_modules.0.input_gated_resnet.3.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401320490384 -> 1410724824976
	1410724824976 [label=AccumulateGrad]
	1410724826416 -> 1410753903104
	1401320490864 [label="ul_modules.0.input_gated_resnet.3.conv_2.bias
 (512)" fillcolor=lightblue]
	1401320490864 -> 1410724826416
	1410724826416 [label=AccumulateGrad]
	1410753900272 -> 1410753900032
	1410753900272 -> 1410754192176 [dir=none]
	1410754192176 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410753900272 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753902912 -> 1410753900272
	1410753899888 -> 1410753899792
	1410753899888 -> 1410753959152 [dir=none]
	1410753959152 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753899888 -> 1410753958960 [dir=none]
	1410753958960 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753899888 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753903584 -> 1410753899888
	1410753903584 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410753903776 -> 1410753903584
	1410753903776 -> 1410753958768 [dir=none]
	1410753958768 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410753903776 -> 1410753957616 [dir=none]
	1410753957616 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410753903776 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410753903680 -> 1410753903776
	1410753903680 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410753904064 -> 1410753903680
	1410753904064 -> 1410754193232 [dir=none]
	1410754193232 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410753904064 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410753904592 -> 1410753904064
	1410753904592 -> 1410753958672 [dir=none]
	1410753958672 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410753904592 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410753904208 -> 1410753904592
	1410753904208 [label="CatBackward0
------------
dim: 1"]
	1410753904352 -> 1410753904208
	1410753904352 -> 1410753958384 [dir=none]
	1410753958384 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410753904352 -> 1410753957712 [dir=none]
	1410753957712 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410753904352 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754248960 -> 1410753904352
	1410754248960 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754249104 -> 1410754248960
	1410754249104 -> 1410753958192 [dir=none]
	1410753958192 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754249104 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754249200 -> 1410754249104
	1410754249200 [label="CatBackward0
------------
dim: 1"]
	1410753899936 -> 1410754249200
	1410754249296 -> 1410754249200
	1410754249296 [label=NegBackward0]
	1410753899936 -> 1410754249296
	1410754248912 -> 1410753904352
	1410754248912 -> 1401320490768 [dir=none]
	1401320490768 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754248912 -> 1410754195152 [dir=none]
	1410754195152 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754248912 -> 1401320491632 [dir=none]
	1401320491632 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754248912 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724823296 -> 1410754248912
	1401320491632 [label="ul_modules.0.input_gated_resnet.4.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401320491632 -> 1410724823296
	1410724823296 [label=AccumulateGrad]
	1410724823392 -> 1410754248912
	1401320490768 [label="ul_modules.0.input_gated_resnet.4.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320490768 -> 1410724823392
	1410724823392 [label=AccumulateGrad]
	1410724823728 -> 1410753904352
	1401320491440 [label="ul_modules.0.input_gated_resnet.4.conv_1.bias
 (256)" fillcolor=lightblue]
	1401320491440 -> 1410724823728
	1410724823728 [label=AccumulateGrad]
	1410754248816 -> 1410753904208
	1410754248816 [label=NegBackward0]
	1410753904352 -> 1410754248816
	1410753903872 -> 1410753903776
	1410753903872 -> 1401320491344 [dir=none]
	1401320491344 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410753903872 -> 1410754196784 [dir=none]
	1410754196784 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410753903872 -> 1401320492016 [dir=none]
	1401320492016 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410753903872 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724823920 -> 1410753903872
	1401320492016 [label="ul_modules.0.input_gated_resnet.4.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401320492016 -> 1410724823920
	1410724823920 [label=AccumulateGrad]
	1410724824016 -> 1410753903872
	1401320491344 [label="ul_modules.0.input_gated_resnet.4.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401320491344 -> 1410724824016
	1410724824016 [label=AccumulateGrad]
	1410724825504 -> 1410753903776
	1401320491824 [label="ul_modules.0.input_gated_resnet.4.conv_2.bias
 (512)" fillcolor=lightblue]
	1401320491824 -> 1410724825504
	1410724825504 [label=AccumulateGrad]
	1410753900128 -> 1410753899888
	1410753900128 -> 1410754197936 [dir=none]
	1410754197936 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410753900128 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753903584 -> 1410753900128
	1410753899744 -> 1410753899648
	1410753899744 -> 1410753963472 [dir=none]
	1410753963472 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753899744 -> 1410753963280 [dir=none]
	1410753963280 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753899744 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753904256 -> 1410753899744
	1410753904256 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410753904448 -> 1410753904256
	1410753904448 -> 1410753963088 [dir=none]
	1410753963088 [label="input
 (1, 512, 28, 28)" fillcolor=orange]
	1410753904448 -> 1410753962608 [dir=none]
	1410753962608 [label="weight
 (512, 512, 1, 1)" fillcolor=orange]
	1410753904448 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410753904544 -> 1410753904448
	1410753904544 -> 1410754198992 [dir=none]
	1410754198992 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410753904544 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754248864 -> 1410753904544
	1410754248864 -> 1410753962992 [dir=none]
	1410753962992 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754248864 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754249392 -> 1410754248864
	1410754249392 [label="CatBackward0
------------
dim: 1"]
	1410754249008 -> 1410754249392
	1410754249008 [label="AddBackward0
------------
alpha: 1"]
	1410754249536 -> 1410754249008
	1410754249536 -> 1410753962128 [dir=none]
	1410753962128 [label="input
 (1, 512, 28, 28)" fillcolor=orange]
	1410754249536 -> 1410753962512 [dir=none]
	1410753962512 [label="weight
 (256, 512, 1, 1)" fillcolor=orange]
	1410754249536 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754249680 -> 1410754249536
	1410754249680 -> 1410753962416 [dir=none]
	1410753962416 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754249680 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754249824 -> 1410754249680
	1410754249824 [label="CatBackward0
------------
dim: 1"]
	1410753899792 -> 1410754249824
	1410754249920 -> 1410754249824
	1410754249920 [label=NegBackward0]
	1410753899792 -> 1410754249920
	1410754249632 -> 1410754249536
	1410754249632 -> 1401320494608 [dir=none]
	1401320494608 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754249632 -> 1410754282800 [dir=none]
	1410754282800 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754249632 -> 1401320495280 [dir=none]
	1401320495280 [label="v
 (256, 512, 1, 1)" fillcolor=orange]
	1410754249632 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724838080 -> 1410754249632
	1401320495280 [label="ul_modules.0.out_proj.conv_1.weight_v
 (256, 512, 1, 1)" fillcolor=lightblue]
	1401320495280 -> 1410724838080
	1410724838080 [label=AccumulateGrad]
	1410724838512 -> 1410754249632
	1401320494608 [label="ul_modules.0.out_proj.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320494608 -> 1410724838512
	1410724838512 [label=AccumulateGrad]
	1410724837840 -> 1410754249536
	1401320495088 [label="ul_modules.0.out_proj.conv_1.bias
 (256)" fillcolor=lightblue]
	1401320495088 -> 1410724837840
	1410724837840 [label=AccumulateGrad]
	1410754249488 -> 1410754249008
	1410754249488 -> 1410753962704 [dir=none]
	1410753962704 [label="input
 (1, 256, 28, 28)" fillcolor=orange]
	1410754249488 -> 1410753962896 [dir=none]
	1410753962896 [label="weight
 (256, 256, 1, 1)" fillcolor=orange]
	1410754249488 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754249872 -> 1410754249488
	1410754249872 -> 1410753962800 [dir=none]
	1410753962800 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410754249872 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754249728 -> 1410754249872
	1410754249728 [label="CatBackward0
------------
dim: 1"]
	1410754250112 -> 1410754249728
	1410754250112 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (1, 1, 128, 784)"]
	1410754250256 -> 1410754250112
	1410754250256 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410754250352 -> 1410754250256
	1410754250352 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (1, 784, 128)"]
	1410754250448 -> 1410754250352
	1410754250448 -> 1410754284816 [dir=none]
	1410754284816 [label="mat2
 (1, 784, 128)" fillcolor=orange]
	1410754250448 -> 1410754285104 [dir=none]
	1410754285104 [label="self
 (1, 784, 784)" fillcolor=orange]
	1410754250448 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	1410754250544 -> 1410754250448
	1410754250544 [label="ViewBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 784)"]
	1410754250688 -> 1410754250544
	1410754250688 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 784)"]
	1410754250784 -> 1410754250688
	1410754250784 -> 1410754285392 [dir=none]
	1410754285392 [label="result
 (1, 1, 784, 784)" fillcolor=orange]
	1410754250784 [label="SoftmaxBackward0
----------------------
dim   :     4294967295
result: [saved tensor]"]
	1410754250880 -> 1410754250784
	1410754250880 -> 1410753961648 [dir=none]
	1410753961648 [label="mask
 (1, 1, 784, 784)" fillcolor=orange]
	1410754250880 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	1410754250976 -> 1410754250880
	1410754250976 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (1, 784, 784)"]
	1410754251072 -> 1410754250976
	1410754251072 -> 1410754286064 [dir=none]
	1410754286064 [label="mat2
 (1, 16, 784)" fillcolor=orange]
	1410754251072 -> 1410754286352 [dir=none]
	1410754286352 [label="self
 (1, 784, 16)" fillcolor=orange]
	1410754251072 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	1410754251168 -> 1410754251072
	1410754251168 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (1, 1, 784, 16)"]
	1410754251312 -> 1410754251168
	1410754251312 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 784, 16)"]
	1410754251408 -> 1410754251312
	1410754251408 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410754251504 -> 1410754251408
	1410754251504 -> 1410754286736 [dir=none]
	1410754286736 [label="other
 ()" fillcolor=orange]
	1410754251504 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	1410754251600 -> 1410754251504
	1410754251600 [label="ViewBackward0
----------------------------------
self_sym_sizes: (1, 1, 16, 28, 28)"]
	1410754251696 -> 1410754251600
	1410754251696 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 16, 28, 28)"]
	1410754251792 -> 1410754251696
	1410754251792 -> 1410753961552 [dir=none]
	1410753961552 [label="input
 (1, 258, 28, 28)" fillcolor=orange]
	1410754251792 -> 1410753961264 [dir=none]
	1410753961264 [label="weight
 (16, 258, 1, 1)" fillcolor=orange]
	1410754251792 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754251936 -> 1410754251792
	1410754251936 [label="AddBackward0
------------
alpha: 1"]
	1410754252080 -> 1410754251936
	1410754252080 [label="CatBackward0
------------
dim: 1"]
	1410753899792 -> 1410754252080
	1410754252032 -> 1410754251936
	1410754252032 -> 1410753961360 [dir=none]
	1410753961360 [label="other
 (1, 258, 28, 28)" fillcolor=orange]
	1410754252032 -> 1410753961168 [dir=none]
	1410753961168 [label="self
 (1, 258, 28, 28)" fillcolor=orange]
	1410754252032 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410754252128 -> 1410754252032
	1410754252128 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 516, 28, 28)
split_size    :              258"]
	1410754252368 -> 1410754252128
	1410754252368 -> 1410753960784 [dir=none]
	1410753960784 [label="input
 (1, 516, 28, 28)" fillcolor=orange]
	1410754252368 -> 1410753960880 [dir=none]
	1410753960880 [label="weight
 (516, 516, 1, 1)" fillcolor=orange]
	1410754252368 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (516,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754252560 -> 1410754252368
	1410754252560 -> 1410754288560 [dir=none]
	1410754288560 [label="result1
 (1, 516, 28, 28)" fillcolor=orange]
	1410754252560 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754252704 -> 1410754252560
	1410754252704 -> 1410753960976 [dir=none]
	1410753960976 [label="self
 (1, 516, 28, 28)" fillcolor=orange]
	1410754252704 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754252848 -> 1410754252704
	1410754252848 [label="CatBackward0
------------
dim: 1"]
	1410754252944 -> 1410754252848
	1410754252944 -> 1410753960496 [dir=none]
	1410753960496 [label="input
 (1, 516, 28, 28)" fillcolor=orange]
	1410754252944 -> 1410753960688 [dir=none]
	1410753960688 [label="weight
 (258, 516, 1, 1)" fillcolor=orange]
	1410754252944 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (258,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754253184 -> 1410754252944
	1410754253184 -> 1410753960592 [dir=none]
	1410753960592 [label="self
 (1, 516, 28, 28)" fillcolor=orange]
	1410754253184 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754253328 -> 1410754253184
	1410754253328 [label="CatBackward0
------------
dim: 1"]
	1410754252080 -> 1410754253328
	1410754253424 -> 1410754253328
	1410754253424 [label=NegBackward0]
	1410754252080 -> 1410754253424
	1410754253040 -> 1410754252944
	1410754253040 -> 1401320493264 [dir=none]
	1401320493264 [label="g
 (258, 1, 1, 1)" fillcolor=orange]
	1410754253040 -> 1410754290288 [dir=none]
	1410754290288 [label="result1
 (258, 1, 1, 1)" fillcolor=orange]
	1410754253040 -> 1401320493936 [dir=none]
	1401320493936 [label="v
 (258, 516, 1, 1)" fillcolor=orange]
	1410754253040 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724970848 -> 1410754253040
	1401320493936 [label="ul_modules.0.in_proj_q.0.conv_1.weight_v
 (258, 516, 1, 1)" fillcolor=lightblue]
	1401320493936 -> 1410724970848
	1410724970848 [label=AccumulateGrad]
	1410724970992 -> 1410754253040
	1401320493264 [label="ul_modules.0.in_proj_q.0.conv_1.weight_g
 (258, 1, 1, 1)" fillcolor=lightblue]
	1401320493264 -> 1410724970992
	1410724970992 [label=AccumulateGrad]
	1410724972480 -> 1410754252944
	1401320493744 [label="ul_modules.0.in_proj_q.0.conv_1.bias
 (258)" fillcolor=lightblue]
	1401320493744 -> 1410724972480
	1410724972480 [label=AccumulateGrad]
	1410754252896 -> 1410754252848
	1410754252896 [label=NegBackward0]
	1410754252944 -> 1410754252896
	1410754252416 -> 1410754252368
	1410754252416 -> 1401320493648 [dir=none]
	1401320493648 [label="g
 (516, 1, 1, 1)" fillcolor=orange]
	1410754252416 -> 1410754291920 [dir=none]
	1410754291920 [label="result1
 (516, 1, 1, 1)" fillcolor=orange]
	1410754252416 -> 1401320494320 [dir=none]
	1401320494320 [label="v
 (516, 516, 1, 1)" fillcolor=orange]
	1410754252416 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724973920 -> 1410754252416
	1401320494320 [label="ul_modules.0.in_proj_q.0.conv_2.weight_v
 (516, 516, 1, 1)" fillcolor=lightblue]
	1401320494320 -> 1410724973920
	1410724973920 [label=AccumulateGrad]
	1410724974304 -> 1410754252416
	1401320493648 [label="ul_modules.0.in_proj_q.0.conv_2.weight_g
 (516, 1, 1, 1)" fillcolor=lightblue]
	1401320493648 -> 1410724974304
	1410724974304 [label=AccumulateGrad]
	1410724971088 -> 1410754252368
	1401320494128 [label="ul_modules.0.in_proj_q.0.conv_2.bias
 (516)" fillcolor=lightblue]
	1401320494128 -> 1410724971088
	1410724971088 [label=AccumulateGrad]
	1410754252176 -> 1410754252032
	1410754252176 -> 1410754293072 [dir=none]
	1410754293072 [label="result
 (1, 258, 28, 28)" fillcolor=orange]
	1410754252176 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410754252128 -> 1410754252176
	1410754251840 -> 1410754251792
	1410754251840 -> 1401320494032 [dir=none]
	1401320494032 [label="g
 (16, 1, 1, 1)" fillcolor=orange]
	1410754251840 -> 1410754293648 [dir=none]
	1410754293648 [label="result1
 (16, 1, 1, 1)" fillcolor=orange]
	1410754251840 -> 1401320494896 [dir=none]
	1401320494896 [label="v
 (16, 258, 1, 1)" fillcolor=orange]
	1410754251840 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724970656 -> 1410754251840
	1401320494896 [label="ul_modules.0.in_proj_q.1.weight_v
 (16, 258, 1, 1)" fillcolor=lightblue]
	1401320494896 -> 1410724970656
	1410724970656 [label=AccumulateGrad]
	1410724971136 -> 1410754251840
	1401320494032 [label="ul_modules.0.in_proj_q.1.weight_g
 (16, 1, 1, 1)" fillcolor=lightblue]
	1401320494032 -> 1410724971136
	1410724971136 [label=AccumulateGrad]
	1410724973344 -> 1410754251792
	1401320494704 [label="ul_modules.0.in_proj_q.1.bias
 (16)" fillcolor=lightblue]
	1401320494704 -> 1410724973344
	1410724973344 [label=AccumulateGrad]
	1410754251120 -> 1410754251072
	1410754251120 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 1, 16, 784)"]
	1410754251456 -> 1410754251120
	1410754251456 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 16, 784)"]
	1410754251648 -> 1410754251456
	1410754251648 [label="ViewBackward0
----------------------------------
self_sym_sizes: (1, 1, 16, 28, 28)"]
	1410754251216 -> 1410754251648
	1410754251216 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 16, 28, 28)"]
	1410754252224 -> 1410754251216
	1410754252224 [label="SplitWithSizesBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 144, 28, 28)
split_sizes   :        (16, 128)"]
	1410754252272 -> 1410754252224
	1410754252272 -> 1410753960208 [dir=none]
	1410753960208 [label="input
 (1, 514, 28, 28)" fillcolor=orange]
	1410754252272 -> 1410753959920 [dir=none]
	1410753959920 [label="weight
 (144, 514, 1, 1)" fillcolor=orange]
	1410754252272 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (144,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754252752 -> 1410754252272
	1410754252752 [label="AddBackward0
------------
alpha: 1"]
	1410754253280 -> 1410754252752
	1410754253280 [label="CatBackward0
------------
dim: 1"]
	1410753900512 -> 1410754253280
	1410753899792 -> 1410754253280
	1410754253376 -> 1410754252752
	1410754253376 -> 1410753960016 [dir=none]
	1410753960016 [label="other
 (1, 514, 28, 28)" fillcolor=orange]
	1410754253376 -> 1410753959824 [dir=none]
	1410753959824 [label="self
 (1, 514, 28, 28)" fillcolor=orange]
	1410754253376 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410754252992 -> 1410754253376
	1410754252992 [label="SplitBackward0
---------------------------------
dim           :                 1
self_sym_sizes: (1, 1028, 28, 28)
split_size    :               514"]
	1410754253616 -> 1410754252992
	1410754253616 -> 1410753959440 [dir=none]
	1410753959440 [label="input
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754253616 -> 1410753959536 [dir=none]
	1410753959536 [label="weight
 (1028, 1028, 1, 1)" fillcolor=orange]
	1410754253616 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1028,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754253712 -> 1410754253616
	1410754253712 -> 1410754296624 [dir=none]
	1410754296624 [label="result1
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754253712 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754253856 -> 1410754253712
	1410754253856 -> 1410753959632 [dir=none]
	1410753959632 [label="self
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754253856 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754254000 -> 1410754253856
	1410754254000 [label="CatBackward0
------------
dim: 1"]
	1410754254096 -> 1410754254000
	1410754254096 -> 1410753959056 [dir=none]
	1410753959056 [label="input
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754254096 -> 1410753959248 [dir=none]
	1410753959248 [label="weight
 (514, 1028, 1, 1)" fillcolor=orange]
	1410754254096 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (514,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754254336 -> 1410754254096
	1410754254336 -> 1410753958576 [dir=none]
	1410753958576 [label="self
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754254336 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754254480 -> 1410754254336
	1410754254480 [label="CatBackward0
------------
dim: 1"]
	1410754253280 -> 1410754254480
	1410754254576 -> 1410754254480
	1410754254576 [label=NegBackward0]
	1410754253280 -> 1410754254576
	1410754254192 -> 1410754254096
	1410754254192 -> 1401320491728 [dir=none]
	1401320491728 [label="g
 (514, 1, 1, 1)" fillcolor=orange]
	1410754254192 -> 1410754298416 [dir=none]
	1410754298416 [label="result1
 (514, 1, 1, 1)" fillcolor=orange]
	1410754254192 -> 1401320492592 [dir=none]
	1401320492592 [label="v
 (514, 1028, 1, 1)" fillcolor=orange]
	1410754254192 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724977808 -> 1410754254192
	1401320492592 [label="ul_modules.0.in_proj_kv.0.conv_1.weight_v
 (514, 1028, 1, 1)" fillcolor=lightblue]
	1401320492592 -> 1410724977808
	1410724977808 [label=AccumulateGrad]
	1410724977664 -> 1410754254192
	1401320491728 [label="ul_modules.0.in_proj_kv.0.conv_1.weight_g
 (514, 1, 1, 1)" fillcolor=lightblue]
	1401320491728 -> 1410724977664
	1410724977664 [label=AccumulateGrad]
	1410724977232 -> 1410754254096
	1401320492400 [label="ul_modules.0.in_proj_kv.0.conv_1.bias
 (514)" fillcolor=lightblue]
	1401320492400 -> 1410724977232
	1410724977232 [label=AccumulateGrad]
	1410754254048 -> 1410754254000
	1410754254048 [label=NegBackward0]
	1410754254096 -> 1410754254048
	1410754253664 -> 1410754253616
	1410754253664 -> 1401320492304 [dir=none]
	1401320492304 [label="g
 (1028, 1, 1, 1)" fillcolor=orange]
	1410754253664 -> 1410754300048 [dir=none]
	1410754300048 [label="result1
 (1028, 1, 1, 1)" fillcolor=orange]
	1410754253664 -> 1401320492976 [dir=none]
	1401320492976 [label="v
 (1028, 1028, 1, 1)" fillcolor=orange]
	1410754253664 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724976848 -> 1410754253664
	1401320492976 [label="ul_modules.0.in_proj_kv.0.conv_2.weight_v
 (1028, 1028, 1, 1)" fillcolor=lightblue]
	1401320492976 -> 1410724976848
	1410724976848 [label=AccumulateGrad]
	1410724976992 -> 1410754253664
	1401320492304 [label="ul_modules.0.in_proj_kv.0.conv_2.weight_g
 (1028, 1, 1, 1)" fillcolor=lightblue]
	1401320492304 -> 1410724976992
	1410724976992 [label=AccumulateGrad]
	1410724976560 -> 1410754253616
	1401320492784 [label="ul_modules.0.in_proj_kv.0.conv_2.bias
 (1028)" fillcolor=lightblue]
	1401320492784 -> 1410724976560
	1410724976560 [label=AccumulateGrad]
	1410754253520 -> 1410754253376
	1410754253520 -> 1410754301200 [dir=none]
	1410754301200 [label="result
 (1, 514, 28, 28)" fillcolor=orange]
	1410754253520 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410754252992 -> 1410754253520
	1410754252320 -> 1410754252272
	1410754252320 -> 1401320492688 [dir=none]
	1401320492688 [label="g
 (144, 1, 1, 1)" fillcolor=orange]
	1410754252320 -> 1410754301776 [dir=none]
	1410754301776 [label="result1
 (144, 1, 1, 1)" fillcolor=orange]
	1410754252320 -> 1401320493552 [dir=none]
	1401320493552 [label="v
 (144, 514, 1, 1)" fillcolor=orange]
	1410754252320 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724977520 -> 1410754252320
	1401320493552 [label="ul_modules.0.in_proj_kv.1.weight_v
 (144, 514, 1, 1)" fillcolor=lightblue]
	1401320493552 -> 1410724977520
	1410724977520 [label=AccumulateGrad]
	1410724976512 -> 1410754252320
	1401320492688 [label="ul_modules.0.in_proj_kv.1.weight_g
 (144, 1, 1, 1)" fillcolor=lightblue]
	1401320492688 -> 1410724976512
	1410724976512 [label=AccumulateGrad]
	1410724972912 -> 1410754252272
	1401320493360 [label="ul_modules.0.in_proj_kv.1.bias
 (144)" fillcolor=lightblue]
	1401320493360 -> 1410724972912
	1410724972912 [label=AccumulateGrad]
	1410754250496 -> 1410754250448
	1410754250496 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 128)"]
	1410754250832 -> 1410754250496
	1410754250832 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 128)"]
	1410754251024 -> 1410754250832
	1410754251024 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410754251360 -> 1410754251024
	1410754251360 [label="ViewBackward0
-----------------------------------
self_sym_sizes: (1, 1, 128, 28, 28)"]
	1410754251744 -> 1410754251360
	1410754251744 [label="ViewBackward0
--------------------------------
self_sym_sizes: (1, 128, 28, 28)"]
	1410754252224 -> 1410754251744
	1410754250064 -> 1410754249728
	1410754250064 [label=NegBackward0]
	1410754250112 -> 1410754250064
	1410754249776 -> 1410754249488
	1410754249776 -> 1401320495568 [dir=none]
	1401320495568 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754249776 -> 1410754303984 [dir=none]
	1410754303984 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754249776 -> 1401320495760 [dir=none]
	1401320495760 [label="v
 (256, 256, 1, 1)" fillcolor=orange]
	1410754249776 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724837264 -> 1410754249776
	1401320495760 [label="ul_modules.0.out_proj.conv_1_sc.weight_v
 (256, 256, 1, 1)" fillcolor=lightblue]
	1401320495760 -> 1410724837264
	1410724837264 [label=AccumulateGrad]
	1410724838416 -> 1410754249776
	1401320495568 [label="ul_modules.0.out_proj.conv_1_sc.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320495568 -> 1410724838416
	1410724838416 [label=AccumulateGrad]
	1410724838560 -> 1410754249488
	1401320495472 [label="ul_modules.0.out_proj.conv_1_sc.bias
 (256)" fillcolor=lightblue]
	1401320495472 -> 1410724838560
	1410724838560 [label=AccumulateGrad]
	1410754249344 -> 1410754249392
	1410754249344 [label=NegBackward0]
	1410754249008 -> 1410754249344
	1410753901232 -> 1410753904448
	1410753901232 -> 1401320496048 [dir=none]
	1401320496048 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410753901232 -> 1410754305616 [dir=none]
	1410754305616 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410753901232 -> 1401320496240 [dir=none]
	1401320496240 [label="v
 (512, 512, 1, 1)" fillcolor=orange]
	1410753901232 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724823488 -> 1410753901232
	1401320496240 [label="ul_modules.0.out_proj.conv_2.weight_v
 (512, 512, 1, 1)" fillcolor=lightblue]
	1401320496240 -> 1410724823488
	1410724823488 [label=AccumulateGrad]
	1410724837552 -> 1410753901232
	1401320496048 [label="ul_modules.0.out_proj.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401320496048 -> 1410724837552
	1410724837552 [label=AccumulateGrad]
	1410724824544 -> 1410753904448
	1401320495952 [label="ul_modules.0.out_proj.conv_2.bias
 (512)" fillcolor=lightblue]
	1401320495952 -> 1410724824544
	1410724824544 [label=AccumulateGrad]
	1410753899984 -> 1410753899744
	1410753899984 -> 1410754306768 [dir=none]
	1410754306768 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410753899984 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753904256 -> 1410753899984
	1410753899600 -> 1410753899504
	1410753899600 -> 1410753963376 [dir=none]
	1410753963376 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753899600 -> 1410753958480 [dir=none]
	1410753958480 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753899600 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753901328 -> 1410753899600
	1410753901328 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754249248 -> 1410753901328
	1410754249248 -> 1410753961456 [dir=none]
	1410753961456 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754249248 -> 1410753954640 [dir=none]
	1410753954640 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754249248 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754249968 -> 1410754249248
	1410754249968 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754250016 -> 1410754249968
	1410754250016 -> 1410754307824 [dir=none]
	1410754307824 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754250016 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754250304 -> 1410754250016
	1410754250304 -> 1410753961744 [dir=none]
	1410753961744 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754250304 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754250160 -> 1410754250304
	1410754250160 [label="CatBackward0
------------
dim: 1"]
	1410754250928 -> 1410754250160
	1410754250928 -> 1410753962032 [dir=none]
	1410753962032 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754250928 -> 1410753962224 [dir=none]
	1410753962224 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754250928 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754252656 -> 1410754250928
	1410754252656 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754251264 -> 1410754252656
	1410754251264 -> 1410753962320 [dir=none]
	1410753962320 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754251264 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754253472 -> 1410754251264
	1410754253472 [label="CatBackward0
------------
dim: 1"]
	1410753899648 -> 1410754253472
	1410754253232 -> 1410754253472
	1410754253232 [label=NegBackward0]
	1410753899648 -> 1410754253232
	1410754251552 -> 1410754250928
	1410754251552 -> 1401320495856 [dir=none]
	1401320495856 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754251552 -> 1410754309744 [dir=none]
	1410754309744 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754251552 -> 1401320496816 [dir=none]
	1401320496816 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754251552 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724977904 -> 1410754251552
	1401320496816 [label="ul_modules.1.input_gated_resnet.0.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401320496816 -> 1410724977904
	1410724977904 [label=AccumulateGrad]
	1410724977568 -> 1410754251552
	1401320495856 [label="ul_modules.1.input_gated_resnet.0.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320495856 -> 1410724977568
	1410724977568 [label=AccumulateGrad]
	1410724970608 -> 1410754250928
	1401320496624 [label="ul_modules.1.input_gated_resnet.0.conv_1.bias
 (256)" fillcolor=lightblue]
	1401320496624 -> 1410724970608
	1410724970608 [label=AccumulateGrad]
	1410754250736 -> 1410754250160
	1410754250736 [label=NegBackward0]
	1410754250928 -> 1410754250736
	1410754249152 -> 1410754249248
	1410754249152 -> 1401320496528 [dir=none]
	1401320496528 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754249152 -> 1410754311376 [dir=none]
	1410754311376 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754249152 -> 1401320497200 [dir=none]
	1401320497200 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754249152 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724839088 -> 1410754249152
	1401320497200 [label="ul_modules.1.input_gated_resnet.0.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401320497200 -> 1410724839088
	1410724839088 [label=AccumulateGrad]
	1410724973248 -> 1410754249152
	1401320496528 [label="ul_modules.1.input_gated_resnet.0.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401320496528 -> 1410724973248
	1410724973248 [label=AccumulateGrad]
	1410724823584 -> 1410754249248
	1401320497008 [label="ul_modules.1.input_gated_resnet.0.conv_2.bias
 (512)" fillcolor=lightblue]
	1401320497008 -> 1410724823584
	1410724823584 [label=AccumulateGrad]
	1410753899840 -> 1410753899600
	1410753899840 -> 1410754312528 [dir=none]
	1410754312528 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410753899840 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753901328 -> 1410753899840
	1410753899456 -> 1410753899360
	1410753899456 -> 1410753964528 [dir=none]
	1410753964528 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753899456 -> 1410753964336 [dir=none]
	1410753964336 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753899456 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753899696 -> 1410753899456
	1410753899696 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754250400 -> 1410753899696
	1410754250400 -> 1410753964144 [dir=none]
	1410753964144 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754250400 -> 1410753959344 [dir=none]
	1410753959344 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754250400 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754249584 -> 1410754250400
	1410754249584 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754250592 -> 1410754249584
	1410754250592 -> 1410754313584 [dir=none]
	1410754313584 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754250592 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754253904 -> 1410754250592
	1410754253904 -> 1410753964048 [dir=none]
	1410753964048 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754253904 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754251984 -> 1410754253904
	1410754251984 [label="CatBackward0
------------
dim: 1"]
	1410754254528 -> 1410754251984
	1410754254528 -> 1410753963856 [dir=none]
	1410753963856 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754254528 -> 1410753963568 [dir=none]
	1410753963568 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754254528 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754254672 -> 1410754254528
	1410754254672 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754254720 -> 1410754254672
	1410754254720 -> 1410753961840 [dir=none]
	1410753961840 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754254720 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754254816 -> 1410754254720
	1410754254816 [label="CatBackward0
------------
dim: 1"]
	1410753899504 -> 1410754254816
	1410754254912 -> 1410754254816
	1410754254912 [label=NegBackward0]
	1410753899504 -> 1410754254912
	1410754254144 -> 1410754254528
	1410754254144 -> 1401320496912 [dir=none]
	1401320496912 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754254144 -> 1410754315568 [dir=none]
	1410754315568 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754254144 -> 1401320497776 [dir=none]
	1401320497776 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754254144 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724978864 -> 1410754254144
	1401320497776 [label="ul_modules.1.input_gated_resnet.1.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401320497776 -> 1410724978864
	1410724978864 [label=AccumulateGrad]
	1410724978768 -> 1410754254144
	1401320496912 [label="ul_modules.1.input_gated_resnet.1.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320496912 -> 1410724978768
	1410724978768 [label=AccumulateGrad]
	1410724978432 -> 1410754254528
	1401320497584 [label="ul_modules.1.input_gated_resnet.1.conv_1.bias
 (256)" fillcolor=lightblue]
	1401320497584 -> 1410724978432
	1410724978432 [label=AccumulateGrad]
	1410754253760 -> 1410754251984
	1410754253760 [label=NegBackward0]
	1410754254528 -> 1410754253760
	1410754250208 -> 1410754250400
	1410754250208 -> 1401320497488 [dir=none]
	1401320497488 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754250208 -> 1410754317200 [dir=none]
	1410754317200 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754250208 -> 1401320498160 [dir=none]
	1401320498160 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754250208 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724978240 -> 1410754250208
	1401320498160 [label="ul_modules.1.input_gated_resnet.1.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401320498160 -> 1410724978240
	1410724978240 [label=AccumulateGrad]
	1410724978144 -> 1410754250208
	1401320497488 [label="ul_modules.1.input_gated_resnet.1.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401320497488 -> 1410724978144
	1410724978144 [label=AccumulateGrad]
	1410724978048 -> 1410754250400
	1401320497968 [label="ul_modules.1.input_gated_resnet.1.conv_2.bias
 (512)" fillcolor=lightblue]
	1401320497968 -> 1410724978048
	1410724978048 [label=AccumulateGrad]
	1410753899552 -> 1410753899456
	1410753899552 -> 1410754318352 [dir=none]
	1410754318352 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410753899552 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753899696 -> 1410753899552
	1410753899312 -> 1410753899216
	1410753899312 -> 1410753965392 [dir=none]
	1410753965392 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753899312 -> 1410753965200 [dir=none]
	1410753965200 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753899312 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753899408 -> 1410753899312
	1410753899408 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754253808 -> 1410753899408
	1410754253808 -> 1410753965008 [dir=none]
	1410753965008 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754253808 -> 1410753963760 [dir=none]
	1410753963760 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754253808 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754252608 -> 1410754253808
	1410754252608 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754254432 -> 1410754252608
	1410754254432 -> 1410754319408 [dir=none]
	1410754319408 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754254432 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754255008 -> 1410754254432
	1410754255008 -> 1410753964912 [dir=none]
	1410753964912 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754255008 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754254624 -> 1410754255008
	1410754254624 [label="CatBackward0
------------
dim: 1"]
	1410754255104 -> 1410754254624
	1410754255104 -> 1410753964624 [dir=none]
	1410753964624 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754255104 -> 1410753963952 [dir=none]
	1410753963952 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754255104 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754255248 -> 1410754255104
	1410754255248 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754255392 -> 1410754255248
	1410754255392 -> 1410753964432 [dir=none]
	1410753964432 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754255392 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754255488 -> 1410754255392
	1410754255488 [label="CatBackward0
------------
dim: 1"]
	1410753899360 -> 1410754255488
	1410754255584 -> 1410754255488
	1410754255584 [label=NegBackward0]
	1410753899360 -> 1410754255584
	1410754255200 -> 1410754255104
	1410754255200 -> 1401320497872 [dir=none]
	1401320497872 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754255200 -> 1410754321328 [dir=none]
	1410754321328 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754255200 -> 1401320498736 [dir=none]
	1401320498736 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754255200 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724979824 -> 1410754255200
	1401320498736 [label="ul_modules.1.input_gated_resnet.2.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401320498736 -> 1410724979824
	1410724979824 [label=AccumulateGrad]
	1410724979728 -> 1410754255200
	1401320497872 [label="ul_modules.1.input_gated_resnet.2.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320497872 -> 1410724979728
	1410724979728 [label=AccumulateGrad]
	1410724979392 -> 1410754255104
	1401320498544 [label="ul_modules.1.input_gated_resnet.2.conv_1.bias
 (256)" fillcolor=lightblue]
	1401320498544 -> 1410724979392
	1410724979392 [label=AccumulateGrad]
	1410754255056 -> 1410754254624
	1410754255056 [label=NegBackward0]
	1410754255104 -> 1410754255056
	1410754253568 -> 1410754253808
	1410754253568 -> 1401320498448 [dir=none]
	1401320498448 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754253568 -> 1410754322960 [dir=none]
	1410754322960 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754253568 -> 1401320499120 [dir=none]
	1401320499120 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754253568 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724979200 -> 1410754253568
	1401320499120 [label="ul_modules.1.input_gated_resnet.2.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401320499120 -> 1410724979200
	1410724979200 [label=AccumulateGrad]
	1410724979104 -> 1410754253568
	1401320498448 [label="ul_modules.1.input_gated_resnet.2.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401320498448 -> 1410724979104
	1410724979104 [label=AccumulateGrad]
	1410724970800 -> 1410754253808
	1401320498928 [label="ul_modules.1.input_gated_resnet.2.conv_2.bias
 (512)" fillcolor=lightblue]
	1401320498928 -> 1410724970800
	1410724970800 [label=AccumulateGrad]
	1410754250640 -> 1410753899312
	1410754250640 -> 1410754324112 [dir=none]
	1410754324112 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754250640 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753899408 -> 1410754250640
	1410753899168 -> 1410753899072
	1410753899168 -> 1410753966256 [dir=none]
	1410753966256 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753899168 -> 1410753966064 [dir=none]
	1410753966064 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753899168 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753899264 -> 1410753899168
	1410753899264 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754254864 -> 1410753899264
	1410754254864 -> 1410753965872 [dir=none]
	1410753965872 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754254864 -> 1410753964720 [dir=none]
	1410753964720 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754254864 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754254768 -> 1410754254864
	1410754254768 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754255152 -> 1410754254768
	1410754255152 -> 1410754325168 [dir=none]
	1410754325168 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754255152 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754255680 -> 1410754255152
	1410754255680 -> 1410753965776 [dir=none]
	1410753965776 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754255680 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754255296 -> 1410754255680
	1410754255296 [label="CatBackward0
------------
dim: 1"]
	1410754255776 -> 1410754255296
	1410754255776 -> 1410753965488 [dir=none]
	1410753965488 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754255776 -> 1410753964816 [dir=none]
	1410753964816 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754255776 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754255920 -> 1410754255776
	1410754255920 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754256064 -> 1410754255920
	1410754256064 -> 1410753965296 [dir=none]
	1410753965296 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754256064 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754256160 -> 1410754256064
	1410754256160 [label="CatBackward0
------------
dim: 1"]
	1410753899216 -> 1410754256160
	1410754256256 -> 1410754256160
	1410754256256 [label=NegBackward0]
	1410753899216 -> 1410754256256
	1410754255872 -> 1410754255776
	1410754255872 -> 1401320498832 [dir=none]
	1401320498832 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754255872 -> 1410754327088 [dir=none]
	1410754327088 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754255872 -> 1401320499696 [dir=none]
	1401320499696 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754255872 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724980784 -> 1410754255872
	1401320499696 [label="ul_modules.1.input_gated_resnet.3.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401320499696 -> 1410724980784
	1410724980784 [label=AccumulateGrad]
	1410724980688 -> 1410754255872
	1401320498832 [label="ul_modules.1.input_gated_resnet.3.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320498832 -> 1410724980688
	1410724980688 [label=AccumulateGrad]
	1410724980352 -> 1410754255776
	1401320499504 [label="ul_modules.1.input_gated_resnet.3.conv_1.bias
 (256)" fillcolor=lightblue]
	1401320499504 -> 1410724980352
	1410724980352 [label=AccumulateGrad]
	1410754255728 -> 1410754255296
	1410754255728 [label=NegBackward0]
	1410754255776 -> 1410754255728
	1410754254960 -> 1410754254864
	1410754254960 -> 1401320499408 [dir=none]
	1401320499408 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754254960 -> 1410754328720 [dir=none]
	1410754328720 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754254960 -> 1401320500080 [dir=none]
	1401320500080 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754254960 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724980160 -> 1410754254960
	1401320500080 [label="ul_modules.1.input_gated_resnet.3.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401320500080 -> 1410724980160
	1410724980160 [label=AccumulateGrad]
	1410724980064 -> 1410754254960
	1401320499408 [label="ul_modules.1.input_gated_resnet.3.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401320499408 -> 1410724980064
	1410724980064 [label=AccumulateGrad]
	1410724978576 -> 1410754254864
	1401320499888 [label="ul_modules.1.input_gated_resnet.3.conv_2.bias
 (512)" fillcolor=lightblue]
	1401320499888 -> 1410724978576
	1410724978576 [label=AccumulateGrad]
	1410754254384 -> 1410753899168
	1410754254384 -> 1410754329872 [dir=none]
	1410754329872 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754254384 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753899264 -> 1410754254384
	1410753899024 -> 1410753898928
	1410753899024 -> 1410753967120 [dir=none]
	1410753967120 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753899024 -> 1410753966928 [dir=none]
	1410753966928 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753899024 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753899120 -> 1410753899024
	1410753899120 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754255536 -> 1410753899120
	1410754255536 -> 1410753966736 [dir=none]
	1410753966736 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754255536 -> 1410753965584 [dir=none]
	1410753965584 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754255536 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754255440 -> 1410754255536
	1410754255440 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754255824 -> 1410754255440
	1410754255824 -> 1410754347376 [dir=none]
	1410754347376 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754255824 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754256352 -> 1410754255824
	1410754256352 -> 1410753966640 [dir=none]
	1410753966640 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754256352 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754255968 -> 1410754256352
	1410754255968 [label="CatBackward0
------------
dim: 1"]
	1410754256448 -> 1410754255968
	1410754256448 -> 1410753966352 [dir=none]
	1410753966352 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754256448 -> 1410753965680 [dir=none]
	1410753965680 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754256448 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754256592 -> 1410754256448
	1410754256592 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754256736 -> 1410754256592
	1410754256736 -> 1410753966160 [dir=none]
	1410753966160 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754256736 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754256832 -> 1410754256736
	1410754256832 [label="CatBackward0
------------
dim: 1"]
	1410753899072 -> 1410754256832
	1410754256928 -> 1410754256832
	1410754256928 [label=NegBackward0]
	1410753899072 -> 1410754256928
	1410754256544 -> 1410754256448
	1410754256544 -> 1401320499792 [dir=none]
	1401320499792 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754256544 -> 1410754349296 [dir=none]
	1410754349296 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754256544 -> 1401320500656 [dir=none]
	1401320500656 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754256544 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724981744 -> 1410754256544
	1401320500656 [label="ul_modules.1.input_gated_resnet.4.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401320500656 -> 1410724981744
	1410724981744 [label=AccumulateGrad]
	1410724981648 -> 1410754256544
	1401320499792 [label="ul_modules.1.input_gated_resnet.4.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320499792 -> 1410724981648
	1410724981648 [label=AccumulateGrad]
	1410724981312 -> 1410754256448
	1401320500464 [label="ul_modules.1.input_gated_resnet.4.conv_1.bias
 (256)" fillcolor=lightblue]
	1401320500464 -> 1410724981312
	1410724981312 [label=AccumulateGrad]
	1410754256400 -> 1410754255968
	1410754256400 [label=NegBackward0]
	1410754256448 -> 1410754256400
	1410754255632 -> 1410754255536
	1410754255632 -> 1401320500368 [dir=none]
	1401320500368 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754255632 -> 1410754350928 [dir=none]
	1410754350928 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754255632 -> 1401320501040 [dir=none]
	1401320501040 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754255632 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724981120 -> 1410754255632
	1401320501040 [label="ul_modules.1.input_gated_resnet.4.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401320501040 -> 1410724981120
	1410724981120 [label=AccumulateGrad]
	1410724981024 -> 1410754255632
	1401320500368 [label="ul_modules.1.input_gated_resnet.4.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401320500368 -> 1410724981024
	1410724981024 [label=AccumulateGrad]
	1410724979536 -> 1410754255536
	1401320500848 [label="ul_modules.1.input_gated_resnet.4.conv_2.bias
 (512)" fillcolor=lightblue]
	1401320500848 -> 1410724979536
	1410724979536 [label=AccumulateGrad]
	1410754255344 -> 1410753899024
	1410754255344 -> 1410754352080 [dir=none]
	1410754352080 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754255344 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753899120 -> 1410754255344
	1410753898880 -> 1410753898784
	1410753898880 -> 1410753938736 [dir=none]
	1410753938736 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753898880 -> 1410753938544 [dir=none]
	1410753938544 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753898880 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753898976 -> 1410753898880
	1410753898976 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754256208 -> 1410753898976
	1410754256208 -> 1410753938352 [dir=none]
	1410753938352 [label="input
 (1, 512, 28, 28)" fillcolor=orange]
	1410754256208 -> 1410753937872 [dir=none]
	1410753937872 [label="weight
 (512, 512, 1, 1)" fillcolor=orange]
	1410754256208 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754256112 -> 1410754256208
	1410754256112 -> 1410754353136 [dir=none]
	1410754353136 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754256112 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754256496 -> 1410754256112
	1410754256496 -> 1410753938256 [dir=none]
	1410753938256 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754256496 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754257024 -> 1410754256496
	1410754257024 [label="CatBackward0
------------
dim: 1"]
	1410754256640 -> 1410754257024
	1410754256640 [label="AddBackward0
------------
alpha: 1"]
	1410754257168 -> 1410754256640
	1410754257168 -> 1410753970096 [dir=none]
	1410753970096 [label="input
 (1, 512, 28, 28)" fillcolor=orange]
	1410754257168 -> 1410753937776 [dir=none]
	1410753937776 [label="weight
 (256, 512, 1, 1)" fillcolor=orange]
	1410754257168 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754257312 -> 1410754257168
	1410754257312 -> 1410753937680 [dir=none]
	1410753937680 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754257312 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754257456 -> 1410754257312
	1410754257456 [label="CatBackward0
------------
dim: 1"]
	1410753898928 -> 1410754257456
	1410754257552 -> 1410754257456
	1410754257552 [label=NegBackward0]
	1410753898928 -> 1410754257552
	1410754257264 -> 1410754257168
	1410754257264 -> 1401320503728 [dir=none]
	1401320503728 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754257264 -> 1410754354960 [dir=none]
	1410754354960 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754257264 -> 1401320504400 [dir=none]
	1401320504400 [label="v
 (256, 512, 1, 1)" fillcolor=orange]
	1410754257264 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724982800 -> 1410754257264
	1401320504400 [label="ul_modules.1.out_proj.conv_1.weight_v
 (256, 512, 1, 1)" fillcolor=lightblue]
	1401320504400 -> 1410724982800
	1410724982800 [label=AccumulateGrad]
	1410724982656 -> 1410754257264
	1401320503728 [label="ul_modules.1.out_proj.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320503728 -> 1410724982656
	1410724982656 [label=AccumulateGrad]
	1410724982320 -> 1410754257168
	1401320504208 [label="ul_modules.1.out_proj.conv_1.bias
 (256)" fillcolor=lightblue]
	1401320504208 -> 1410724982320
	1410724982320 [label=AccumulateGrad]
	1410754257120 -> 1410754256640
	1410754257120 -> 1410753937968 [dir=none]
	1410753937968 [label="input
 (1, 256, 28, 28)" fillcolor=orange]
	1410754257120 -> 1410753938160 [dir=none]
	1410753938160 [label="weight
 (256, 256, 1, 1)" fillcolor=orange]
	1410754257120 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754257504 -> 1410754257120
	1410754257504 -> 1410753938064 [dir=none]
	1410753938064 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410754257504 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754257360 -> 1410754257504
	1410754257360 [label="CatBackward0
------------
dim: 1"]
	1410754257744 -> 1410754257360
	1410754257744 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (1, 1, 128, 784)"]
	1410754257888 -> 1410754257744
	1410754257888 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410754257984 -> 1410754257888
	1410754257984 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (1, 784, 128)"]
	1410754258080 -> 1410754257984
	1410754258080 -> 1410754356976 [dir=none]
	1410754356976 [label="mat2
 (1, 784, 128)" fillcolor=orange]
	1410754258080 -> 1410754357264 [dir=none]
	1410754357264 [label="self
 (1, 784, 784)" fillcolor=orange]
	1410754258080 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	1410754258176 -> 1410754258080
	1410754258176 [label="ViewBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 784)"]
	1410754258320 -> 1410754258176
	1410754258320 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 784)"]
	1410754258416 -> 1410754258320
	1410754258416 -> 1410754357552 [dir=none]
	1410754357552 [label="result
 (1, 1, 784, 784)" fillcolor=orange]
	1410754258416 [label="SoftmaxBackward0
----------------------
dim   :     4294967295
result: [saved tensor]"]
	1410754258512 -> 1410754258416
	1410754258512 -> 1410753969616 [dir=none]
	1410753969616 [label="mask
 (1, 1, 784, 784)" fillcolor=orange]
	1410754258512 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	1410754258608 -> 1410754258512
	1410754258608 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (1, 784, 784)"]
	1410754258704 -> 1410754258608
	1410754258704 -> 1410754358224 [dir=none]
	1410754358224 [label="mat2
 (1, 16, 784)" fillcolor=orange]
	1410754258704 -> 1410754358512 [dir=none]
	1410754358512 [label="self
 (1, 784, 16)" fillcolor=orange]
	1410754258704 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	1410754258800 -> 1410754258704
	1410754258800 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (1, 1, 784, 16)"]
	1410754258944 -> 1410754258800
	1410754258944 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 784, 16)"]
	1410754259040 -> 1410754258944
	1410754259040 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410754259136 -> 1410754259040
	1410754259136 -> 1410754358896 [dir=none]
	1410754358896 [label="other
 ()" fillcolor=orange]
	1410754259136 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	1410754259232 -> 1410754259136
	1410754259232 [label="ViewBackward0
----------------------------------
self_sym_sizes: (1, 1, 16, 28, 28)"]
	1410754259328 -> 1410754259232
	1410754259328 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 16, 28, 28)"]
	1410754259424 -> 1410754259328
	1410754259424 -> 1410753969520 [dir=none]
	1410753969520 [label="input
 (1, 258, 28, 28)" fillcolor=orange]
	1410754259424 -> 1410753969232 [dir=none]
	1410753969232 [label="weight
 (16, 258, 1, 1)" fillcolor=orange]
	1410754259424 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754259520 -> 1410754259424
	1410754259520 [label="AddBackward0
------------
alpha: 1"]
	1410754259664 -> 1410754259520
	1410754259664 [label="CatBackward0
------------
dim: 1"]
	1410753898928 -> 1410754259664
	1410754259616 -> 1410754259520
	1410754259616 -> 1410753969328 [dir=none]
	1410753969328 [label="other
 (1, 258, 28, 28)" fillcolor=orange]
	1410754259616 -> 1410753969136 [dir=none]
	1410753969136 [label="self
 (1, 258, 28, 28)" fillcolor=orange]
	1410754259616 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410754259712 -> 1410754259616
	1410754259712 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 516, 28, 28)
split_size    :              258"]
	1410754259952 -> 1410754259712
	1410754259952 -> 1410753968752 [dir=none]
	1410753968752 [label="input
 (1, 516, 28, 28)" fillcolor=orange]
	1410754259952 -> 1410753968848 [dir=none]
	1410753968848 [label="weight
 (516, 516, 1, 1)" fillcolor=orange]
	1410754259952 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (516,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754260048 -> 1410754259952
	1410754260048 -> 1410754360720 [dir=none]
	1410754360720 [label="result1
 (1, 516, 28, 28)" fillcolor=orange]
	1410754260048 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754260192 -> 1410754260048
	1410754260192 -> 1410753968944 [dir=none]
	1410753968944 [label="self
 (1, 516, 28, 28)" fillcolor=orange]
	1410754260192 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754260288 -> 1410754260192
	1410754260288 [label="CatBackward0
------------
dim: 1"]
	1410754260384 -> 1410754260288
	1410754260384 -> 1410753968464 [dir=none]
	1410753968464 [label="input
 (1, 516, 28, 28)" fillcolor=orange]
	1410754260384 -> 1410753968656 [dir=none]
	1410753968656 [label="weight
 (258, 516, 1, 1)" fillcolor=orange]
	1410754260384 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (258,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754260528 -> 1410754260384
	1410754260528 -> 1410753968560 [dir=none]
	1410753968560 [label="self
 (1, 516, 28, 28)" fillcolor=orange]
	1410754260528 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754260672 -> 1410754260528
	1410754260672 [label="CatBackward0
------------
dim: 1"]
	1410754259664 -> 1410754260672
	1410754260768 -> 1410754260672
	1410754260768 [label=NegBackward0]
	1410754259664 -> 1410754260768
	1410754260480 -> 1410754260384
	1410754260480 -> 1401320502288 [dir=none]
	1401320502288 [label="g
 (258, 1, 1, 1)" fillcolor=orange]
	1410754260480 -> 1410754362448 [dir=none]
	1410754362448 [label="result1
 (258, 1, 1, 1)" fillcolor=orange]
	1410754260480 -> 1401320502960 [dir=none]
	1401320502960 [label="v
 (258, 516, 1, 1)" fillcolor=orange]
	1410754260480 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724986304 -> 1410754260480
	1401320502960 [label="ul_modules.1.in_proj_q.0.conv_1.weight_v
 (258, 516, 1, 1)" fillcolor=lightblue]
	1401320502960 -> 1410724986304
	1410724986304 [label=AccumulateGrad]
	1410724986160 -> 1410754260480
	1401320502288 [label="ul_modules.1.in_proj_q.0.conv_1.weight_g
 (258, 1, 1, 1)" fillcolor=lightblue]
	1401320502288 -> 1410724986160
	1410724986160 [label=AccumulateGrad]
	1410724985824 -> 1410754260384
	1401320502768 [label="ul_modules.1.in_proj_q.0.conv_1.bias
 (258)" fillcolor=lightblue]
	1401320502768 -> 1410724985824
	1410724985824 [label=AccumulateGrad]
	1410754260336 -> 1410754260288
	1410754260336 [label=NegBackward0]
	1410754260384 -> 1410754260336
	1410754260000 -> 1410754259952
	1410754260000 -> 1401320503248 [dir=none]
	1401320503248 [label="g
 (516, 1, 1, 1)" fillcolor=orange]
	1410754260000 -> 1410754396912 [dir=none]
	1410754396912 [label="result1
 (516, 1, 1, 1)" fillcolor=orange]
	1410754260000 -> 1401320503440 [dir=none]
	1401320503440 [label="v
 (516, 516, 1, 1)" fillcolor=orange]
	1410754260000 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724985488 -> 1410754260000
	1401320503440 [label="ul_modules.1.in_proj_q.0.conv_2.weight_v
 (516, 516, 1, 1)" fillcolor=lightblue]
	1401320503440 -> 1410724985488
	1410724985488 [label=AccumulateGrad]
	1410724985632 -> 1410754260000
	1401320503248 [label="ul_modules.1.in_proj_q.0.conv_2.weight_g
 (516, 1, 1, 1)" fillcolor=lightblue]
	1401320503248 -> 1410724985632
	1410724985632 [label=AccumulateGrad]
	1410724985200 -> 1410754259952
	1401320503152 [label="ul_modules.1.in_proj_q.0.conv_2.bias
 (516)" fillcolor=lightblue]
	1401320503152 -> 1410724985200
	1410724985200 [label=AccumulateGrad]
	1410754259760 -> 1410754259616
	1410754259760 -> 1410754398064 [dir=none]
	1410754398064 [label="result
 (1, 258, 28, 28)" fillcolor=orange]
	1410754259760 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410754259712 -> 1410754259760
	1410754259472 -> 1410754259424
	1410754259472 -> 1401320503056 [dir=none]
	1401320503056 [label="g
 (16, 1, 1, 1)" fillcolor=orange]
	1410754259472 -> 1410754398640 [dir=none]
	1410754398640 [label="result1
 (16, 1, 1, 1)" fillcolor=orange]
	1410754259472 -> 1401320504016 [dir=none]
	1401320504016 [label="v
 (16, 258, 1, 1)" fillcolor=orange]
	1410754259472 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724986016 -> 1410754259472
	1401320504016 [label="ul_modules.1.in_proj_q.1.weight_v
 (16, 258, 1, 1)" fillcolor=lightblue]
	1401320504016 -> 1410724986016
	1410724986016 [label=AccumulateGrad]
	1410724985152 -> 1410754259472
	1401320503056 [label="ul_modules.1.in_proj_q.1.weight_g
 (16, 1, 1, 1)" fillcolor=lightblue]
	1401320503056 -> 1410724985152
	1410724985152 [label=AccumulateGrad]
	1410724984144 -> 1410754259424
	1401320503824 [label="ul_modules.1.in_proj_q.1.bias
 (16)" fillcolor=lightblue]
	1401320503824 -> 1410724984144
	1410724984144 [label=AccumulateGrad]
	1410754258752 -> 1410754258704
	1410754258752 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 1, 16, 784)"]
	1410754259088 -> 1410754258752
	1410754259088 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 16, 784)"]
	1410754259280 -> 1410754259088
	1410754259280 [label="ViewBackward0
----------------------------------
self_sym_sizes: (1, 1, 16, 28, 28)"]
	1410754258848 -> 1410754259280
	1410754258848 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 16, 28, 28)"]
	1410754259808 -> 1410754258848
	1410754259808 [label="SplitWithSizesBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 144, 28, 28)
split_sizes   :        (16, 128)"]
	1410754259856 -> 1410754259808
	1410754259856 -> 1410753968176 [dir=none]
	1410753968176 [label="input
 (1, 514, 28, 28)" fillcolor=orange]
	1410754259856 -> 1410753967888 [dir=none]
	1410753967888 [label="weight
 (144, 514, 1, 1)" fillcolor=orange]
	1410754259856 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (144,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754260240 -> 1410754259856
	1410754260240 [label="AddBackward0
------------
alpha: 1"]
	1410754260624 -> 1410754260240
	1410754260624 [label="CatBackward0
------------
dim: 1"]
	1410753899648 -> 1410754260624
	1410753898928 -> 1410754260624
	1410754260720 -> 1410754260240
	1410754260720 -> 1410753967984 [dir=none]
	1410753967984 [label="other
 (1, 514, 28, 28)" fillcolor=orange]
	1410754260720 -> 1410753967792 [dir=none]
	1410753967792 [label="self
 (1, 514, 28, 28)" fillcolor=orange]
	1410754260720 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410754260432 -> 1410754260720
	1410754260432 [label="SplitBackward0
---------------------------------
dim           :                 1
self_sym_sizes: (1, 1028, 28, 28)
split_size    :               514"]
	1410754260960 -> 1410754260432
	1410754260960 -> 1410753967408 [dir=none]
	1410753967408 [label="input
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754260960 -> 1410753967504 [dir=none]
	1410753967504 [label="weight
 (1028, 1028, 1, 1)" fillcolor=orange]
	1410754260960 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1028,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754261056 -> 1410754260960
	1410754261056 -> 1410754401616 [dir=none]
	1410754401616 [label="result1
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754261056 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754261200 -> 1410754261056
	1410754261200 -> 1410753967600 [dir=none]
	1410753967600 [label="self
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754261200 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754261296 -> 1410754261200
	1410754261296 [label="CatBackward0
------------
dim: 1"]
	1410754261392 -> 1410754261296
	1410754261392 -> 1410753967024 [dir=none]
	1410753967024 [label="input
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754261392 -> 1410753967216 [dir=none]
	1410753967216 [label="weight
 (514, 1028, 1, 1)" fillcolor=orange]
	1410754261392 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (514,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754261536 -> 1410754261392
	1410754261536 -> 1410753966544 [dir=none]
	1410753966544 [label="self
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754261536 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754261680 -> 1410754261536
	1410754261680 [label="CatBackward0
------------
dim: 1"]
	1410754260624 -> 1410754261680
	1410754261776 -> 1410754261680
	1410754261776 [label=NegBackward0]
	1410754260624 -> 1410754261776
	1410754261488 -> 1410754261392
	1410754261488 -> 1401320500752 [dir=none]
	1401320500752 [label="g
 (514, 1, 1, 1)" fillcolor=orange]
	1410754261488 -> 1410754403344 [dir=none]
	1410754403344 [label="result1
 (514, 1, 1, 1)" fillcolor=orange]
	1410754261488 -> 1401320501616 [dir=none]
	1401320501616 [label="v
 (514, 1028, 1, 1)" fillcolor=orange]
	1410754261488 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725200800 -> 1410754261488
	1401320501616 [label="ul_modules.1.in_proj_kv.0.conv_1.weight_v
 (514, 1028, 1, 1)" fillcolor=lightblue]
	1401320501616 -> 1410725200800
	1410725200800 [label=AccumulateGrad]
	1410725200656 -> 1410754261488
	1401320500752 [label="ul_modules.1.in_proj_kv.0.conv_1.weight_g
 (514, 1, 1, 1)" fillcolor=lightblue]
	1401320500752 -> 1410725200656
	1410725200656 [label=AccumulateGrad]
	1410725200320 -> 1410754261392
	1401320501424 [label="ul_modules.1.in_proj_kv.0.conv_1.bias
 (514)" fillcolor=lightblue]
	1401320501424 -> 1410725200320
	1410725200320 [label=AccumulateGrad]
	1410754261344 -> 1410754261296
	1410754261344 [label=NegBackward0]
	1410754261392 -> 1410754261344
	1410754261008 -> 1410754260960
	1410754261008 -> 1401320501328 [dir=none]
	1401320501328 [label="g
 (1028, 1, 1, 1)" fillcolor=orange]
	1410754261008 -> 1410754404976 [dir=none]
	1410754404976 [label="result1
 (1028, 1, 1, 1)" fillcolor=orange]
	1410754261008 -> 1401320502000 [dir=none]
	1401320502000 [label="v
 (1028, 1028, 1, 1)" fillcolor=orange]
	1410754261008 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725199984 -> 1410754261008
	1401320502000 [label="ul_modules.1.in_proj_kv.0.conv_2.weight_v
 (1028, 1028, 1, 1)" fillcolor=lightblue]
	1401320502000 -> 1410725199984
	1410725199984 [label=AccumulateGrad]
	1410725200128 -> 1410754261008
	1401320501328 [label="ul_modules.1.in_proj_kv.0.conv_2.weight_g
 (1028, 1, 1, 1)" fillcolor=lightblue]
	1401320501328 -> 1410725200128
	1410725200128 [label=AccumulateGrad]
	1410724986640 -> 1410754260960
	1401320501808 [label="ul_modules.1.in_proj_kv.0.conv_2.bias
 (1028)" fillcolor=lightblue]
	1401320501808 -> 1410724986640
	1410724986640 [label=AccumulateGrad]
	1410754260864 -> 1410754260720
	1410754260864 -> 1410754406128 [dir=none]
	1410754406128 [label="result
 (1, 514, 28, 28)" fillcolor=orange]
	1410754260864 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410754260432 -> 1410754260864
	1410754259904 -> 1410754259856
	1410754259904 -> 1401320501712 [dir=none]
	1401320501712 [label="g
 (144, 1, 1, 1)" fillcolor=orange]
	1410754259904 -> 1410754406704 [dir=none]
	1410754406704 [label="result1
 (144, 1, 1, 1)" fillcolor=orange]
	1410754259904 -> 1401320502576 [dir=none]
	1401320502576 [label="v
 (144, 514, 1, 1)" fillcolor=orange]
	1410754259904 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724986688 -> 1410754259904
	1401320502576 [label="ul_modules.1.in_proj_kv.1.weight_v
 (144, 514, 1, 1)" fillcolor=lightblue]
	1401320502576 -> 1410724986688
	1410724986688 [label=AccumulateGrad]
	1410724986592 -> 1410754259904
	1401320501712 [label="ul_modules.1.in_proj_kv.1.weight_g
 (144, 1, 1, 1)" fillcolor=lightblue]
	1401320501712 -> 1410724986592
	1410724986592 [label=AccumulateGrad]
	1410724984192 -> 1410754259856
	1401320502384 [label="ul_modules.1.in_proj_kv.1.bias
 (144)" fillcolor=lightblue]
	1401320502384 -> 1410724984192
	1410724984192 [label=AccumulateGrad]
	1410754258128 -> 1410754258080
	1410754258128 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 128)"]
	1410754258464 -> 1410754258128
	1410754258464 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 128)"]
	1410754258656 -> 1410754258464
	1410754258656 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410754258992 -> 1410754258656
	1410754258992 [label="ViewBackward0
-----------------------------------
self_sym_sizes: (1, 1, 128, 28, 28)"]
	1410754259376 -> 1410754258992
	1410754259376 [label="ViewBackward0
--------------------------------
self_sym_sizes: (1, 128, 28, 28)"]
	1410754259808 -> 1410754259376
	1410754257696 -> 1410754257360
	1410754257696 [label=NegBackward0]
	1410754257744 -> 1410754257696
	1410754257408 -> 1410754257120
	1410754257408 -> 1401320504112 [dir=none]
	1401320504112 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754257408 -> 1410754408912 [dir=none]
	1410754408912 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754257408 -> 1401320504784 [dir=none]
	1401320504784 [label="v
 (256, 256, 1, 1)" fillcolor=orange]
	1410754257408 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724983328 -> 1410754257408
	1401320504784 [label="ul_modules.1.out_proj.conv_1_sc.weight_v
 (256, 256, 1, 1)" fillcolor=lightblue]
	1401320504784 -> 1410724983328
	1410724983328 [label=AccumulateGrad]
	1410724982848 -> 1410754257408
	1401320504112 [label="ul_modules.1.out_proj.conv_1_sc.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320504112 -> 1410724982848
	1410724982848 [label=AccumulateGrad]
	1410724982464 -> 1410754257120
	1401320504592 [label="ul_modules.1.out_proj.conv_1_sc.bias
 (256)" fillcolor=lightblue]
	1401320504592 -> 1410724982464
	1410724982464 [label=AccumulateGrad]
	1410754256976 -> 1410754257024
	1410754256976 [label=NegBackward0]
	1410754256640 -> 1410754256976
	1410754256304 -> 1410754256208
	1410754256304 -> 1401320504496 [dir=none]
	1401320504496 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754256304 -> 1410754410544 [dir=none]
	1410754410544 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754256304 -> 1401320505168 [dir=none]
	1401320505168 [label="v
 (512, 512, 1, 1)" fillcolor=orange]
	1410754256304 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724981552 -> 1410754256304
	1401320505168 [label="ul_modules.1.out_proj.conv_2.weight_v
 (512, 512, 1, 1)" fillcolor=lightblue]
	1401320505168 -> 1410724981552
	1410724981552 [label=AccumulateGrad]
	1410724981984 -> 1410754256304
	1401320504496 [label="ul_modules.1.out_proj.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401320504496 -> 1410724981984
	1410724981984 [label=AccumulateGrad]
	1410724980496 -> 1410754256208
	1401320504976 [label="ul_modules.1.out_proj.conv_2.bias
 (512)" fillcolor=lightblue]
	1401320504976 -> 1410724980496
	1410724980496 [label=AccumulateGrad]
	1410754256016 -> 1410753898880
	1410754256016 -> 1410754411696 [dir=none]
	1410754411696 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754256016 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753898976 -> 1410754256016
	1410753898736 -> 1410753898640
	1410753898736 -> 1410753938640 [dir=none]
	1410753938640 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753898736 -> 1410753937584 [dir=none]
	1410753937584 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753898736 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753898832 -> 1410753898736
	1410753898832 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754256880 -> 1410753898832
	1410754256880 -> 1410753966448 [dir=none]
	1410753966448 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754256880 -> 1410753963664 [dir=none]
	1410753963664 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754256880 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754257600 -> 1410754256880
	1410754257600 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754257648 -> 1410754257600
	1410754257648 -> 1410754412816 [dir=none]
	1410754412816 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754257648 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754257936 -> 1410754257648
	1410754257936 -> 1410753968272 [dir=none]
	1410753968272 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754257936 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754257792 -> 1410754257936
	1410754257792 [label="CatBackward0
------------
dim: 1"]
	1410754258560 -> 1410754257792
	1410754258560 -> 1410753969712 [dir=none]
	1410753969712 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754258560 -> 1410753969808 [dir=none]
	1410753969808 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754258560 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754260144 -> 1410754258560
	1410754260144 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754258896 -> 1410754260144
	1410754258896 -> 1410753970000 [dir=none]
	1410753970000 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754258896 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754260816 -> 1410754258896
	1410754260816 [label="CatBackward0
------------
dim: 1"]
	1410753898784 -> 1410754260816
	1410754260576 -> 1410754260816
	1410754260576 [label=NegBackward0]
	1410753898784 -> 1410754260576
	1410754259184 -> 1410754258560
	1410754259184 -> 1401320504880 [dir=none]
	1401320504880 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754259184 -> 1410754414736 [dir=none]
	1410754414736 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754259184 -> 1401320964560 [dir=none]
	1401320964560 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754259184 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725200896 -> 1410754259184
	1401320964560 [label="ul_modules.2.input_gated_resnet.0.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401320964560 -> 1410725200896
	1410725200896 [label=AccumulateGrad]
	1410725200560 -> 1410754259184
	1401320504880 [label="ul_modules.2.input_gated_resnet.0.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320504880 -> 1410725200560
	1410725200560 [label=AccumulateGrad]
	1410725199936 -> 1410754258560
	1401320964368 [label="ul_modules.2.input_gated_resnet.0.conv_1.bias
 (256)" fillcolor=lightblue]
	1401320964368 -> 1410725199936
	1410725199936 [label=AccumulateGrad]
	1410754258368 -> 1410754257792
	1410754258368 [label=NegBackward0]
	1410754258560 -> 1410754258368
	1410754256784 -> 1410754256880
	1410754256784 -> 1401320964272 [dir=none]
	1401320964272 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754256784 -> 1410754416368 [dir=none]
	1410754416368 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754256784 -> 1401320964944 [dir=none]
	1401320964944 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754256784 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410724984480 -> 1410754256784
	1401320964944 [label="ul_modules.2.input_gated_resnet.0.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401320964944 -> 1410724984480
	1410724984480 [label=AccumulateGrad]
	1410724983856 -> 1410754256784
	1401320964272 [label="ul_modules.2.input_gated_resnet.0.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401320964272 -> 1410724983856
	1410724983856 [label=AccumulateGrad]
	1410724981456 -> 1410754256880
	1401320964752 [label="ul_modules.2.input_gated_resnet.0.conv_2.bias
 (512)" fillcolor=lightblue]
	1401320964752 -> 1410724981456
	1410724981456 [label=AccumulateGrad]
	1410754256688 -> 1410753898736
	1410754256688 -> 1410754417520 [dir=none]
	1410754417520 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754256688 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753898832 -> 1410754256688
	1410753898592 -> 1410753898496
	1410753898592 -> 1410753939792 [dir=none]
	1410753939792 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753898592 -> 1410753939600 [dir=none]
	1410753939600 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753898592 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753898688 -> 1410753898592
	1410753898688 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754258032 -> 1410753898688
	1410754258032 -> 1410753939408 [dir=none]
	1410753939408 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754258032 -> 1410753969424 [dir=none]
	1410753969424 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754258032 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754257216 -> 1410754258032
	1410754257216 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754258224 -> 1410754257216
	1410754258224 -> 1410754418576 [dir=none]
	1410754418576 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754258224 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754261248 -> 1410754258224
	1410754261248 -> 1410753939312 [dir=none]
	1410753939312 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754261248 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754259568 -> 1410754261248
	1410754259568 [label="CatBackward0
------------
dim: 1"]
	1410754261728 -> 1410754259568
	1410754261728 -> 1410753939120 [dir=none]
	1410753939120 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754261728 -> 1410753938832 [dir=none]
	1410753938832 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754261728 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754261872 -> 1410754261728
	1410754261872 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754261920 -> 1410754261872
	1410754261920 -> 1410753937488 [dir=none]
	1410753937488 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754261920 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754262016 -> 1410754261920
	1410754262016 [label="CatBackward0
------------
dim: 1"]
	1410753898640 -> 1410754262016
	1410754262112 -> 1410754262016
	1410754262112 [label=NegBackward0]
	1410753898640 -> 1410754262112
	1410754261440 -> 1410754261728
	1410754261440 -> 1401320502672 [dir=none]
	1401320502672 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754261440 -> 1410754420496 [dir=none]
	1410754420496 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754261440 -> 1401320965424 [dir=none]
	1401320965424 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754261440 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725201856 -> 1410754261440
	1401320965424 [label="ul_modules.2.input_gated_resnet.1.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401320965424 -> 1410725201856
	1410725201856 [label=AccumulateGrad]
	1410725201760 -> 1410754261440
	1401320502672 [label="ul_modules.2.input_gated_resnet.1.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320502672 -> 1410725201760
	1410725201760 [label=AccumulateGrad]
	1410725201424 -> 1410754261728
	1401320965328 [label="ul_modules.2.input_gated_resnet.1.conv_1.bias
 (256)" fillcolor=lightblue]
	1401320965328 -> 1410725201424
	1410725201424 [label=AccumulateGrad]
	1410754261104 -> 1410754259568
	1410754261104 [label=NegBackward0]
	1410754261728 -> 1410754261104
	1410754257840 -> 1410754258032
	1410754257840 -> 1401320965232 [dir=none]
	1401320965232 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754257840 -> 1410754422128 [dir=none]
	1410754422128 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754257840 -> 1401320965808 [dir=none]
	1401320965808 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754257840 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725201232 -> 1410754257840
	1401320965808 [label="ul_modules.2.input_gated_resnet.1.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401320965808 -> 1410725201232
	1410725201232 [label=AccumulateGrad]
	1410725201136 -> 1410754257840
	1401320965232 [label="ul_modules.2.input_gated_resnet.1.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401320965232 -> 1410725201136
	1410725201136 [label=AccumulateGrad]
	1410725201040 -> 1410754258032
	1401320965616 [label="ul_modules.2.input_gated_resnet.1.conv_2.bias
 (512)" fillcolor=lightblue]
	1401320965616 -> 1410725201040
	1410725201040 [label=AccumulateGrad]
	1410754257072 -> 1410753898592
	1410754257072 -> 1410754423280 [dir=none]
	1410754423280 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754257072 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753898688 -> 1410754257072
	1410753898448 -> 1410753898352
	1410753898448 -> 1410753940656 [dir=none]
	1410753940656 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753898448 -> 1410753940464 [dir=none]
	1410753940464 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753898448 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753898544 -> 1410753898448
	1410753898544 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754261152 -> 1410753898544
	1410754261152 -> 1410753940272 [dir=none]
	1410753940272 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754261152 -> 1410753939024 [dir=none]
	1410753939024 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754261152 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754260096 -> 1410754261152
	1410754260096 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754261632 -> 1410754260096
	1410754261632 -> 1410754424336 [dir=none]
	1410754424336 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754261632 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754262208 -> 1410754261632
	1410754262208 -> 1410753940176 [dir=none]
	1410753940176 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754262208 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754261824 -> 1410754262208
	1410754261824 [label="CatBackward0
------------
dim: 1"]
	1410754262304 -> 1410754261824
	1410754262304 -> 1410753939888 [dir=none]
	1410753939888 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754262304 -> 1410753939216 [dir=none]
	1410753939216 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754262304 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754262448 -> 1410754262304
	1410754262448 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754262592 -> 1410754262448
	1410754262592 -> 1410753939696 [dir=none]
	1410753939696 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754262592 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754262688 -> 1410754262592
	1410754262688 [label="CatBackward0
------------
dim: 1"]
	1410753898496 -> 1410754262688
	1410754262784 -> 1410754262688
	1410754262784 [label=NegBackward0]
	1410753898496 -> 1410754262784
	1410754262400 -> 1410754262304
	1410754262400 -> 1401320966288 [dir=none]
	1401320966288 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754262400 -> 1410754426256 [dir=none]
	1410754426256 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754262400 -> 1401320966480 [dir=none]
	1401320966480 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754262400 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725202816 -> 1410754262400
	1401320966480 [label="ul_modules.2.input_gated_resnet.2.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401320966480 -> 1410725202816
	1410725202816 [label=AccumulateGrad]
	1410725202720 -> 1410754262400
	1401320966288 [label="ul_modules.2.input_gated_resnet.2.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320966288 -> 1410725202720
	1410725202720 [label=AccumulateGrad]
	1410725202384 -> 1410754262304
	1401320966096 [label="ul_modules.2.input_gated_resnet.2.conv_1.bias
 (256)" fillcolor=lightblue]
	1401320966096 -> 1410725202384
	1410725202384 [label=AccumulateGrad]
	1410754262256 -> 1410754261824
	1410754262256 [label=NegBackward0]
	1410754262304 -> 1410754262256
	1410754260912 -> 1410754261152
	1410754260912 -> 1401320966000 [dir=none]
	1401320966000 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754260912 -> 1410754427888 [dir=none]
	1410754427888 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754260912 -> 1401320966672 [dir=none]
	1401320966672 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754260912 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725202192 -> 1410754260912
	1401320966672 [label="ul_modules.2.input_gated_resnet.2.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401320966672 -> 1410725202192
	1410725202192 [label=AccumulateGrad]
	1410725202096 -> 1410754260912
	1401320966000 [label="ul_modules.2.input_gated_resnet.2.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401320966000 -> 1410725202096
	1410725202096 [label=AccumulateGrad]
	1410725202000 -> 1410754261152
	1401320966384 [label="ul_modules.2.input_gated_resnet.2.conv_2.bias
 (512)" fillcolor=lightblue]
	1401320966384 -> 1410725202000
	1410725202000 [label=AccumulateGrad]
	1410754258272 -> 1410753898448
	1410754258272 -> 1410754445488 [dir=none]
	1410754445488 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754258272 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753898544 -> 1410754258272
	1410753898304 -> 1410753898208
	1410753898304 -> 1410753941520 [dir=none]
	1410753941520 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753898304 -> 1410753941328 [dir=none]
	1410753941328 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753898304 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753898400 -> 1410753898304
	1410753898400 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754262064 -> 1410753898400
	1410754262064 -> 1410753941136 [dir=none]
	1410753941136 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754262064 -> 1410753939984 [dir=none]
	1410753939984 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754262064 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754261968 -> 1410754262064
	1410754261968 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754262352 -> 1410754261968
	1410754262352 -> 1410754446544 [dir=none]
	1410754446544 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754262352 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754262880 -> 1410754262352
	1410754262880 -> 1410753941040 [dir=none]
	1410753941040 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754262880 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754262496 -> 1410754262880
	1410754262496 [label="CatBackward0
------------
dim: 1"]
	1410754262976 -> 1410754262496
	1410754262976 -> 1410753940752 [dir=none]
	1410753940752 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754262976 -> 1410753940080 [dir=none]
	1410753940080 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754262976 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754263120 -> 1410754262976
	1410754263120 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754263264 -> 1410754263120
	1410754263264 -> 1410753940560 [dir=none]
	1410753940560 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754263264 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754263360 -> 1410754263264
	1410754263360 [label="CatBackward0
------------
dim: 1"]
	1410753898352 -> 1410754263360
	1410754263456 -> 1410754263360
	1410754263456 [label=NegBackward0]
	1410753898352 -> 1410754263456
	1410754263072 -> 1410754262976
	1410754263072 -> 1401320966192 [dir=none]
	1401320966192 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754263072 -> 1410754448464 [dir=none]
	1410754448464 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754263072 -> 1401320967248 [dir=none]
	1401320967248 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754263072 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725203776 -> 1410754263072
	1401320967248 [label="ul_modules.2.input_gated_resnet.3.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401320967248 -> 1410725203776
	1410725203776 [label=AccumulateGrad]
	1410725203680 -> 1410754263072
	1401320966192 [label="ul_modules.2.input_gated_resnet.3.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320966192 -> 1410725203680
	1410725203680 [label=AccumulateGrad]
	1410725203344 -> 1410754262976
	1401320967056 [label="ul_modules.2.input_gated_resnet.3.conv_1.bias
 (256)" fillcolor=lightblue]
	1401320967056 -> 1410725203344
	1410725203344 [label=AccumulateGrad]
	1410754262928 -> 1410754262496
	1410754262928 [label=NegBackward0]
	1410754262976 -> 1410754262928
	1410754262160 -> 1410754262064
	1410754262160 -> 1401320966960 [dir=none]
	1401320966960 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754262160 -> 1410754450096 [dir=none]
	1410754450096 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754262160 -> 1401320967632 [dir=none]
	1401320967632 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754262160 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725203152 -> 1410754262160
	1401320967632 [label="ul_modules.2.input_gated_resnet.3.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401320967632 -> 1410725203152
	1410725203152 [label=AccumulateGrad]
	1410725203056 -> 1410754262160
	1401320966960 [label="ul_modules.2.input_gated_resnet.3.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401320966960 -> 1410725203056
	1410725203056 [label=AccumulateGrad]
	1410725201568 -> 1410754262064
	1401320967440 [label="ul_modules.2.input_gated_resnet.3.conv_2.bias
 (512)" fillcolor=lightblue]
	1401320967440 -> 1410725201568
	1410725201568 [label=AccumulateGrad]
	1410754261584 -> 1410753898304
	1410754261584 -> 1410754451248 [dir=none]
	1410754451248 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754261584 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753898400 -> 1410754261584
	1410753898160 -> 1410753898064
	1410753898160 -> 1410753942384 [dir=none]
	1410753942384 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753898160 -> 1410753942192 [dir=none]
	1410753942192 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753898160 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753898256 -> 1410753898160
	1410753898256 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754262736 -> 1410753898256
	1410754262736 -> 1410753942000 [dir=none]
	1410753942000 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754262736 -> 1410753940848 [dir=none]
	1410753940848 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754262736 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754262640 -> 1410754262736
	1410754262640 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754263024 -> 1410754262640
	1410754263024 -> 1410754452304 [dir=none]
	1410754452304 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754263024 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754263552 -> 1410754263024
	1410754263552 -> 1410753941904 [dir=none]
	1410753941904 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754263552 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754263168 -> 1410754263552
	1410754263168 [label="CatBackward0
------------
dim: 1"]
	1410754263648 -> 1410754263168
	1410754263648 -> 1410753941616 [dir=none]
	1410753941616 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754263648 -> 1410753940944 [dir=none]
	1410753940944 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754263648 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754263792 -> 1410754263648
	1410754263792 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754263936 -> 1410754263792
	1410754263936 -> 1410753941424 [dir=none]
	1410753941424 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754263936 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754264032 -> 1410754263936
	1410754264032 [label="CatBackward0
------------
dim: 1"]
	1410753898208 -> 1410754264032
	1410754264128 -> 1410754264032
	1410754264128 [label=NegBackward0]
	1410753898208 -> 1410754264128
	1410754263744 -> 1410754263648
	1410754263744 -> 1401320967344 [dir=none]
	1401320967344 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754263744 -> 1410754454224 [dir=none]
	1410754454224 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754263744 -> 1401320968208 [dir=none]
	1401320968208 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754263744 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725204736 -> 1410754263744
	1401320968208 [label="ul_modules.2.input_gated_resnet.4.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401320968208 -> 1410725204736
	1410725204736 [label=AccumulateGrad]
	1410725204640 -> 1410754263744
	1401320967344 [label="ul_modules.2.input_gated_resnet.4.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320967344 -> 1410725204640
	1410725204640 [label=AccumulateGrad]
	1410725204304 -> 1410754263648
	1401320968016 [label="ul_modules.2.input_gated_resnet.4.conv_1.bias
 (256)" fillcolor=lightblue]
	1401320968016 -> 1410725204304
	1410725204304 [label=AccumulateGrad]
	1410754263600 -> 1410754263168
	1410754263600 [label=NegBackward0]
	1410754263648 -> 1410754263600
	1410754262832 -> 1410754262736
	1410754262832 -> 1401320967920 [dir=none]
	1401320967920 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754262832 -> 1410754455856 [dir=none]
	1410754455856 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754262832 -> 1401320968592 [dir=none]
	1401320968592 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754262832 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725204112 -> 1410754262832
	1401320968592 [label="ul_modules.2.input_gated_resnet.4.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401320968592 -> 1410725204112
	1410725204112 [label=AccumulateGrad]
	1410725204016 -> 1410754262832
	1401320967920 [label="ul_modules.2.input_gated_resnet.4.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401320967920 -> 1410725204016
	1410725204016 [label=AccumulateGrad]
	1410725202528 -> 1410754262736
	1401320968400 [label="ul_modules.2.input_gated_resnet.4.conv_2.bias
 (512)" fillcolor=lightblue]
	1401320968400 -> 1410725202528
	1410725202528 [label=AccumulateGrad]
	1410754262544 -> 1410753898160
	1410754262544 -> 1410754457008 [dir=none]
	1410754457008 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754262544 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753898256 -> 1410754262544
	1410753898016 -> 1410753897920
	1410753898016 -> 1410753946704 [dir=none]
	1410753946704 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753898016 -> 1410753946512 [dir=none]
	1410753946512 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753898016 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753898112 -> 1410753898016
	1410753898112 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754263408 -> 1410753898112
	1410754263408 -> 1410753946320 [dir=none]
	1410753946320 [label="input
 (1, 512, 28, 28)" fillcolor=orange]
	1410754263408 -> 1410753945840 [dir=none]
	1410753945840 [label="weight
 (512, 512, 1, 1)" fillcolor=orange]
	1410754263408 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754263312 -> 1410754263408
	1410754263312 -> 1410754458064 [dir=none]
	1410754458064 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754263312 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754263696 -> 1410754263312
	1410754263696 -> 1410753946224 [dir=none]
	1410753946224 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754263696 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754264224 -> 1410754263696
	1410754264224 [label="CatBackward0
------------
dim: 1"]
	1410754263840 -> 1410754264224
	1410754263840 [label="AddBackward0
------------
alpha: 1"]
	1410754264368 -> 1410754263840
	1410754264368 -> 1410753945360 [dir=none]
	1410753945360 [label="input
 (1, 512, 28, 28)" fillcolor=orange]
	1410754264368 -> 1410753945744 [dir=none]
	1410753945744 [label="weight
 (256, 512, 1, 1)" fillcolor=orange]
	1410754264368 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754264512 -> 1410754264368
	1410754264512 -> 1410753945648 [dir=none]
	1410753945648 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754264512 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754264656 -> 1410754264512
	1410754264656 [label="CatBackward0
------------
dim: 1"]
	1410753898064 -> 1410754264656
	1410754264752 -> 1410754264656
	1410754264752 [label=NegBackward0]
	1410753898064 -> 1410754264752
	1410754264464 -> 1410754264368
	1410754264464 -> 1401320971184 [dir=none]
	1401320971184 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754264464 -> 1410754459888 [dir=none]
	1410754459888 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754264464 -> 1401320971856 [dir=none]
	1401320971856 [label="v
 (256, 512, 1, 1)" fillcolor=orange]
	1410754264464 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725205792 -> 1410754264464
	1401320971856 [label="ul_modules.2.out_proj.conv_1.weight_v
 (256, 512, 1, 1)" fillcolor=lightblue]
	1401320971856 -> 1410725205792
	1410725205792 [label=AccumulateGrad]
	1410725205648 -> 1410754264464
	1401320971184 [label="ul_modules.2.out_proj.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320971184 -> 1410725205648
	1410725205648 [label=AccumulateGrad]
	1410725205312 -> 1410754264368
	1401320971664 [label="ul_modules.2.out_proj.conv_1.bias
 (256)" fillcolor=lightblue]
	1401320971664 -> 1410725205312
	1410725205312 [label=AccumulateGrad]
	1410754264320 -> 1410754263840
	1410754264320 -> 1410753945936 [dir=none]
	1410753945936 [label="input
 (1, 256, 28, 28)" fillcolor=orange]
	1410754264320 -> 1410753946128 [dir=none]
	1410753946128 [label="weight
 (256, 256, 1, 1)" fillcolor=orange]
	1410754264320 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754264704 -> 1410754264320
	1410754264704 -> 1410753946032 [dir=none]
	1410753946032 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410754264704 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754264560 -> 1410754264704
	1410754264560 [label="CatBackward0
------------
dim: 1"]
	1410754264944 -> 1410754264560
	1410754264944 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (1, 1, 128, 784)"]
	1410754265040 -> 1410754264944
	1410754265040 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410754478240 -> 1410754265040
	1410754478240 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (1, 784, 128)"]
	1410754478336 -> 1410754478240
	1410754478336 -> 1410754494736 [dir=none]
	1410754494736 [label="mat2
 (1, 784, 128)" fillcolor=orange]
	1410754478336 -> 1410754495024 [dir=none]
	1410754495024 [label="self
 (1, 784, 784)" fillcolor=orange]
	1410754478336 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	1410754478432 -> 1410754478336
	1410754478432 [label="ViewBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 784)"]
	1410754478576 -> 1410754478432
	1410754478576 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 784)"]
	1410754478672 -> 1410754478576
	1410754478672 -> 1410754495312 [dir=none]
	1410754495312 [label="result
 (1, 1, 784, 784)" fillcolor=orange]
	1410754478672 [label="SoftmaxBackward0
----------------------
dim   :     4294967295
result: [saved tensor]"]
	1410754478768 -> 1410754478672
	1410754478768 -> 1410753944880 [dir=none]
	1410753944880 [label="mask
 (1, 1, 784, 784)" fillcolor=orange]
	1410754478768 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	1410754478864 -> 1410754478768
	1410754478864 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (1, 784, 784)"]
	1410754478960 -> 1410754478864
	1410754478960 -> 1410754495984 [dir=none]
	1410754495984 [label="mat2
 (1, 16, 784)" fillcolor=orange]
	1410754478960 -> 1410754496272 [dir=none]
	1410754496272 [label="self
 (1, 784, 16)" fillcolor=orange]
	1410754478960 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	1410754479056 -> 1410754478960
	1410754479056 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (1, 1, 784, 16)"]
	1410754479200 -> 1410754479056
	1410754479200 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 784, 16)"]
	1410754479296 -> 1410754479200
	1410754479296 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410754479392 -> 1410754479296
	1410754479392 -> 1410754496656 [dir=none]
	1410754496656 [label="other
 ()" fillcolor=orange]
	1410754479392 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	1410754479488 -> 1410754479392
	1410754479488 [label="ViewBackward0
----------------------------------
self_sym_sizes: (1, 1, 16, 28, 28)"]
	1410754479584 -> 1410754479488
	1410754479584 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 16, 28, 28)"]
	1410754479680 -> 1410754479584
	1410754479680 -> 1410753944784 [dir=none]
	1410753944784 [label="input
 (1, 258, 28, 28)" fillcolor=orange]
	1410754479680 -> 1410753944496 [dir=none]
	1410753944496 [label="weight
 (16, 258, 1, 1)" fillcolor=orange]
	1410754479680 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754479776 -> 1410754479680
	1410754479776 [label="AddBackward0
------------
alpha: 1"]
	1410754479920 -> 1410754479776
	1410754479920 [label="CatBackward0
------------
dim: 1"]
	1410753898064 -> 1410754479920
	1410754479872 -> 1410754479776
	1410754479872 -> 1410753944592 [dir=none]
	1410753944592 [label="other
 (1, 258, 28, 28)" fillcolor=orange]
	1410754479872 -> 1410753944400 [dir=none]
	1410753944400 [label="self
 (1, 258, 28, 28)" fillcolor=orange]
	1410754479872 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410754479968 -> 1410754479872
	1410754479968 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 516, 28, 28)
split_size    :              258"]
	1410754480208 -> 1410754479968
	1410754480208 -> 1410753944016 [dir=none]
	1410753944016 [label="input
 (1, 516, 28, 28)" fillcolor=orange]
	1410754480208 -> 1410753944112 [dir=none]
	1410753944112 [label="weight
 (516, 516, 1, 1)" fillcolor=orange]
	1410754480208 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (516,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754480304 -> 1410754480208
	1410754480304 -> 1410754498480 [dir=none]
	1410754498480 [label="result1
 (1, 516, 28, 28)" fillcolor=orange]
	1410754480304 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754480448 -> 1410754480304
	1410754480448 -> 1410753944208 [dir=none]
	1410753944208 [label="self
 (1, 516, 28, 28)" fillcolor=orange]
	1410754480448 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754480544 -> 1410754480448
	1410754480544 [label="CatBackward0
------------
dim: 1"]
	1410754480640 -> 1410754480544
	1410754480640 -> 1410753943728 [dir=none]
	1410753943728 [label="input
 (1, 516, 28, 28)" fillcolor=orange]
	1410754480640 -> 1410753943920 [dir=none]
	1410753943920 [label="weight
 (258, 516, 1, 1)" fillcolor=orange]
	1410754480640 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (258,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754480784 -> 1410754480640
	1410754480784 -> 1410753943824 [dir=none]
	1410753943824 [label="self
 (1, 516, 28, 28)" fillcolor=orange]
	1410754480784 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754480928 -> 1410754480784
	1410754480928 [label="CatBackward0
------------
dim: 1"]
	1410754479920 -> 1410754480928
	1410754481024 -> 1410754480928
	1410754481024 [label=NegBackward0]
	1410754479920 -> 1410754481024
	1410754480736 -> 1410754480640
	1410754480736 -> 1401320969840 [dir=none]
	1401320969840 [label="g
 (258, 1, 1, 1)" fillcolor=orange]
	1410754480736 -> 1410754500208 [dir=none]
	1410754500208 [label="result1
 (258, 1, 1, 1)" fillcolor=orange]
	1410754480736 -> 1401320970512 [dir=none]
	1401320970512 [label="v
 (258, 516, 1, 1)" fillcolor=orange]
	1410754480736 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725209296 -> 1410754480736
	1401320970512 [label="ul_modules.2.in_proj_q.0.conv_1.weight_v
 (258, 516, 1, 1)" fillcolor=lightblue]
	1401320970512 -> 1410725209296
	1410725209296 [label=AccumulateGrad]
	1410725209152 -> 1410754480736
	1401320969840 [label="ul_modules.2.in_proj_q.0.conv_1.weight_g
 (258, 1, 1, 1)" fillcolor=lightblue]
	1401320969840 -> 1410725209152
	1410725209152 [label=AccumulateGrad]
	1410725208816 -> 1410754480640
	1401320970320 [label="ul_modules.2.in_proj_q.0.conv_1.bias
 (258)" fillcolor=lightblue]
	1401320970320 -> 1410725208816
	1410725208816 [label=AccumulateGrad]
	1410754480592 -> 1410754480544
	1410754480592 [label=NegBackward0]
	1410754480640 -> 1410754480592
	1410754480256 -> 1410754480208
	1410754480256 -> 1401320970224 [dir=none]
	1401320970224 [label="g
 (516, 1, 1, 1)" fillcolor=orange]
	1410754480256 -> 1410754501840 [dir=none]
	1410754501840 [label="result1
 (516, 1, 1, 1)" fillcolor=orange]
	1410754480256 -> 1401320970896 [dir=none]
	1401320970896 [label="v
 (516, 516, 1, 1)" fillcolor=orange]
	1410754480256 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725208480 -> 1410754480256
	1401320970896 [label="ul_modules.2.in_proj_q.0.conv_2.weight_v
 (516, 516, 1, 1)" fillcolor=lightblue]
	1401320970896 -> 1410725208480
	1410725208480 [label=AccumulateGrad]
	1410725208624 -> 1410754480256
	1401320970224 [label="ul_modules.2.in_proj_q.0.conv_2.weight_g
 (516, 1, 1, 1)" fillcolor=lightblue]
	1401320970224 -> 1410725208624
	1410725208624 [label=AccumulateGrad]
	1410725208192 -> 1410754480208
	1401320970704 [label="ul_modules.2.in_proj_q.0.conv_2.bias
 (516)" fillcolor=lightblue]
	1401320970704 -> 1410725208192
	1410725208192 [label=AccumulateGrad]
	1410754480016 -> 1410754479872
	1410754480016 -> 1410754502992 [dir=none]
	1410754502992 [label="result
 (1, 258, 28, 28)" fillcolor=orange]
	1410754480016 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410754479968 -> 1410754480016
	1410754479728 -> 1410754479680
	1410754479728 -> 1401320970608 [dir=none]
	1401320970608 [label="g
 (16, 1, 1, 1)" fillcolor=orange]
	1410754479728 -> 1410754503568 [dir=none]
	1410754503568 [label="result1
 (16, 1, 1, 1)" fillcolor=orange]
	1410754479728 -> 1401320971472 [dir=none]
	1401320971472 [label="v
 (16, 258, 1, 1)" fillcolor=orange]
	1410754479728 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725209008 -> 1410754479728
	1401320971472 [label="ul_modules.2.in_proj_q.1.weight_v
 (16, 258, 1, 1)" fillcolor=lightblue]
	1401320971472 -> 1410725209008
	1410725209008 [label=AccumulateGrad]
	1410725208144 -> 1410754479728
	1401320970608 [label="ul_modules.2.in_proj_q.1.weight_g
 (16, 1, 1, 1)" fillcolor=lightblue]
	1401320970608 -> 1410725208144
	1410725208144 [label=AccumulateGrad]
	1410725207136 -> 1410754479680
	1401320971280 [label="ul_modules.2.in_proj_q.1.bias
 (16)" fillcolor=lightblue]
	1401320971280 -> 1410725207136
	1410725207136 [label=AccumulateGrad]
	1410754479008 -> 1410754478960
	1410754479008 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 1, 16, 784)"]
	1410754479344 -> 1410754479008
	1410754479344 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 16, 784)"]
	1410754479536 -> 1410754479344
	1410754479536 [label="ViewBackward0
----------------------------------
self_sym_sizes: (1, 1, 16, 28, 28)"]
	1410754479104 -> 1410754479536
	1410754479104 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 16, 28, 28)"]
	1410754480064 -> 1410754479104
	1410754480064 [label="SplitWithSizesBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 144, 28, 28)
split_sizes   :        (16, 128)"]
	1410754480112 -> 1410754480064
	1410754480112 -> 1410753943440 [dir=none]
	1410753943440 [label="input
 (1, 514, 28, 28)" fillcolor=orange]
	1410754480112 -> 1410753943152 [dir=none]
	1410753943152 [label="weight
 (144, 514, 1, 1)" fillcolor=orange]
	1410754480112 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (144,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754480496 -> 1410754480112
	1410754480496 [label="AddBackward0
------------
alpha: 1"]
	1410754480880 -> 1410754480496
	1410754480880 [label="CatBackward0
------------
dim: 1"]
	1410753898784 -> 1410754480880
	1410753898064 -> 1410754480880
	1410754480976 -> 1410754480496
	1410754480976 -> 1410753943248 [dir=none]
	1410753943248 [label="other
 (1, 514, 28, 28)" fillcolor=orange]
	1410754480976 -> 1410753943056 [dir=none]
	1410753943056 [label="self
 (1, 514, 28, 28)" fillcolor=orange]
	1410754480976 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410754480688 -> 1410754480976
	1410754480688 [label="SplitBackward0
---------------------------------
dim           :                 1
self_sym_sizes: (1, 1028, 28, 28)
split_size    :               514"]
	1410754481216 -> 1410754480688
	1410754481216 -> 1410753942672 [dir=none]
	1410753942672 [label="input
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754481216 -> 1410753942768 [dir=none]
	1410753942768 [label="weight
 (1028, 1028, 1, 1)" fillcolor=orange]
	1410754481216 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1028,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754481312 -> 1410754481216
	1410754481312 -> 1410754506544 [dir=none]
	1410754506544 [label="result1
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754481312 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754481456 -> 1410754481312
	1410754481456 -> 1410753942864 [dir=none]
	1410753942864 [label="self
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754481456 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754481552 -> 1410754481456
	1410754481552 [label="CatBackward0
------------
dim: 1"]
	1410754481648 -> 1410754481552
	1410754481648 -> 1410753942288 [dir=none]
	1410753942288 [label="input
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754481648 -> 1410753942480 [dir=none]
	1410753942480 [label="weight
 (514, 1028, 1, 1)" fillcolor=orange]
	1410754481648 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (514,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754481792 -> 1410754481648
	1410754481792 -> 1410753941808 [dir=none]
	1410753941808 [label="self
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754481792 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754481936 -> 1410754481792
	1410754481936 [label="CatBackward0
------------
dim: 1"]
	1410754480880 -> 1410754481936
	1410754482032 -> 1410754481936
	1410754482032 [label=NegBackward0]
	1410754480880 -> 1410754482032
	1410754481744 -> 1410754481648
	1410754481744 -> 1401320968304 [dir=none]
	1401320968304 [label="g
 (514, 1, 1, 1)" fillcolor=orange]
	1410754481744 -> 1410754508272 [dir=none]
	1410754508272 [label="result1
 (514, 1, 1, 1)" fillcolor=orange]
	1410754481744 -> 1401320969168 [dir=none]
	1401320969168 [label="v
 (514, 1028, 1, 1)" fillcolor=orange]
	1410754481744 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725210736 -> 1410754481744
	1401320969168 [label="ul_modules.2.in_proj_kv.0.conv_1.weight_v
 (514, 1028, 1, 1)" fillcolor=lightblue]
	1401320969168 -> 1410725210736
	1410725210736 [label=AccumulateGrad]
	1410725210592 -> 1410754481744
	1401320968304 [label="ul_modules.2.in_proj_kv.0.conv_1.weight_g
 (514, 1, 1, 1)" fillcolor=lightblue]
	1401320968304 -> 1410725210592
	1410725210592 [label=AccumulateGrad]
	1410725210256 -> 1410754481648
	1401320968976 [label="ul_modules.2.in_proj_kv.0.conv_1.bias
 (514)" fillcolor=lightblue]
	1401320968976 -> 1410725210256
	1410725210256 [label=AccumulateGrad]
	1410754481600 -> 1410754481552
	1410754481600 [label=NegBackward0]
	1410754481648 -> 1410754481600
	1410754481264 -> 1410754481216
	1410754481264 -> 1401320968880 [dir=none]
	1401320968880 [label="g
 (1028, 1, 1, 1)" fillcolor=orange]
	1410754481264 -> 1410754509904 [dir=none]
	1410754509904 [label="result1
 (1028, 1, 1, 1)" fillcolor=orange]
	1410754481264 -> 1401320969552 [dir=none]
	1401320969552 [label="v
 (1028, 1028, 1, 1)" fillcolor=orange]
	1410754481264 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725209920 -> 1410754481264
	1401320969552 [label="ul_modules.2.in_proj_kv.0.conv_2.weight_v
 (1028, 1028, 1, 1)" fillcolor=lightblue]
	1401320969552 -> 1410725209920
	1410725209920 [label=AccumulateGrad]
	1410725210064 -> 1410754481264
	1401320968880 [label="ul_modules.2.in_proj_kv.0.conv_2.weight_g
 (1028, 1, 1, 1)" fillcolor=lightblue]
	1401320968880 -> 1410725210064
	1410725210064 [label=AccumulateGrad]
	1410725209632 -> 1410754481216
	1401320969360 [label="ul_modules.2.in_proj_kv.0.conv_2.bias
 (1028)" fillcolor=lightblue]
	1401320969360 -> 1410725209632
	1410725209632 [label=AccumulateGrad]
	1410754481120 -> 1410754480976
	1410754481120 -> 1410754543888 [dir=none]
	1410754543888 [label="result
 (1, 514, 28, 28)" fillcolor=orange]
	1410754481120 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410754480688 -> 1410754481120
	1410754480160 -> 1410754480112
	1410754480160 -> 1401320969264 [dir=none]
	1401320969264 [label="g
 (144, 1, 1, 1)" fillcolor=orange]
	1410754480160 -> 1410754544464 [dir=none]
	1410754544464 [label="result1
 (144, 1, 1, 1)" fillcolor=orange]
	1410754480160 -> 1401320970128 [dir=none]
	1401320970128 [label="v
 (144, 514, 1, 1)" fillcolor=orange]
	1410754480160 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725210448 -> 1410754480160
	1401320970128 [label="ul_modules.2.in_proj_kv.1.weight_v
 (144, 514, 1, 1)" fillcolor=lightblue]
	1401320970128 -> 1410725210448
	1410725210448 [label=AccumulateGrad]
	1410725209584 -> 1410754480160
	1401320969264 [label="ul_modules.2.in_proj_kv.1.weight_g
 (144, 1, 1, 1)" fillcolor=lightblue]
	1401320969264 -> 1410725209584
	1410725209584 [label=AccumulateGrad]
	1410725207184 -> 1410754480112
	1401320969936 [label="ul_modules.2.in_proj_kv.1.bias
 (144)" fillcolor=lightblue]
	1401320969936 -> 1410725207184
	1410725207184 [label=AccumulateGrad]
	1410754478384 -> 1410754478336
	1410754478384 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 128)"]
	1410754478720 -> 1410754478384
	1410754478720 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 128)"]
	1410754478912 -> 1410754478720
	1410754478912 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410754479248 -> 1410754478912
	1410754479248 [label="ViewBackward0
-----------------------------------
self_sym_sizes: (1, 1, 128, 28, 28)"]
	1410754479632 -> 1410754479248
	1410754479632 [label="ViewBackward0
--------------------------------
self_sym_sizes: (1, 128, 28, 28)"]
	1410754480064 -> 1410754479632
	1410754264896 -> 1410754264560
	1410754264896 [label=NegBackward0]
	1410754264944 -> 1410754264896
	1410754264608 -> 1410754264320
	1410754264608 -> 1401320971568 [dir=none]
	1401320971568 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754264608 -> 1410754546672 [dir=none]
	1410754546672 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754264608 -> 1401320972240 [dir=none]
	1401320972240 [label="v
 (256, 256, 1, 1)" fillcolor=orange]
	1410754264608 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725206320 -> 1410754264608
	1401320972240 [label="ul_modules.2.out_proj.conv_1_sc.weight_v
 (256, 256, 1, 1)" fillcolor=lightblue]
	1401320972240 -> 1410725206320
	1410725206320 [label=AccumulateGrad]
	1410725205840 -> 1410754264608
	1401320971568 [label="ul_modules.2.out_proj.conv_1_sc.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320971568 -> 1410725205840
	1410725205840 [label=AccumulateGrad]
	1410725205456 -> 1410754264320
	1401320972048 [label="ul_modules.2.out_proj.conv_1_sc.bias
 (256)" fillcolor=lightblue]
	1401320972048 -> 1410725205456
	1410725205456 [label=AccumulateGrad]
	1410754264176 -> 1410754264224
	1410754264176 [label=NegBackward0]
	1410754263840 -> 1410754264176
	1410754263504 -> 1410754263408
	1410754263504 -> 1401320971952 [dir=none]
	1401320971952 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754263504 -> 1410754548304 [dir=none]
	1410754548304 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754263504 -> 1401320972624 [dir=none]
	1401320972624 [label="v
 (512, 512, 1, 1)" fillcolor=orange]
	1410754263504 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725204544 -> 1410754263504
	1401320972624 [label="ul_modules.2.out_proj.conv_2.weight_v
 (512, 512, 1, 1)" fillcolor=lightblue]
	1401320972624 -> 1410725204544
	1410725204544 [label=AccumulateGrad]
	1410725204976 -> 1410754263504
	1401320971952 [label="ul_modules.2.out_proj.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401320971952 -> 1410725204976
	1410725204976 [label=AccumulateGrad]
	1410725203488 -> 1410754263408
	1401320972432 [label="ul_modules.2.out_proj.conv_2.bias
 (512)" fillcolor=lightblue]
	1401320972432 -> 1410725203488
	1410725203488 [label=AccumulateGrad]
	1410754263216 -> 1410753898016
	1410754263216 -> 1410754549456 [dir=none]
	1410754549456 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754263216 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753898112 -> 1410754263216
	1410753897872 -> 1410753897776
	1410753897872 -> 1410753946608 [dir=none]
	1410753946608 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753897872 -> 1410753941712 [dir=none]
	1410753941712 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753897872 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753897968 -> 1410753897872
	1410753897968 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754264080 -> 1410753897968
	1410754264080 -> 1410753944688 [dir=none]
	1410753944688 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754264080 -> 1410753938928 [dir=none]
	1410753938928 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754264080 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754264800 -> 1410754264080
	1410754264800 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754264848 -> 1410754264800
	1410754264848 -> 1410754550512 [dir=none]
	1410754550512 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754264848 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754264992 -> 1410754264848
	1410754264992 -> 1410753944976 [dir=none]
	1410753944976 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754264992 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754478144 -> 1410754264992
	1410754478144 [label="CatBackward0
------------
dim: 1"]
	1410754478816 -> 1410754478144
	1410754478816 -> 1410753945264 [dir=none]
	1410753945264 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754478816 -> 1410753945456 [dir=none]
	1410753945456 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754478816 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754480400 -> 1410754478816
	1410754480400 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754479152 -> 1410754480400
	1410754479152 -> 1410753945552 [dir=none]
	1410753945552 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754479152 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754481072 -> 1410754479152
	1410754481072 [label="CatBackward0
------------
dim: 1"]
	1410753897920 -> 1410754481072
	1410754480832 -> 1410754481072
	1410754480832 [label=NegBackward0]
	1410753897920 -> 1410754480832
	1410754479440 -> 1410754478816
	1410754479440 -> 1401320972336 [dir=none]
	1401320972336 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754479440 -> 1410754552432 [dir=none]
	1410754552432 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754479440 -> 1401320973200 [dir=none]
	1401320973200 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754479440 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725210832 -> 1410754479440
	1401320973200 [label="ul_modules.3.input_gated_resnet.0.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401320973200 -> 1410725210832
	1410725210832 [label=AccumulateGrad]
	1410725210496 -> 1410754479440
	1401320972336 [label="ul_modules.3.input_gated_resnet.0.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320972336 -> 1410725210496
	1410725210496 [label=AccumulateGrad]
	1410725209056 -> 1410754478816
	1401320973008 [label="ul_modules.3.input_gated_resnet.0.conv_1.bias
 (256)" fillcolor=lightblue]
	1401320973008 -> 1410725209056
	1410725209056 [label=AccumulateGrad]
	1410754478624 -> 1410754478144
	1410754478624 [label=NegBackward0]
	1410754478816 -> 1410754478624
	1410754263984 -> 1410754264080
	1410754263984 -> 1401320972912 [dir=none]
	1401320972912 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754263984 -> 1410754554064 [dir=none]
	1410754554064 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754263984 -> 1401320973584 [dir=none]
	1401320973584 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754263984 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725207472 -> 1410754263984
	1401320973584 [label="ul_modules.3.input_gated_resnet.0.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401320973584 -> 1410725207472
	1410725207472 [label=AccumulateGrad]
	1410725206848 -> 1410754263984
	1401320972912 [label="ul_modules.3.input_gated_resnet.0.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401320972912 -> 1410725206848
	1410725206848 [label=AccumulateGrad]
	1410725204448 -> 1410754264080
	1401320973392 [label="ul_modules.3.input_gated_resnet.0.conv_2.bias
 (512)" fillcolor=lightblue]
	1401320973392 -> 1410725204448
	1410725204448 [label=AccumulateGrad]
	1410754263888 -> 1410753897872
	1410754263888 -> 1410754555216 [dir=none]
	1410754555216 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754263888 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753897968 -> 1410754263888
	1410753897728 -> 1410753897632
	1410753897728 -> 1410753947760 [dir=none]
	1410753947760 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753897728 -> 1410753947568 [dir=none]
	1410753947568 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753897728 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753897824 -> 1410753897728
	1410753897824 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754264416 -> 1410753897824
	1410754264416 -> 1410753947376 [dir=none]
	1410753947376 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754264416 -> 1410753942576 [dir=none]
	1410753942576 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754264416 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754249056 -> 1410754264416
	1410754249056 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754478480 -> 1410754249056
	1410754478480 -> 1410754556272 [dir=none]
	1410754556272 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754478480 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754481504 -> 1410754478480
	1410754481504 -> 1410753947280 [dir=none]
	1410753947280 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754481504 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754479824 -> 1410754481504
	1410754479824 [label="CatBackward0
------------
dim: 1"]
	1410754481984 -> 1410754479824
	1410754481984 -> 1410753947088 [dir=none]
	1410753947088 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754481984 -> 1410753946800 [dir=none]
	1410753946800 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754481984 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754482128 -> 1410754481984
	1410754482128 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754482176 -> 1410754482128
	1410754482176 -> 1410753945072 [dir=none]
	1410753945072 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754482176 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754482272 -> 1410754482176
	1410754482272 [label="CatBackward0
------------
dim: 1"]
	1410753897776 -> 1410754482272
	1410754482368 -> 1410754482272
	1410754482368 [label=NegBackward0]
	1410753897776 -> 1410754482368
	1410754481696 -> 1410754481984
	1410754481696 -> 1401320973296 [dir=none]
	1401320973296 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754481696 -> 1410754558192 [dir=none]
	1410754558192 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754481696 -> 1401320974160 [dir=none]
	1401320974160 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754481696 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725211792 -> 1410754481696
	1401320974160 [label="ul_modules.3.input_gated_resnet.1.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401320974160 -> 1410725211792
	1410725211792 [label=AccumulateGrad]
	1410725211696 -> 1410754481696
	1401320973296 [label="ul_modules.3.input_gated_resnet.1.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320973296 -> 1410725211696
	1410725211696 [label=AccumulateGrad]
	1410725211360 -> 1410754481984
	1401320973968 [label="ul_modules.3.input_gated_resnet.1.conv_1.bias
 (256)" fillcolor=lightblue]
	1401320973968 -> 1410725211360
	1410725211360 [label=AccumulateGrad]
	1410754481360 -> 1410754479824
	1410754481360 [label=NegBackward0]
	1410754481984 -> 1410754481360
	1410754478288 -> 1410754264416
	1410754478288 -> 1401320973872 [dir=none]
	1401320973872 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754478288 -> 1410754559824 [dir=none]
	1410754559824 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754478288 -> 1401320974544 [dir=none]
	1401320974544 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754478288 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725211168 -> 1410754478288
	1401320974544 [label="ul_modules.3.input_gated_resnet.1.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401320974544 -> 1410725211168
	1410725211168 [label=AccumulateGrad]
	1410725211072 -> 1410754478288
	1401320973872 [label="ul_modules.3.input_gated_resnet.1.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401320973872 -> 1410725211072
	1410725211072 [label=AccumulateGrad]
	1410725206224 -> 1410754264416
	1401320974352 [label="ul_modules.3.input_gated_resnet.1.conv_2.bias
 (512)" fillcolor=lightblue]
	1401320974352 -> 1410725206224
	1410725206224 [label=AccumulateGrad]
	1410754264272 -> 1410753897728
	1410754264272 -> 1410754593808 [dir=none]
	1410754593808 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754264272 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753897824 -> 1410754264272
	1410753897584 -> 1410753897488
	1410753897584 -> 1410753948624 [dir=none]
	1410753948624 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753897584 -> 1410753948432 [dir=none]
	1410753948432 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753897584 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410754248768 -> 1410753897584
	1410754248768 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754481408 -> 1410754248768
	1410754481408 -> 1410753948240 [dir=none]
	1410753948240 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754481408 -> 1410753946992 [dir=none]
	1410753946992 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754481408 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754480352 -> 1410754481408
	1410754480352 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754481888 -> 1410754480352
	1410754481888 -> 1410754594864 [dir=none]
	1410754594864 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754481888 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754482464 -> 1410754481888
	1410754482464 -> 1410753948144 [dir=none]
	1410753948144 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754482464 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754482080 -> 1410754482464
	1410754482080 [label="CatBackward0
------------
dim: 1"]
	1410754482560 -> 1410754482080
	1410754482560 -> 1410753947856 [dir=none]
	1410753947856 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754482560 -> 1410753947184 [dir=none]
	1410753947184 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754482560 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754482704 -> 1410754482560
	1410754482704 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754482848 -> 1410754482704
	1410754482848 -> 1410753947664 [dir=none]
	1410753947664 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754482848 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754482944 -> 1410754482848
	1410754482944 [label="CatBackward0
------------
dim: 1"]
	1410753897632 -> 1410754482944
	1410754483040 -> 1410754482944
	1410754483040 [label=NegBackward0]
	1410753897632 -> 1410754483040
	1410754482656 -> 1410754482560
	1410754482656 -> 1401320974256 [dir=none]
	1401320974256 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754482656 -> 1410754596784 [dir=none]
	1410754596784 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754482656 -> 1401320975120 [dir=none]
	1401320975120 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754482656 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725212752 -> 1410754482656
	1401320975120 [label="ul_modules.3.input_gated_resnet.2.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401320975120 -> 1410725212752
	1410725212752 [label=AccumulateGrad]
	1410725212656 -> 1410754482656
	1401320974256 [label="ul_modules.3.input_gated_resnet.2.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320974256 -> 1410725212656
	1410725212656 [label=AccumulateGrad]
	1410725212320 -> 1410754482560
	1401320974928 [label="ul_modules.3.input_gated_resnet.2.conv_1.bias
 (256)" fillcolor=lightblue]
	1401320974928 -> 1410725212320
	1410725212320 [label=AccumulateGrad]
	1410754482512 -> 1410754482080
	1410754482512 [label=NegBackward0]
	1410754482560 -> 1410754482512
	1410754481168 -> 1410754481408
	1410754481168 -> 1401320974832 [dir=none]
	1401320974832 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754481168 -> 1410754598416 [dir=none]
	1410754598416 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754481168 -> 1401320975504 [dir=none]
	1401320975504 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754481168 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725212128 -> 1410754481168
	1401320975504 [label="ul_modules.3.input_gated_resnet.2.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401320975504 -> 1410725212128
	1410725212128 [label=AccumulateGrad]
	1410725212032 -> 1410754481168
	1401320974832 [label="ul_modules.3.input_gated_resnet.2.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401320974832 -> 1410725212032
	1410725212032 [label=AccumulateGrad]
	1410725209344 -> 1410754481408
	1401320975312 [label="ul_modules.3.input_gated_resnet.2.conv_2.bias
 (512)" fillcolor=lightblue]
	1401320975312 -> 1410725209344
	1410725209344 [label=AccumulateGrad]
	1410754249440 -> 1410753897584
	1410754249440 -> 1410754599568 [dir=none]
	1410754599568 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754249440 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410754248768 -> 1410754249440
	1410753897440 -> 1410753897344
	1410753897440 -> 1410753949488 [dir=none]
	1410753949488 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753897440 -> 1410753949296 [dir=none]
	1410753949296 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753897440 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753897680 -> 1410753897440
	1410753897680 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754482320 -> 1410753897680
	1410754482320 -> 1410753949104 [dir=none]
	1410753949104 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754482320 -> 1410753947952 [dir=none]
	1410753947952 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754482320 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754482224 -> 1410754482320
	1410754482224 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754482608 -> 1410754482224
	1410754482608 -> 1410754600624 [dir=none]
	1410754600624 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754482608 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754483136 -> 1410754482608
	1410754483136 -> 1410753949008 [dir=none]
	1410753949008 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754483136 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754482752 -> 1410754483136
	1410754482752 [label="CatBackward0
------------
dim: 1"]
	1410754483232 -> 1410754482752
	1410754483232 -> 1410753948720 [dir=none]
	1410753948720 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754483232 -> 1410753948048 [dir=none]
	1410753948048 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754483232 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754483376 -> 1410754483232
	1410754483376 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754483520 -> 1410754483376
	1410754483520 -> 1410753948528 [dir=none]
	1410753948528 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754483520 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754483616 -> 1410754483520
	1410754483616 [label="CatBackward0
------------
dim: 1"]
	1410753897488 -> 1410754483616
	1410754483712 -> 1410754483616
	1410754483712 [label=NegBackward0]
	1410753897488 -> 1410754483712
	1410754483328 -> 1410754483232
	1410754483328 -> 1401320975216 [dir=none]
	1401320975216 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754483328 -> 1410754602544 [dir=none]
	1410754602544 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754483328 -> 1401320976080 [dir=none]
	1401320976080 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754483328 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725213712 -> 1410754483328
	1401320976080 [label="ul_modules.3.input_gated_resnet.3.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401320976080 -> 1410725213712
	1410725213712 [label=AccumulateGrad]
	1410725213616 -> 1410754483328
	1401320975216 [label="ul_modules.3.input_gated_resnet.3.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320975216 -> 1410725213616
	1410725213616 [label=AccumulateGrad]
	1410725213280 -> 1410754483232
	1401320975888 [label="ul_modules.3.input_gated_resnet.3.conv_1.bias
 (256)" fillcolor=lightblue]
	1401320975888 -> 1410725213280
	1410725213280 [label=AccumulateGrad]
	1410754483184 -> 1410754482752
	1410754483184 [label=NegBackward0]
	1410754483232 -> 1410754483184
	1410754482416 -> 1410754482320
	1410754482416 -> 1401320975792 [dir=none]
	1401320975792 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754482416 -> 1410754604176 [dir=none]
	1410754604176 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754482416 -> 1401320976464 [dir=none]
	1401320976464 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754482416 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725213088 -> 1410754482416
	1401320976464 [label="ul_modules.3.input_gated_resnet.3.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401320976464 -> 1410725213088
	1410725213088 [label=AccumulateGrad]
	1410725212992 -> 1410754482416
	1401320975792 [label="ul_modules.3.input_gated_resnet.3.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401320975792 -> 1410725212992
	1410725212992 [label=AccumulateGrad]
	1410725211504 -> 1410754482320
	1401320976272 [label="ul_modules.3.input_gated_resnet.3.conv_2.bias
 (512)" fillcolor=lightblue]
	1401320976272 -> 1410725211504
	1410725211504 [label=AccumulateGrad]
	1410753897536 -> 1410753897440
	1410753897536 -> 1410754605328 [dir=none]
	1410754605328 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410753897536 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753897680 -> 1410753897536
	1410753897296 -> 1410753897200
	1410753897296 -> 1410753950352 [dir=none]
	1410753950352 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753897296 -> 1410753950160 [dir=none]
	1410753950160 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753897296 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753897392 -> 1410753897296
	1410753897392 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754482992 -> 1410753897392
	1410754482992 -> 1410753949968 [dir=none]
	1410753949968 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754482992 -> 1410753948816 [dir=none]
	1410753948816 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754482992 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754482896 -> 1410754482992
	1410754482896 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754483280 -> 1410754482896
	1410754483280 -> 1410754606384 [dir=none]
	1410754606384 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754483280 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754483808 -> 1410754483280
	1410754483808 -> 1410753949872 [dir=none]
	1410753949872 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754483808 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754483424 -> 1410754483808
	1410754483424 [label="CatBackward0
------------
dim: 1"]
	1410754483904 -> 1410754483424
	1410754483904 -> 1410753949584 [dir=none]
	1410753949584 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754483904 -> 1410753948912 [dir=none]
	1410753948912 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754483904 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754484048 -> 1410754483904
	1410754484048 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754484192 -> 1410754484048
	1410754484192 -> 1410753949392 [dir=none]
	1410753949392 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754484192 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754484288 -> 1410754484192
	1410754484288 [label="CatBackward0
------------
dim: 1"]
	1410753897344 -> 1410754484288
	1410754484384 -> 1410754484288
	1410754484384 [label=NegBackward0]
	1410753897344 -> 1410754484384
	1410754484000 -> 1410754483904
	1410754484000 -> 1401320976176 [dir=none]
	1401320976176 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754484000 -> 1410754608304 [dir=none]
	1410754608304 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754484000 -> 1401320977040 [dir=none]
	1401320977040 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754484000 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725214672 -> 1410754484000
	1401320977040 [label="ul_modules.3.input_gated_resnet.4.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401320977040 -> 1410725214672
	1410725214672 [label=AccumulateGrad]
	1410725214576 -> 1410754484000
	1401320976176 [label="ul_modules.3.input_gated_resnet.4.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320976176 -> 1410725214576
	1410725214576 [label=AccumulateGrad]
	1410725214240 -> 1410754483904
	1401320976848 [label="ul_modules.3.input_gated_resnet.4.conv_1.bias
 (256)" fillcolor=lightblue]
	1401320976848 -> 1410725214240
	1410725214240 [label=AccumulateGrad]
	1410754483856 -> 1410754483424
	1410754483856 [label=NegBackward0]
	1410754483904 -> 1410754483856
	1410754483088 -> 1410754482992
	1410754483088 -> 1401320976752 [dir=none]
	1401320976752 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754483088 -> 1410754610000 [dir=none]
	1410754610000 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754483088 -> 1401320977424 [dir=none]
	1401320977424 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754483088 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725214048 -> 1410754483088
	1401320977424 [label="ul_modules.3.input_gated_resnet.4.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401320977424 -> 1410725214048
	1410725214048 [label=AccumulateGrad]
	1410725213952 -> 1410754483088
	1401320976752 [label="ul_modules.3.input_gated_resnet.4.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401320976752 -> 1410725213952
	1410725213952 [label=AccumulateGrad]
	1410725212464 -> 1410754482992
	1401320977232 [label="ul_modules.3.input_gated_resnet.4.conv_2.bias
 (512)" fillcolor=lightblue]
	1401320977232 -> 1410725212464
	1410725212464 [label=AccumulateGrad]
	1410754482800 -> 1410753897296
	1410754482800 -> 1410754611152 [dir=none]
	1410754611152 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754482800 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753897392 -> 1410754482800
	1410753897152 -> 1410753897056
	1410753897152 -> 1410753971120 [dir=none]
	1410753971120 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753897152 -> 1410753970928 [dir=none]
	1410753970928 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753897152 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753897248 -> 1410753897152
	1410753897248 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754483664 -> 1410753897248
	1410754483664 -> 1410753970736 [dir=none]
	1410753970736 [label="input
 (1, 512, 28, 28)" fillcolor=orange]
	1410754483664 -> 1410753970256 [dir=none]
	1410753970256 [label="weight
 (512, 512, 1, 1)" fillcolor=orange]
	1410754483664 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754483568 -> 1410754483664
	1410754483568 -> 1410754612208 [dir=none]
	1410754612208 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754483568 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754483952 -> 1410754483568
	1410754483952 -> 1410753970640 [dir=none]
	1410753970640 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754483952 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754484480 -> 1410754483952
	1410754484480 [label="CatBackward0
------------
dim: 1"]
	1410754484096 -> 1410754484480
	1410754484096 [label="AddBackward0
------------
alpha: 1"]
	1410754484624 -> 1410754484096
	1410754484624 -> 1410753953328 [dir=none]
	1410753953328 [label="input
 (1, 512, 28, 28)" fillcolor=orange]
	1410754484624 -> 1410753953712 [dir=none]
	1410753953712 [label="weight
 (256, 512, 1, 1)" fillcolor=orange]
	1410754484624 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754484768 -> 1410754484624
	1410754484768 -> 1410753953616 [dir=none]
	1410753953616 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754484768 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754484912 -> 1410754484768
	1410754484912 [label="CatBackward0
------------
dim: 1"]
	1410753897200 -> 1410754484912
	1410754485008 -> 1410754484912
	1410754485008 [label=NegBackward0]
	1410753897200 -> 1410754485008
	1410754484720 -> 1410754484624
	1410754484720 -> 1401320980016 [dir=none]
	1401320980016 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754484720 -> 1410754614032 [dir=none]
	1410754614032 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754484720 -> 1401321242896 [dir=none]
	1401321242896 [label="v
 (256, 512, 1, 1)" fillcolor=orange]
	1410754484720 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725215728 -> 1410754484720
	1401321242896 [label="ul_modules.3.out_proj.conv_1.weight_v
 (256, 512, 1, 1)" fillcolor=lightblue]
	1401321242896 -> 1410725215728
	1410725215728 [label=AccumulateGrad]
	1410725215584 -> 1410754484720
	1401320980016 [label="ul_modules.3.out_proj.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320980016 -> 1410725215584
	1410725215584 [label=AccumulateGrad]
	1410725215248 -> 1410754484624
	1401321242704 [label="ul_modules.3.out_proj.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321242704 -> 1410725215248
	1410725215248 [label=AccumulateGrad]
	1410754484576 -> 1410754484096
	1410754484576 -> 1410753970352 [dir=none]
	1410753970352 [label="input
 (1, 256, 28, 28)" fillcolor=orange]
	1410754484576 -> 1410753970544 [dir=none]
	1410753970544 [label="weight
 (256, 256, 1, 1)" fillcolor=orange]
	1410754484576 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754484960 -> 1410754484576
	1410754484960 -> 1410753970448 [dir=none]
	1410753970448 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410754484960 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754484816 -> 1410754484960
	1410754484816 [label="CatBackward0
------------
dim: 1"]
	1410754485200 -> 1410754484816
	1410754485200 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (1, 1, 128, 784)"]
	1410754485344 -> 1410754485200
	1410754485344 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410754485440 -> 1410754485344
	1410754485440 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (1, 784, 128)"]
	1410754485536 -> 1410754485440
	1410754485536 -> 1410754616048 [dir=none]
	1410754616048 [label="mat2
 (1, 784, 128)" fillcolor=orange]
	1410754485536 -> 1410754616336 [dir=none]
	1410754616336 [label="self
 (1, 784, 784)" fillcolor=orange]
	1410754485536 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	1410754485632 -> 1410754485536
	1410754485632 [label="ViewBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 784)"]
	1410754485776 -> 1410754485632
	1410754485776 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 784)"]
	1410754485872 -> 1410754485776
	1410754485872 -> 1410754616624 [dir=none]
	1410754616624 [label="result
 (1, 1, 784, 784)" fillcolor=orange]
	1410754485872 [label="SoftmaxBackward0
----------------------
dim   :     4294967295
result: [saved tensor]"]
	1410754485968 -> 1410754485872
	1410754485968 -> 1410753952848 [dir=none]
	1410753952848 [label="mask
 (1, 1, 784, 784)" fillcolor=orange]
	1410754485968 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	1410754486064 -> 1410754485968
	1410754486064 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (1, 784, 784)"]
	1410754486160 -> 1410754486064
	1410754486160 -> 1410754617296 [dir=none]
	1410754617296 [label="mat2
 (1, 16, 784)" fillcolor=orange]
	1410754486160 -> 1410754617584 [dir=none]
	1410754617584 [label="self
 (1, 784, 16)" fillcolor=orange]
	1410754486160 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	1410754486256 -> 1410754486160
	1410754486256 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (1, 1, 784, 16)"]
	1410754486400 -> 1410754486256
	1410754486400 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 784, 16)"]
	1410754486496 -> 1410754486400
	1410754486496 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410754486592 -> 1410754486496
	1410754486592 -> 1410754617968 [dir=none]
	1410754617968 [label="other
 ()" fillcolor=orange]
	1410754486592 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	1410754486688 -> 1410754486592
	1410754486688 [label="ViewBackward0
----------------------------------
self_sym_sizes: (1, 1, 16, 28, 28)"]
	1410754486784 -> 1410754486688
	1410754486784 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 16, 28, 28)"]
	1410754486880 -> 1410754486784
	1410754486880 -> 1410753952752 [dir=none]
	1410753952752 [label="input
 (1, 258, 28, 28)" fillcolor=orange]
	1410754486880 -> 1410753952464 [dir=none]
	1410753952464 [label="weight
 (16, 258, 1, 1)" fillcolor=orange]
	1410754486880 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754486976 -> 1410754486880
	1410754486976 [label="AddBackward0
------------
alpha: 1"]
	1410754487120 -> 1410754486976
	1410754487120 [label="CatBackward0
------------
dim: 1"]
	1410753897200 -> 1410754487120
	1410754487072 -> 1410754486976
	1410754487072 -> 1410753952560 [dir=none]
	1410753952560 [label="other
 (1, 258, 28, 28)" fillcolor=orange]
	1410754487072 -> 1410753952368 [dir=none]
	1410753952368 [label="self
 (1, 258, 28, 28)" fillcolor=orange]
	1410754487072 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410754487168 -> 1410754487072
	1410754487168 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 516, 28, 28)
split_size    :              258"]
	1410754487408 -> 1410754487168
	1410754487408 -> 1410753951984 [dir=none]
	1410753951984 [label="input
 (1, 516, 28, 28)" fillcolor=orange]
	1410754487408 -> 1410753952080 [dir=none]
	1410753952080 [label="weight
 (516, 516, 1, 1)" fillcolor=orange]
	1410754487408 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (516,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754487504 -> 1410754487408
	1410754487504 -> 1410754619792 [dir=none]
	1410754619792 [label="result1
 (1, 516, 28, 28)" fillcolor=orange]
	1410754487504 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754487648 -> 1410754487504
	1410754487648 -> 1410753952176 [dir=none]
	1410753952176 [label="self
 (1, 516, 28, 28)" fillcolor=orange]
	1410754487648 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754487744 -> 1410754487648
	1410754487744 [label="CatBackward0
------------
dim: 1"]
	1410754487840 -> 1410754487744
	1410754487840 -> 1410753951696 [dir=none]
	1410753951696 [label="input
 (1, 516, 28, 28)" fillcolor=orange]
	1410754487840 -> 1410753951888 [dir=none]
	1410753951888 [label="weight
 (258, 516, 1, 1)" fillcolor=orange]
	1410754487840 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (258,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754487984 -> 1410754487840
	1410754487984 -> 1410753951792 [dir=none]
	1410753951792 [label="self
 (1, 516, 28, 28)" fillcolor=orange]
	1410754487984 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754488128 -> 1410754487984
	1410754488128 [label="CatBackward0
------------
dim: 1"]
	1410754487120 -> 1410754488128
	1410754488224 -> 1410754488128
	1410754488224 [label=NegBackward0]
	1410754487120 -> 1410754488224
	1410754487936 -> 1410754487840
	1410754487936 -> 1401320978672 [dir=none]
	1401320978672 [label="g
 (258, 1, 1, 1)" fillcolor=orange]
	1410754487936 -> 1410754621520 [dir=none]
	1410754621520 [label="result1
 (258, 1, 1, 1)" fillcolor=orange]
	1410754487936 -> 1401320979344 [dir=none]
	1401320979344 [label="v
 (258, 516, 1, 1)" fillcolor=orange]
	1410754487936 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570839520 -> 1410754487936
	1401320979344 [label="ul_modules.3.in_proj_q.0.conv_1.weight_v
 (258, 516, 1, 1)" fillcolor=lightblue]
	1401320979344 -> 1401570839520
	1401570839520 [label=AccumulateGrad]
	1401570839376 -> 1410754487936
	1401320978672 [label="ul_modules.3.in_proj_q.0.conv_1.weight_g
 (258, 1, 1, 1)" fillcolor=lightblue]
	1401320978672 -> 1401570839376
	1401570839376 [label=AccumulateGrad]
	1401570839040 -> 1410754487840
	1401320979152 [label="ul_modules.3.in_proj_q.0.conv_1.bias
 (258)" fillcolor=lightblue]
	1401320979152 -> 1401570839040
	1401570839040 [label=AccumulateGrad]
	1410754487792 -> 1410754487744
	1410754487792 [label=NegBackward0]
	1410754487840 -> 1410754487792
	1410754487456 -> 1410754487408
	1410754487456 -> 1401320979056 [dir=none]
	1401320979056 [label="g
 (516, 1, 1, 1)" fillcolor=orange]
	1410754487456 -> 1410754623152 [dir=none]
	1410754623152 [label="result1
 (516, 1, 1, 1)" fillcolor=orange]
	1410754487456 -> 1401320979728 [dir=none]
	1401320979728 [label="v
 (516, 516, 1, 1)" fillcolor=orange]
	1410754487456 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570838704 -> 1410754487456
	1401320979728 [label="ul_modules.3.in_proj_q.0.conv_2.weight_v
 (516, 516, 1, 1)" fillcolor=lightblue]
	1401320979728 -> 1401570838704
	1401570838704 [label=AccumulateGrad]
	1401570838848 -> 1410754487456
	1401320979056 [label="ul_modules.3.in_proj_q.0.conv_2.weight_g
 (516, 1, 1, 1)" fillcolor=lightblue]
	1401320979056 -> 1401570838848
	1401570838848 [label=AccumulateGrad]
	1401570838416 -> 1410754487408
	1401320979536 [label="ul_modules.3.in_proj_q.0.conv_2.bias
 (516)" fillcolor=lightblue]
	1401320979536 -> 1401570838416
	1401570838416 [label=AccumulateGrad]
	1410754487216 -> 1410754487072
	1410754487216 -> 1410754624304 [dir=none]
	1410754624304 [label="result
 (1, 258, 28, 28)" fillcolor=orange]
	1410754487216 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410754487168 -> 1410754487216
	1410754486928 -> 1410754486880
	1410754486928 -> 1401320979440 [dir=none]
	1401320979440 [label="g
 (16, 1, 1, 1)" fillcolor=orange]
	1410754486928 -> 1410754624880 [dir=none]
	1410754624880 [label="result1
 (16, 1, 1, 1)" fillcolor=orange]
	1410754486928 -> 1401320980304 [dir=none]
	1401320980304 [label="v
 (16, 258, 1, 1)" fillcolor=orange]
	1410754486928 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570839232 -> 1410754486928
	1401320980304 [label="ul_modules.3.in_proj_q.1.weight_v
 (16, 258, 1, 1)" fillcolor=lightblue]
	1401320980304 -> 1401570839232
	1401570839232 [label=AccumulateGrad]
	1401570838368 -> 1410754486928
	1401320979440 [label="ul_modules.3.in_proj_q.1.weight_g
 (16, 1, 1, 1)" fillcolor=lightblue]
	1401320979440 -> 1401570838368
	1401570838368 [label=AccumulateGrad]
	1401570837360 -> 1410754486880
	1401320980112 [label="ul_modules.3.in_proj_q.1.bias
 (16)" fillcolor=lightblue]
	1401320980112 -> 1401570837360
	1401570837360 [label=AccumulateGrad]
	1410754486208 -> 1410754486160
	1410754486208 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 1, 16, 784)"]
	1410754486544 -> 1410754486208
	1410754486544 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 16, 784)"]
	1410754486736 -> 1410754486544
	1410754486736 [label="ViewBackward0
----------------------------------
self_sym_sizes: (1, 1, 16, 28, 28)"]
	1410754486304 -> 1410754486736
	1410754486304 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 16, 28, 28)"]
	1410754487264 -> 1410754486304
	1410754487264 [label="SplitWithSizesBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 144, 28, 28)
split_sizes   :        (16, 128)"]
	1410754487312 -> 1410754487264
	1410754487312 -> 1410753951408 [dir=none]
	1410753951408 [label="input
 (1, 514, 28, 28)" fillcolor=orange]
	1410754487312 -> 1410753951120 [dir=none]
	1410753951120 [label="weight
 (144, 514, 1, 1)" fillcolor=orange]
	1410754487312 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (144,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754487696 -> 1410754487312
	1410754487696 [label="AddBackward0
------------
alpha: 1"]
	1410754488080 -> 1410754487696
	1410754488080 [label="CatBackward0
------------
dim: 1"]
	1410753897920 -> 1410754488080
	1410753897200 -> 1410754488080
	1410754488176 -> 1410754487696
	1410754488176 -> 1410753951216 [dir=none]
	1410753951216 [label="other
 (1, 514, 28, 28)" fillcolor=orange]
	1410754488176 -> 1410753951024 [dir=none]
	1410753951024 [label="self
 (1, 514, 28, 28)" fillcolor=orange]
	1410754488176 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410754487888 -> 1410754488176
	1410754487888 [label="SplitBackward0
---------------------------------
dim           :                 1
self_sym_sizes: (1, 1028, 28, 28)
split_size    :               514"]
	1410754488416 -> 1410754487888
	1410754488416 -> 1410753950640 [dir=none]
	1410753950640 [label="input
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754488416 -> 1410753950736 [dir=none]
	1410753950736 [label="weight
 (1028, 1028, 1, 1)" fillcolor=orange]
	1410754488416 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1028,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754488512 -> 1410754488416
	1410754488512 -> 1410754627920 [dir=none]
	1410754627920 [label="result1
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754488512 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754488656 -> 1410754488512
	1410754488656 -> 1410753950832 [dir=none]
	1410753950832 [label="self
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754488656 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754488752 -> 1410754488656
	1410754488752 [label="CatBackward0
------------
dim: 1"]
	1410754488848 -> 1410754488752
	1410754488848 -> 1410753950256 [dir=none]
	1410753950256 [label="input
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754488848 -> 1410753950448 [dir=none]
	1410753950448 [label="weight
 (514, 1028, 1, 1)" fillcolor=orange]
	1410754488848 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (514,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754488992 -> 1410754488848
	1410754488992 -> 1410753949776 [dir=none]
	1410753949776 [label="self
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754488992 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754489136 -> 1410754488992
	1410754489136 [label="CatBackward0
------------
dim: 1"]
	1410754488080 -> 1410754489136
	1410754489232 -> 1410754489136
	1410754489232 [label=NegBackward0]
	1410754488080 -> 1410754489232
	1410754488944 -> 1410754488848
	1410754488944 -> 1401320977136 [dir=none]
	1401320977136 [label="g
 (514, 1, 1, 1)" fillcolor=orange]
	1410754488944 -> 1410754629648 [dir=none]
	1410754629648 [label="result1
 (514, 1, 1, 1)" fillcolor=orange]
	1410754488944 -> 1401320978000 [dir=none]
	1401320978000 [label="v
 (514, 1028, 1, 1)" fillcolor=orange]
	1410754488944 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570840960 -> 1410754488944
	1401320978000 [label="ul_modules.3.in_proj_kv.0.conv_1.weight_v
 (514, 1028, 1, 1)" fillcolor=lightblue]
	1401320978000 -> 1401570840960
	1401570840960 [label=AccumulateGrad]
	1401570840816 -> 1410754488944
	1401320977136 [label="ul_modules.3.in_proj_kv.0.conv_1.weight_g
 (514, 1, 1, 1)" fillcolor=lightblue]
	1401320977136 -> 1401570840816
	1401570840816 [label=AccumulateGrad]
	1401570840480 -> 1410754488848
	1401320977808 [label="ul_modules.3.in_proj_kv.0.conv_1.bias
 (514)" fillcolor=lightblue]
	1401320977808 -> 1401570840480
	1401570840480 [label=AccumulateGrad]
	1410754488800 -> 1410754488752
	1410754488800 [label=NegBackward0]
	1410754488848 -> 1410754488800
	1410754488464 -> 1410754488416
	1410754488464 -> 1401320977712 [dir=none]
	1401320977712 [label="g
 (1028, 1, 1, 1)" fillcolor=orange]
	1410754488464 -> 1410754631280 [dir=none]
	1410754631280 [label="result1
 (1028, 1, 1, 1)" fillcolor=orange]
	1410754488464 -> 1401320978384 [dir=none]
	1401320978384 [label="v
 (1028, 1028, 1, 1)" fillcolor=orange]
	1410754488464 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570840144 -> 1410754488464
	1401320978384 [label="ul_modules.3.in_proj_kv.0.conv_2.weight_v
 (1028, 1028, 1, 1)" fillcolor=lightblue]
	1401320978384 -> 1401570840144
	1401570840144 [label=AccumulateGrad]
	1401570840288 -> 1410754488464
	1401320977712 [label="ul_modules.3.in_proj_kv.0.conv_2.weight_g
 (1028, 1, 1, 1)" fillcolor=lightblue]
	1401320977712 -> 1401570840288
	1401570840288 [label=AccumulateGrad]
	1401570839856 -> 1410754488416
	1401320978192 [label="ul_modules.3.in_proj_kv.0.conv_2.bias
 (1028)" fillcolor=lightblue]
	1401320978192 -> 1401570839856
	1401570839856 [label=AccumulateGrad]
	1410754488320 -> 1410754488176
	1410754488320 -> 1410754632432 [dir=none]
	1410754632432 [label="result
 (1, 514, 28, 28)" fillcolor=orange]
	1410754488320 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410754487888 -> 1410754488320
	1410754487360 -> 1410754487312
	1410754487360 -> 1401320978096 [dir=none]
	1401320978096 [label="g
 (144, 1, 1, 1)" fillcolor=orange]
	1410754487360 -> 1410754633008 [dir=none]
	1410754633008 [label="result1
 (144, 1, 1, 1)" fillcolor=orange]
	1410754487360 -> 1401320978960 [dir=none]
	1401320978960 [label="v
 (144, 514, 1, 1)" fillcolor=orange]
	1410754487360 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570840672 -> 1410754487360
	1401320978960 [label="ul_modules.3.in_proj_kv.1.weight_v
 (144, 514, 1, 1)" fillcolor=lightblue]
	1401320978960 -> 1401570840672
	1401570840672 [label=AccumulateGrad]
	1401570839808 -> 1410754487360
	1401320978096 [label="ul_modules.3.in_proj_kv.1.weight_g
 (144, 1, 1, 1)" fillcolor=lightblue]
	1401320978096 -> 1401570839808
	1401570839808 [label=AccumulateGrad]
	1401570837408 -> 1410754487312
	1401320978768 [label="ul_modules.3.in_proj_kv.1.bias
 (144)" fillcolor=lightblue]
	1401320978768 -> 1401570837408
	1401570837408 [label=AccumulateGrad]
	1410754485584 -> 1410754485536
	1410754485584 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 128)"]
	1410754485920 -> 1410754485584
	1410754485920 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 128)"]
	1410754486112 -> 1410754485920
	1410754486112 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410754486448 -> 1410754486112
	1410754486448 [label="ViewBackward0
-----------------------------------
self_sym_sizes: (1, 1, 128, 28, 28)"]
	1410754486832 -> 1410754486448
	1410754486832 [label="ViewBackward0
--------------------------------
self_sym_sizes: (1, 128, 28, 28)"]
	1410754487264 -> 1410754486832
	1410754485152 -> 1410754484816
	1410754485152 [label=NegBackward0]
	1410754485200 -> 1410754485152
	1410754484864 -> 1410754484576
	1410754484864 -> 1401320980400 [dir=none]
	1401320980400 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754484864 -> 1410754635216 [dir=none]
	1410754635216 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754484864 -> 1401321243280 [dir=none]
	1401321243280 [label="v
 (256, 256, 1, 1)" fillcolor=orange]
	1410754484864 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725216160 -> 1410754484864
	1401321243280 [label="ul_modules.3.out_proj.conv_1_sc.weight_v
 (256, 256, 1, 1)" fillcolor=lightblue]
	1401321243280 -> 1410725216160
	1410725216160 [label=AccumulateGrad]
	1410725215776 -> 1410754484864
	1401320980400 [label="ul_modules.3.out_proj.conv_1_sc.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401320980400 -> 1410725215776
	1410725215776 [label=AccumulateGrad]
	1410725215392 -> 1410754484576
	1401321243088 [label="ul_modules.3.out_proj.conv_1_sc.bias
 (256)" fillcolor=lightblue]
	1401321243088 -> 1410725215392
	1410725215392 [label=AccumulateGrad]
	1410754484432 -> 1410754484480
	1410754484432 [label=NegBackward0]
	1410754484096 -> 1410754484432
	1410754483760 -> 1410754483664
	1410754483760 -> 1401321242992 [dir=none]
	1401321242992 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754483760 -> 1410754636848 [dir=none]
	1410754636848 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754483760 -> 1401321243664 [dir=none]
	1401321243664 [label="v
 (512, 512, 1, 1)" fillcolor=orange]
	1410754483760 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1410725214480 -> 1410754483760
	1401321243664 [label="ul_modules.3.out_proj.conv_2.weight_v
 (512, 512, 1, 1)" fillcolor=lightblue]
	1401321243664 -> 1410725214480
	1410725214480 [label=AccumulateGrad]
	1410725214912 -> 1410754483760
	1401321242992 [label="ul_modules.3.out_proj.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321242992 -> 1410725214912
	1410725214912 [label=AccumulateGrad]
	1410725213424 -> 1410754483664
	1401321243472 [label="ul_modules.3.out_proj.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321243472 -> 1410725213424
	1410725213424 [label=AccumulateGrad]
	1410754483472 -> 1410753897152
	1410754483472 -> 1410754638000 [dir=none]
	1410754638000 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754483472 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753897248 -> 1410754483472
	1410753897008 -> 1410753896912
	1410753897008 -> 1410753971024 [dir=none]
	1410753971024 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753897008 -> 1410753949680 [dir=none]
	1410753949680 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753897008 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753897104 -> 1410753897008
	1410753897104 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754484336 -> 1410753897104
	1410754484336 -> 1410753952656 [dir=none]
	1410753952656 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754484336 -> 1410753946896 [dir=none]
	1410753946896 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754484336 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754485056 -> 1410754484336
	1410754485056 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754485104 -> 1410754485056
	1410754485104 -> 1410754639056 [dir=none]
	1410754639056 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754485104 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754485392 -> 1410754485104
	1410754485392 -> 1410753952944 [dir=none]
	1410753952944 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754485392 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754485248 -> 1410754485392
	1410754485248 [label="CatBackward0
------------
dim: 1"]
	1410754486016 -> 1410754485248
	1410754486016 -> 1410753953232 [dir=none]
	1410753953232 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754486016 -> 1410753953424 [dir=none]
	1410753953424 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754486016 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754487600 -> 1410754486016
	1410754487600 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754486352 -> 1410754487600
	1410754486352 -> 1410753953520 [dir=none]
	1410753953520 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754486352 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754488272 -> 1410754486352
	1410754488272 [label="CatBackward0
------------
dim: 1"]
	1410753897056 -> 1410754488272
	1410754488032 -> 1410754488272
	1410754488032 [label=NegBackward0]
	1410753897056 -> 1410754488032
	1410754486640 -> 1410754486016
	1410754486640 -> 1401321243376 [dir=none]
	1401321243376 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754486640 -> 1410754640976 [dir=none]
	1410754640976 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754486640 -> 1401321244240 [dir=none]
	1401321244240 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754486640 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570841056 -> 1410754486640
	1401321244240 [label="ul_modules.4.input_gated_resnet.0.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321244240 -> 1401570841056
	1401570841056 [label=AccumulateGrad]
	1401570840720 -> 1410754486640
	1401321243376 [label="ul_modules.4.input_gated_resnet.0.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321243376 -> 1401570840720
	1401570840720 [label=AccumulateGrad]
	1401570839280 -> 1410754486016
	1401321244048 [label="ul_modules.4.input_gated_resnet.0.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321244048 -> 1401570839280
	1401570839280 [label=AccumulateGrad]
	1410754485824 -> 1410754485248
	1410754485824 [label=NegBackward0]
	1410754486016 -> 1410754485824
	1410754484240 -> 1410754484336
	1410754484240 -> 1401321243952 [dir=none]
	1401321243952 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754484240 -> 1410754691824 [dir=none]
	1410754691824 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754484240 -> 1401321244624 [dir=none]
	1401321244624 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754484240 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570837696 -> 1410754484240
	1401321244624 [label="ul_modules.4.input_gated_resnet.0.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321244624 -> 1401570837696
	1401570837696 [label=AccumulateGrad]
	1401570837072 -> 1410754484240
	1401321243952 [label="ul_modules.4.input_gated_resnet.0.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321243952 -> 1401570837072
	1401570837072 [label=AccumulateGrad]
	1410725214384 -> 1410754484336
	1401321244432 [label="ul_modules.4.input_gated_resnet.0.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321244432 -> 1410725214384
	1410725214384 [label=AccumulateGrad]
	1410754484144 -> 1410753897008
	1410754484144 -> 1410754692976 [dir=none]
	1410754692976 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754484144 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753897104 -> 1410754484144
	1410753896864 -> 1410753896768
	1410753896864 -> 1410753972176 [dir=none]
	1410753972176 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753896864 -> 1410753971984 [dir=none]
	1410753971984 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753896864 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753896960 -> 1410753896864
	1410753896960 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754485488 -> 1410753896960
	1410754485488 -> 1410753971792 [dir=none]
	1410753971792 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754485488 -> 1410753950544 [dir=none]
	1410753950544 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754485488 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754484672 -> 1410754485488
	1410754484672 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754485680 -> 1410754484672
	1410754485680 -> 1410754694032 [dir=none]
	1410754694032 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754485680 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754488704 -> 1410754485680
	1410754488704 -> 1410753971696 [dir=none]
	1410753971696 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754488704 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754487024 -> 1410754488704
	1410754487024 [label="CatBackward0
------------
dim: 1"]
	1410754489184 -> 1410754487024
	1410754489184 -> 1410753971504 [dir=none]
	1410753971504 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754489184 -> 1410753971216 [dir=none]
	1410753971216 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754489184 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754489328 -> 1410754489184
	1410754489328 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754489376 -> 1410754489328
	1410754489376 -> 1410753953040 [dir=none]
	1410753953040 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754489376 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754489472 -> 1410754489376
	1410754489472 [label="CatBackward0
------------
dim: 1"]
	1410753896912 -> 1410754489472
	1410754489568 -> 1410754489472
	1410754489568 [label=NegBackward0]
	1410753896912 -> 1410754489568
	1410754488896 -> 1410754489184
	1410754488896 -> 1401321244336 [dir=none]
	1401321244336 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754488896 -> 1410754695952 [dir=none]
	1410754695952 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754488896 -> 1401321245200 [dir=none]
	1401321245200 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754488896 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570842016 -> 1410754488896
	1401321245200 [label="ul_modules.4.input_gated_resnet.1.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321245200 -> 1401570842016
	1401570842016 [label=AccumulateGrad]
	1401570841920 -> 1410754488896
	1401321244336 [label="ul_modules.4.input_gated_resnet.1.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321244336 -> 1401570841920
	1401570841920 [label=AccumulateGrad]
	1401570841584 -> 1410754489184
	1401321245008 [label="ul_modules.4.input_gated_resnet.1.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321245008 -> 1401570841584
	1401570841584 [label=AccumulateGrad]
	1410754488560 -> 1410754487024
	1410754488560 [label=NegBackward0]
	1410754489184 -> 1410754488560
	1410754485296 -> 1410754485488
	1410754485296 -> 1401321244912 [dir=none]
	1401321244912 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754485296 -> 1410754697584 [dir=none]
	1410754697584 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754485296 -> 1401321245584 [dir=none]
	1401321245584 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754485296 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570841392 -> 1410754485296
	1401321245584 [label="ul_modules.4.input_gated_resnet.1.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321245584 -> 1401570841392
	1401570841392 [label=AccumulateGrad]
	1401570841296 -> 1410754485296
	1401321244912 [label="ul_modules.4.input_gated_resnet.1.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321244912 -> 1401570841296
	1401570841296 [label=AccumulateGrad]
	1401570841200 -> 1410754485488
	1401321245392 [label="ul_modules.4.input_gated_resnet.1.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321245392 -> 1401570841200
	1401570841200 [label=AccumulateGrad]
	1410754484528 -> 1410753896864
	1410754484528 -> 1410754698736 [dir=none]
	1410754698736 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754484528 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753896960 -> 1410754484528
	1410753896720 -> 1410753896624
	1410753896720 -> 1410753973040 [dir=none]
	1410753973040 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753896720 -> 1410753972848 [dir=none]
	1410753972848 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753896720 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753896816 -> 1410753896720
	1410753896816 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754488608 -> 1410753896816
	1410754488608 -> 1410753972656 [dir=none]
	1410753972656 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754488608 -> 1410753971408 [dir=none]
	1410753971408 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754488608 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754487552 -> 1410754488608
	1410754487552 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754489088 -> 1410754487552
	1410754489088 -> 1410754699792 [dir=none]
	1410754699792 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754489088 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754489664 -> 1410754489088
	1410754489664 -> 1410753972560 [dir=none]
	1410753972560 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754489664 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754489280 -> 1410754489664
	1410754489280 [label="CatBackward0
------------
dim: 1"]
	1410754489760 -> 1410754489280
	1410754489760 -> 1410753972272 [dir=none]
	1410753972272 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754489760 -> 1410753971600 [dir=none]
	1410753971600 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754489760 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754489904 -> 1410754489760
	1410754489904 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754490048 -> 1410754489904
	1410754490048 -> 1410753972080 [dir=none]
	1410753972080 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754490048 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754490144 -> 1410754490048
	1410754490144 [label="CatBackward0
------------
dim: 1"]
	1410753896768 -> 1410754490144
	1410754490240 -> 1410754490144
	1410754490240 [label=NegBackward0]
	1410753896768 -> 1410754490240
	1410754489856 -> 1410754489760
	1410754489856 -> 1401321245296 [dir=none]
	1401321245296 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754489856 -> 1410754701712 [dir=none]
	1410754701712 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754489856 -> 1401321246160 [dir=none]
	1401321246160 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754489856 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570842976 -> 1410754489856
	1401321246160 [label="ul_modules.4.input_gated_resnet.2.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321246160 -> 1401570842976
	1401570842976 [label=AccumulateGrad]
	1401570842880 -> 1410754489856
	1401321245296 [label="ul_modules.4.input_gated_resnet.2.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321245296 -> 1401570842880
	1401570842880 [label=AccumulateGrad]
	1401570842544 -> 1410754489760
	1401321245968 [label="ul_modules.4.input_gated_resnet.2.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321245968 -> 1401570842544
	1401570842544 [label=AccumulateGrad]
	1410754489712 -> 1410754489280
	1410754489712 [label=NegBackward0]
	1410754489760 -> 1410754489712
	1410754488368 -> 1410754488608
	1410754488368 -> 1401321245872 [dir=none]
	1401321245872 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754488368 -> 1410754703344 [dir=none]
	1410754703344 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754488368 -> 1401321246544 [dir=none]
	1401321246544 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754488368 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570842352 -> 1410754488368
	1401321246544 [label="ul_modules.4.input_gated_resnet.2.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321246544 -> 1401570842352
	1401570842352 [label=AccumulateGrad]
	1401570842256 -> 1410754488368
	1401321245872 [label="ul_modules.4.input_gated_resnet.2.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321245872 -> 1401570842256
	1401570842256 [label=AccumulateGrad]
	1401570839568 -> 1410754488608
	1401321246352 [label="ul_modules.4.input_gated_resnet.2.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321246352 -> 1401570839568
	1401570839568 [label=AccumulateGrad]
	1410754485728 -> 1410753896720
	1410754485728 -> 1410754704496 [dir=none]
	1410754704496 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754485728 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753896816 -> 1410754485728
	1410753896576 -> 1410753896480
	1410753896576 -> 1410753973904 [dir=none]
	1410753973904 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753896576 -> 1410753973712 [dir=none]
	1410753973712 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753896576 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753896672 -> 1410753896576
	1410753896672 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754489520 -> 1410753896672
	1410754489520 -> 1410753973520 [dir=none]
	1410753973520 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754489520 -> 1410753972368 [dir=none]
	1410753972368 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754489520 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754489424 -> 1410754489520
	1410754489424 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754489808 -> 1410754489424
	1410754489808 -> 1410754705552 [dir=none]
	1410754705552 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754489808 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754490336 -> 1410754489808
	1410754490336 -> 1410753973424 [dir=none]
	1410753973424 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754490336 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754489952 -> 1410754490336
	1410754489952 [label="CatBackward0
------------
dim: 1"]
	1410754490432 -> 1410754489952
	1410754490432 -> 1410753973136 [dir=none]
	1410753973136 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754490432 -> 1410753972464 [dir=none]
	1410753972464 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754490432 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754490576 -> 1410754490432
	1410754490576 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754490720 -> 1410754490576
	1410754490720 -> 1410753972944 [dir=none]
	1410753972944 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754490720 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754490816 -> 1410754490720
	1410754490816 [label="CatBackward0
------------
dim: 1"]
	1410753896624 -> 1410754490816
	1410754490912 -> 1410754490816
	1410754490912 [label=NegBackward0]
	1410753896624 -> 1410754490912
	1410754490528 -> 1410754490432
	1410754490528 -> 1401321246256 [dir=none]
	1401321246256 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754490528 -> 1410754707376 [dir=none]
	1410754707376 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754490528 -> 1401321247120 [dir=none]
	1401321247120 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754490528 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570843936 -> 1410754490528
	1401321247120 [label="ul_modules.4.input_gated_resnet.3.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321247120 -> 1401570843936
	1401570843936 [label=AccumulateGrad]
	1401570843840 -> 1410754490528
	1401321246256 [label="ul_modules.4.input_gated_resnet.3.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321246256 -> 1401570843840
	1401570843840 [label=AccumulateGrad]
	1401570843504 -> 1410754490432
	1401321246928 [label="ul_modules.4.input_gated_resnet.3.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321246928 -> 1401570843504
	1401570843504 [label=AccumulateGrad]
	1410754490384 -> 1410754489952
	1410754490384 [label=NegBackward0]
	1410754490432 -> 1410754490384
	1410754489616 -> 1410754489520
	1410754489616 -> 1401321246832 [dir=none]
	1401321246832 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754489616 -> 1410754709168 [dir=none]
	1410754709168 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754489616 -> 1401321247504 [dir=none]
	1401321247504 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754489616 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570843312 -> 1410754489616
	1401321247504 [label="ul_modules.4.input_gated_resnet.3.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321247504 -> 1401570843312
	1401570843312 [label=AccumulateGrad]
	1401570843216 -> 1410754489616
	1401321246832 [label="ul_modules.4.input_gated_resnet.3.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321246832 -> 1401570843216
	1401570843216 [label=AccumulateGrad]
	1401570841728 -> 1410754489520
	1401321247312 [label="ul_modules.4.input_gated_resnet.3.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321247312 -> 1401570841728
	1401570841728 [label=AccumulateGrad]
	1410754489040 -> 1410753896576
	1410754489040 -> 1410754710320 [dir=none]
	1410754710320 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754489040 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753896672 -> 1410754489040
	1410753896432 -> 1410753896336
	1410753896432 -> 1410753974768 [dir=none]
	1410753974768 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753896432 -> 1410753974576 [dir=none]
	1410753974576 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753896432 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753896528 -> 1410753896432
	1410753896528 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754490192 -> 1410753896528
	1410754490192 -> 1410753974384 [dir=none]
	1410753974384 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754490192 -> 1410753973232 [dir=none]
	1410753973232 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754490192 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754490096 -> 1410754490192
	1410754490096 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754490480 -> 1410754490096
	1410754490480 -> 1410754711376 [dir=none]
	1410754711376 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754490480 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754491008 -> 1410754490480
	1410754491008 -> 1410753974288 [dir=none]
	1410753974288 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754491008 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754490624 -> 1410754491008
	1410754490624 [label="CatBackward0
------------
dim: 1"]
	1410754491104 -> 1410754490624
	1410754491104 -> 1410753974000 [dir=none]
	1410753974000 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754491104 -> 1410753973328 [dir=none]
	1410753973328 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754491104 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754491248 -> 1410754491104
	1410754491248 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754491392 -> 1410754491248
	1410754491392 -> 1410753973808 [dir=none]
	1410753973808 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754491392 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754491488 -> 1410754491392
	1410754491488 [label="CatBackward0
------------
dim: 1"]
	1410753896480 -> 1410754491488
	1410754491584 -> 1410754491488
	1410754491584 [label=NegBackward0]
	1410753896480 -> 1410754491584
	1410754491200 -> 1410754491104
	1410754491200 -> 1401321247216 [dir=none]
	1401321247216 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754491200 -> 1410754713296 [dir=none]
	1410754713296 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754491200 -> 1401321248080 [dir=none]
	1401321248080 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754491200 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570844896 -> 1410754491200
	1401321248080 [label="ul_modules.4.input_gated_resnet.4.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321248080 -> 1401570844896
	1401570844896 [label=AccumulateGrad]
	1401570844800 -> 1410754491200
	1401321247216 [label="ul_modules.4.input_gated_resnet.4.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321247216 -> 1401570844800
	1401570844800 [label=AccumulateGrad]
	1401570844464 -> 1410754491104
	1401321247888 [label="ul_modules.4.input_gated_resnet.4.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321247888 -> 1401570844464
	1401570844464 [label=AccumulateGrad]
	1410754491056 -> 1410754490624
	1410754491056 [label=NegBackward0]
	1410754491104 -> 1410754491056
	1410754490288 -> 1410754490192
	1410754490288 -> 1401321247792 [dir=none]
	1401321247792 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754490288 -> 1410754714928 [dir=none]
	1410754714928 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754490288 -> 1401321248464 [dir=none]
	1401321248464 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754490288 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570844272 -> 1410754490288
	1401321248464 [label="ul_modules.4.input_gated_resnet.4.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321248464 -> 1401570844272
	1401570844272 [label=AccumulateGrad]
	1401570844176 -> 1410754490288
	1401321247792 [label="ul_modules.4.input_gated_resnet.4.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321247792 -> 1401570844176
	1401570844176 [label=AccumulateGrad]
	1401570842688 -> 1410754490192
	1401321248272 [label="ul_modules.4.input_gated_resnet.4.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321248272 -> 1401570842688
	1401570842688 [label=AccumulateGrad]
	1410754490000 -> 1410753896432
	1410754490000 -> 1410754716080 [dir=none]
	1410754716080 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754490000 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753896528 -> 1410754490000
	1410753896288 -> 1410753896192
	1410753896288 -> 1410753979088 [dir=none]
	1410753979088 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753896288 -> 1410753978896 [dir=none]
	1410753978896 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753896288 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753896384 -> 1410753896288
	1410753896384 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754490864 -> 1410753896384
	1410754490864 -> 1410753978704 [dir=none]
	1410753978704 [label="input
 (1, 512, 28, 28)" fillcolor=orange]
	1410754490864 -> 1410753978224 [dir=none]
	1410753978224 [label="weight
 (512, 512, 1, 1)" fillcolor=orange]
	1410754490864 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754490768 -> 1410754490864
	1410754490768 -> 1410754717136 [dir=none]
	1410754717136 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754490768 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754491152 -> 1410754490768
	1410754491152 -> 1410753978608 [dir=none]
	1410753978608 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754491152 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754491680 -> 1410754491152
	1410754491680 [label="CatBackward0
------------
dim: 1"]
	1410754491296 -> 1410754491680
	1410754491296 [label="AddBackward0
------------
alpha: 1"]
	1410754491824 -> 1410754491296
	1410754491824 -> 1410753977744 [dir=none]
	1410753977744 [label="input
 (1, 512, 28, 28)" fillcolor=orange]
	1410754491824 -> 1410753978128 [dir=none]
	1410753978128 [label="weight
 (256, 512, 1, 1)" fillcolor=orange]
	1410754491824 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754491968 -> 1410754491824
	1410754491968 -> 1410753978032 [dir=none]
	1410753978032 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754491968 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754492112 -> 1410754491968
	1410754492112 [label="CatBackward0
------------
dim: 1"]
	1410753896336 -> 1410754492112
	1410754492208 -> 1410754492112
	1410754492208 [label=NegBackward0]
	1410753896336 -> 1410754492208
	1410754491920 -> 1410754491824
	1410754491920 -> 1401321251056 [dir=none]
	1401321251056 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754491920 -> 1410754718960 [dir=none]
	1410754718960 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754491920 -> 1401321251728 [dir=none]
	1401321251728 [label="v
 (256, 512, 1, 1)" fillcolor=orange]
	1410754491920 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570845952 -> 1410754491920
	1401321251728 [label="ul_modules.4.out_proj.conv_1.weight_v
 (256, 512, 1, 1)" fillcolor=lightblue]
	1401321251728 -> 1401570845952
	1401570845952 [label=AccumulateGrad]
	1401570845808 -> 1410754491920
	1401321251056 [label="ul_modules.4.out_proj.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321251056 -> 1401570845808
	1401570845808 [label=AccumulateGrad]
	1401570845472 -> 1410754491824
	1401321251536 [label="ul_modules.4.out_proj.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321251536 -> 1401570845472
	1401570845472 [label=AccumulateGrad]
	1410754491776 -> 1410754491296
	1410754491776 -> 1410753978320 [dir=none]
	1410753978320 [label="input
 (1, 256, 28, 28)" fillcolor=orange]
	1410754491776 -> 1410753978512 [dir=none]
	1410753978512 [label="weight
 (256, 256, 1, 1)" fillcolor=orange]
	1410754491776 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754492160 -> 1410754491776
	1410754492160 -> 1410753978416 [dir=none]
	1410753978416 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410754492160 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754492016 -> 1410754492160
	1410754492016 [label="CatBackward0
------------
dim: 1"]
	1410754492400 -> 1410754492016
	1410754492400 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (1, 1, 128, 784)"]
	1410754492544 -> 1410754492400
	1410754492544 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410754492640 -> 1410754492544
	1410754492640 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (1, 784, 128)"]
	1410754492736 -> 1410754492640
	1410754492736 -> 1410754720976 [dir=none]
	1410754720976 [label="mat2
 (1, 784, 128)" fillcolor=orange]
	1410754492736 -> 1410754721264 [dir=none]
	1410754721264 [label="self
 (1, 784, 784)" fillcolor=orange]
	1410754492736 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	1410754492832 -> 1410754492736
	1410754492832 [label="ViewBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 784)"]
	1410754492976 -> 1410754492832
	1410754492976 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 784)"]
	1410754493072 -> 1410754492976
	1410754493072 -> 1410754721552 [dir=none]
	1410754721552 [label="result
 (1, 1, 784, 784)" fillcolor=orange]
	1410754493072 [label="SoftmaxBackward0
----------------------
dim   :     4294967295
result: [saved tensor]"]
	1410754493168 -> 1410754493072
	1410754493168 -> 1410753977264 [dir=none]
	1410753977264 [label="mask
 (1, 1, 784, 784)" fillcolor=orange]
	1410754493168 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	1410754493264 -> 1410754493168
	1410754493264 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (1, 784, 784)"]
	1410754493360 -> 1410754493264
	1410754493360 -> 1410754722224 [dir=none]
	1410754722224 [label="mat2
 (1, 16, 784)" fillcolor=orange]
	1410754493360 -> 1410754722512 [dir=none]
	1410754722512 [label="self
 (1, 784, 16)" fillcolor=orange]
	1410754493360 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	1410754493456 -> 1410754493360
	1410754493456 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (1, 1, 784, 16)"]
	1410754493600 -> 1410754493456
	1410754493600 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 784, 16)"]
	1410754493696 -> 1410754493600
	1410754493696 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410754493792 -> 1410754493696
	1410754493792 -> 1410754722896 [dir=none]
	1410754722896 [label="other
 ()" fillcolor=orange]
	1410754493792 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	1410754493888 -> 1410754493792
	1410754493888 [label="ViewBackward0
----------------------------------
self_sym_sizes: (1, 1, 16, 28, 28)"]
	1410754493984 -> 1410754493888
	1410754493984 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 16, 28, 28)"]
	1410754494080 -> 1410754493984
	1410754494080 -> 1410753977168 [dir=none]
	1410753977168 [label="input
 (1, 258, 28, 28)" fillcolor=orange]
	1410754494080 -> 1410753976880 [dir=none]
	1410753976880 [label="weight
 (16, 258, 1, 1)" fillcolor=orange]
	1410754494080 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754494176 -> 1410754494080
	1410754494176 [label="AddBackward0
------------
alpha: 1"]
	1410754494320 -> 1410754494176
	1410754494320 [label="CatBackward0
------------
dim: 1"]
	1410753896336 -> 1410754494320
	1410754494272 -> 1410754494176
	1410754494272 -> 1410753976976 [dir=none]
	1410753976976 [label="other
 (1, 258, 28, 28)" fillcolor=orange]
	1410754494272 -> 1410753976784 [dir=none]
	1410753976784 [label="self
 (1, 258, 28, 28)" fillcolor=orange]
	1410754494272 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410754494368 -> 1410754494272
	1410754494368 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 516, 28, 28)
split_size    :              258"]
	1410754740432 -> 1410754494368
	1410754740432 -> 1410753976400 [dir=none]
	1410753976400 [label="input
 (1, 516, 28, 28)" fillcolor=orange]
	1410754740432 -> 1410753976496 [dir=none]
	1410753976496 [label="weight
 (516, 516, 1, 1)" fillcolor=orange]
	1410754740432 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (516,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754740528 -> 1410754740432
	1410754740528 -> 1410754757552 [dir=none]
	1410754757552 [label="result1
 (1, 516, 28, 28)" fillcolor=orange]
	1410754740528 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754740672 -> 1410754740528
	1410754740672 -> 1410753976592 [dir=none]
	1410753976592 [label="self
 (1, 516, 28, 28)" fillcolor=orange]
	1410754740672 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754740768 -> 1410754740672
	1410754740768 [label="CatBackward0
------------
dim: 1"]
	1410754740864 -> 1410754740768
	1410754740864 -> 1410753976112 [dir=none]
	1410753976112 [label="input
 (1, 516, 28, 28)" fillcolor=orange]
	1410754740864 -> 1410753976304 [dir=none]
	1410753976304 [label="weight
 (258, 516, 1, 1)" fillcolor=orange]
	1410754740864 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (258,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754741008 -> 1410754740864
	1410754741008 -> 1410753976208 [dir=none]
	1410753976208 [label="self
 (1, 516, 28, 28)" fillcolor=orange]
	1410754741008 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754741152 -> 1410754741008
	1410754741152 [label="CatBackward0
------------
dim: 1"]
	1410754494320 -> 1410754741152
	1410754741248 -> 1410754741152
	1410754741248 [label=NegBackward0]
	1410754494320 -> 1410754741248
	1410754740960 -> 1410754740864
	1410754740960 -> 1401321249712 [dir=none]
	1401321249712 [label="g
 (258, 1, 1, 1)" fillcolor=orange]
	1410754740960 -> 1410754759280 [dir=none]
	1410754759280 [label="result1
 (258, 1, 1, 1)" fillcolor=orange]
	1410754740960 -> 1401321250384 [dir=none]
	1401321250384 [label="v
 (258, 516, 1, 1)" fillcolor=orange]
	1410754740960 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570849456 -> 1410754740960
	1401321250384 [label="ul_modules.4.in_proj_q.0.conv_1.weight_v
 (258, 516, 1, 1)" fillcolor=lightblue]
	1401321250384 -> 1401570849456
	1401570849456 [label=AccumulateGrad]
	1401570849312 -> 1410754740960
	1401321249712 [label="ul_modules.4.in_proj_q.0.conv_1.weight_g
 (258, 1, 1, 1)" fillcolor=lightblue]
	1401321249712 -> 1401570849312
	1401570849312 [label=AccumulateGrad]
	1401570848976 -> 1410754740864
	1401321250192 [label="ul_modules.4.in_proj_q.0.conv_1.bias
 (258)" fillcolor=lightblue]
	1401321250192 -> 1401570848976
	1401570848976 [label=AccumulateGrad]
	1410754740816 -> 1410754740768
	1410754740816 [label=NegBackward0]
	1410754740864 -> 1410754740816
	1410754740480 -> 1410754740432
	1410754740480 -> 1401321250096 [dir=none]
	1401321250096 [label="g
 (516, 1, 1, 1)" fillcolor=orange]
	1410754740480 -> 1410754760912 [dir=none]
	1410754760912 [label="result1
 (516, 1, 1, 1)" fillcolor=orange]
	1410754740480 -> 1401321250768 [dir=none]
	1401321250768 [label="v
 (516, 516, 1, 1)" fillcolor=orange]
	1410754740480 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570848640 -> 1410754740480
	1401321250768 [label="ul_modules.4.in_proj_q.0.conv_2.weight_v
 (516, 516, 1, 1)" fillcolor=lightblue]
	1401321250768 -> 1401570848640
	1401570848640 [label=AccumulateGrad]
	1401570848784 -> 1410754740480
	1401321250096 [label="ul_modules.4.in_proj_q.0.conv_2.weight_g
 (516, 1, 1, 1)" fillcolor=lightblue]
	1401321250096 -> 1401570848784
	1401570848784 [label=AccumulateGrad]
	1401570848352 -> 1410754740432
	1401321250576 [label="ul_modules.4.in_proj_q.0.conv_2.bias
 (516)" fillcolor=lightblue]
	1401321250576 -> 1401570848352
	1401570848352 [label=AccumulateGrad]
	1410754494416 -> 1410754494272
	1410754494416 -> 1410754762064 [dir=none]
	1410754762064 [label="result
 (1, 258, 28, 28)" fillcolor=orange]
	1410754494416 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410754494368 -> 1410754494416
	1410754494128 -> 1410754494080
	1410754494128 -> 1401321250480 [dir=none]
	1401321250480 [label="g
 (16, 1, 1, 1)" fillcolor=orange]
	1410754494128 -> 1410754762640 [dir=none]
	1410754762640 [label="result1
 (16, 1, 1, 1)" fillcolor=orange]
	1410754494128 -> 1401321251344 [dir=none]
	1401321251344 [label="v
 (16, 258, 1, 1)" fillcolor=orange]
	1410754494128 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570849168 -> 1410754494128
	1401321251344 [label="ul_modules.4.in_proj_q.1.weight_v
 (16, 258, 1, 1)" fillcolor=lightblue]
	1401321251344 -> 1401570849168
	1401570849168 [label=AccumulateGrad]
	1401570848304 -> 1410754494128
	1401321250480 [label="ul_modules.4.in_proj_q.1.weight_g
 (16, 1, 1, 1)" fillcolor=lightblue]
	1401321250480 -> 1401570848304
	1401570848304 [label=AccumulateGrad]
	1401570847296 -> 1410754494080
	1401321251152 [label="ul_modules.4.in_proj_q.1.bias
 (16)" fillcolor=lightblue]
	1401321251152 -> 1401570847296
	1401570847296 [label=AccumulateGrad]
	1410754493408 -> 1410754493360
	1410754493408 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 1, 16, 784)"]
	1410754493744 -> 1410754493408
	1410754493744 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 16, 784)"]
	1410754493936 -> 1410754493744
	1410754493936 [label="ViewBackward0
----------------------------------
self_sym_sizes: (1, 1, 16, 28, 28)"]
	1410754493504 -> 1410754493936
	1410754493504 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 16, 28, 28)"]
	1410754494224 -> 1410754493504
	1410754494224 [label="SplitWithSizesBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 144, 28, 28)
split_sizes   :        (16, 128)"]
	1410754740336 -> 1410754494224
	1410754740336 -> 1410753975824 [dir=none]
	1410753975824 [label="input
 (1, 514, 28, 28)" fillcolor=orange]
	1410754740336 -> 1410753975536 [dir=none]
	1410753975536 [label="weight
 (144, 514, 1, 1)" fillcolor=orange]
	1410754740336 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (144,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754740720 -> 1410754740336
	1410754740720 [label="AddBackward0
------------
alpha: 1"]
	1410754741104 -> 1410754740720
	1410754741104 [label="CatBackward0
------------
dim: 1"]
	1410753897056 -> 1410754741104
	1410753896336 -> 1410754741104
	1410754741200 -> 1410754740720
	1410754741200 -> 1410753975632 [dir=none]
	1410753975632 [label="other
 (1, 514, 28, 28)" fillcolor=orange]
	1410754741200 -> 1410753975440 [dir=none]
	1410753975440 [label="self
 (1, 514, 28, 28)" fillcolor=orange]
	1410754741200 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410754740912 -> 1410754741200
	1410754740912 [label="SplitBackward0
---------------------------------
dim           :                 1
self_sym_sizes: (1, 1028, 28, 28)
split_size    :               514"]
	1410754741440 -> 1410754740912
	1410754741440 -> 1410753975056 [dir=none]
	1410753975056 [label="input
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754741440 -> 1410753975152 [dir=none]
	1410753975152 [label="weight
 (1028, 1028, 1, 1)" fillcolor=orange]
	1410754741440 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1028,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754741536 -> 1410754741440
	1410754741536 -> 1410754765616 [dir=none]
	1410754765616 [label="result1
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754741536 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754741680 -> 1410754741536
	1410754741680 -> 1410753975248 [dir=none]
	1410753975248 [label="self
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754741680 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754741776 -> 1410754741680
	1410754741776 [label="CatBackward0
------------
dim: 1"]
	1410754741872 -> 1410754741776
	1410754741872 -> 1410753974672 [dir=none]
	1410753974672 [label="input
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754741872 -> 1410753974864 [dir=none]
	1410753974864 [label="weight
 (514, 1028, 1, 1)" fillcolor=orange]
	1410754741872 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (514,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754742016 -> 1410754741872
	1410754742016 -> 1410753974192 [dir=none]
	1410753974192 [label="self
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754742016 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754742160 -> 1410754742016
	1410754742160 [label="CatBackward0
------------
dim: 1"]
	1410754741104 -> 1410754742160
	1410754742256 -> 1410754742160
	1410754742256 [label=NegBackward0]
	1410754741104 -> 1410754742256
	1410754741968 -> 1410754741872
	1410754741968 -> 1401321248176 [dir=none]
	1401321248176 [label="g
 (514, 1, 1, 1)" fillcolor=orange]
	1410754741968 -> 1410754767344 [dir=none]
	1410754767344 [label="result1
 (514, 1, 1, 1)" fillcolor=orange]
	1410754741968 -> 1401321249040 [dir=none]
	1401321249040 [label="v
 (514, 1028, 1, 1)" fillcolor=orange]
	1410754741968 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570850896 -> 1410754741968
	1401321249040 [label="ul_modules.4.in_proj_kv.0.conv_1.weight_v
 (514, 1028, 1, 1)" fillcolor=lightblue]
	1401321249040 -> 1401570850896
	1401570850896 [label=AccumulateGrad]
	1401570850752 -> 1410754741968
	1401321248176 [label="ul_modules.4.in_proj_kv.0.conv_1.weight_g
 (514, 1, 1, 1)" fillcolor=lightblue]
	1401321248176 -> 1401570850752
	1401570850752 [label=AccumulateGrad]
	1401570850416 -> 1410754741872
	1401321248848 [label="ul_modules.4.in_proj_kv.0.conv_1.bias
 (514)" fillcolor=lightblue]
	1401321248848 -> 1401570850416
	1401570850416 [label=AccumulateGrad]
	1410754741824 -> 1410754741776
	1410754741824 [label=NegBackward0]
	1410754741872 -> 1410754741824
	1410754741488 -> 1410754741440
	1410754741488 -> 1401321248752 [dir=none]
	1401321248752 [label="g
 (1028, 1, 1, 1)" fillcolor=orange]
	1410754741488 -> 1410754768976 [dir=none]
	1410754768976 [label="result1
 (1028, 1, 1, 1)" fillcolor=orange]
	1410754741488 -> 1401321249424 [dir=none]
	1401321249424 [label="v
 (1028, 1028, 1, 1)" fillcolor=orange]
	1410754741488 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570850080 -> 1410754741488
	1401321249424 [label="ul_modules.4.in_proj_kv.0.conv_2.weight_v
 (1028, 1028, 1, 1)" fillcolor=lightblue]
	1401321249424 -> 1401570850080
	1401570850080 [label=AccumulateGrad]
	1401570850224 -> 1410754741488
	1401321248752 [label="ul_modules.4.in_proj_kv.0.conv_2.weight_g
 (1028, 1, 1, 1)" fillcolor=lightblue]
	1401321248752 -> 1401570850224
	1401570850224 [label=AccumulateGrad]
	1401570849792 -> 1410754741440
	1401321249232 [label="ul_modules.4.in_proj_kv.0.conv_2.bias
 (1028)" fillcolor=lightblue]
	1401321249232 -> 1401570849792
	1401570849792 [label=AccumulateGrad]
	1410754741344 -> 1410754741200
	1410754741344 -> 1410754770128 [dir=none]
	1410754770128 [label="result
 (1, 514, 28, 28)" fillcolor=orange]
	1410754741344 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410754740912 -> 1410754741344
	1410754740384 -> 1410754740336
	1410754740384 -> 1401321249136 [dir=none]
	1401321249136 [label="g
 (144, 1, 1, 1)" fillcolor=orange]
	1410754740384 -> 1410754770704 [dir=none]
	1410754770704 [label="result1
 (144, 1, 1, 1)" fillcolor=orange]
	1410754740384 -> 1401321250000 [dir=none]
	1401321250000 [label="v
 (144, 514, 1, 1)" fillcolor=orange]
	1410754740384 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570850608 -> 1410754740384
	1401321250000 [label="ul_modules.4.in_proj_kv.1.weight_v
 (144, 514, 1, 1)" fillcolor=lightblue]
	1401321250000 -> 1401570850608
	1401570850608 [label=AccumulateGrad]
	1401570849744 -> 1410754740384
	1401321249136 [label="ul_modules.4.in_proj_kv.1.weight_g
 (144, 1, 1, 1)" fillcolor=lightblue]
	1401321249136 -> 1401570849744
	1401570849744 [label=AccumulateGrad]
	1401570847344 -> 1410754740336
	1401321249808 [label="ul_modules.4.in_proj_kv.1.bias
 (144)" fillcolor=lightblue]
	1401321249808 -> 1401570847344
	1401570847344 [label=AccumulateGrad]
	1410754492784 -> 1410754492736
	1410754492784 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 128)"]
	1410754493120 -> 1410754492784
	1410754493120 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 128)"]
	1410754493312 -> 1410754493120
	1410754493312 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410754493648 -> 1410754493312
	1410754493648 [label="ViewBackward0
-----------------------------------
self_sym_sizes: (1, 1, 128, 28, 28)"]
	1410754494032 -> 1410754493648
	1410754494032 [label="ViewBackward0
--------------------------------
self_sym_sizes: (1, 128, 28, 28)"]
	1410754494224 -> 1410754494032
	1410754492352 -> 1410754492016
	1410754492352 [label=NegBackward0]
	1410754492400 -> 1410754492352
	1410754492064 -> 1410754491776
	1410754492064 -> 1401321251440 [dir=none]
	1401321251440 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754492064 -> 1410754772912 [dir=none]
	1410754772912 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754492064 -> 1401321252112 [dir=none]
	1401321252112 [label="v
 (256, 256, 1, 1)" fillcolor=orange]
	1410754492064 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570846480 -> 1410754492064
	1401321252112 [label="ul_modules.4.out_proj.conv_1_sc.weight_v
 (256, 256, 1, 1)" fillcolor=lightblue]
	1401321252112 -> 1401570846480
	1401570846480 [label=AccumulateGrad]
	1401570846000 -> 1410754492064
	1401321251440 [label="ul_modules.4.out_proj.conv_1_sc.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321251440 -> 1401570846000
	1401570846000 [label=AccumulateGrad]
	1401570845616 -> 1410754491776
	1401321251920 [label="ul_modules.4.out_proj.conv_1_sc.bias
 (256)" fillcolor=lightblue]
	1401321251920 -> 1401570845616
	1401570845616 [label=AccumulateGrad]
	1410754491632 -> 1410754491680
	1410754491632 [label=NegBackward0]
	1410754491296 -> 1410754491632
	1410754490960 -> 1410754490864
	1410754490960 -> 1401321251824 [dir=none]
	1401321251824 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754490960 -> 1410754790992 [dir=none]
	1410754790992 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754490960 -> 1401321252496 [dir=none]
	1401321252496 [label="v
 (512, 512, 1, 1)" fillcolor=orange]
	1410754490960 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570844704 -> 1410754490960
	1401321252496 [label="ul_modules.4.out_proj.conv_2.weight_v
 (512, 512, 1, 1)" fillcolor=lightblue]
	1401321252496 -> 1401570844704
	1401570844704 [label=AccumulateGrad]
	1401570845136 -> 1410754490960
	1401321251824 [label="ul_modules.4.out_proj.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321251824 -> 1401570845136
	1401570845136 [label=AccumulateGrad]
	1401570843648 -> 1410754490864
	1401321252304 [label="ul_modules.4.out_proj.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321252304 -> 1401570843648
	1401570843648 [label=AccumulateGrad]
	1410754490672 -> 1410753896288
	1410754490672 -> 1410754792144 [dir=none]
	1410754792144 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754490672 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753896384 -> 1410754490672
	1410753896144 -> 1410753896048
	1410753896144 -> 1410753978992 [dir=none]
	1410753978992 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753896144 -> 1410753974096 [dir=none]
	1410753974096 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753896144 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753896240 -> 1410753896144
	1410753896240 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754491536 -> 1410753896240
	1410754491536 -> 1410753977072 [dir=none]
	1410753977072 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754491536 -> 1410753971312 [dir=none]
	1410753971312 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754491536 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754492256 -> 1410754491536
	1410754492256 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754492304 -> 1410754492256
	1410754492304 -> 1410754793200 [dir=none]
	1410754793200 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754492304 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754492592 -> 1410754492304
	1410754492592 -> 1410753977360 [dir=none]
	1410753977360 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754492592 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754492448 -> 1410754492592
	1410754492448 [label="CatBackward0
------------
dim: 1"]
	1410754493216 -> 1410754492448
	1410754493216 -> 1410753977648 [dir=none]
	1410753977648 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754493216 -> 1410753977840 [dir=none]
	1410753977840 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754493216 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754493552 -> 1410754493216
	1410754493552 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754492928 -> 1410754493552
	1410754492928 -> 1410753977936 [dir=none]
	1410753977936 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754492928 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754741296 -> 1410754492928
	1410754741296 [label="CatBackward0
------------
dim: 1"]
	1410753896192 -> 1410754741296
	1410754741056 -> 1410754741296
	1410754741056 [label=NegBackward0]
	1410753896192 -> 1410754741056
	1410754493840 -> 1410754493216
	1410754493840 -> 1401321252208 [dir=none]
	1401321252208 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754493840 -> 1410754795120 [dir=none]
	1410754795120 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754493840 -> 1401321253072 [dir=none]
	1401321253072 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754493840 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570850992 -> 1410754493840
	1401321253072 [label="ul_modules.5.input_gated_resnet.0.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321253072 -> 1401570850992
	1401570850992 [label=AccumulateGrad]
	1401570850656 -> 1410754493840
	1401321252208 [label="ul_modules.5.input_gated_resnet.0.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321252208 -> 1401570850656
	1401570850656 [label=AccumulateGrad]
	1401570849216 -> 1410754493216
	1401321252880 [label="ul_modules.5.input_gated_resnet.0.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321252880 -> 1401570849216
	1401570849216 [label=AccumulateGrad]
	1410754493024 -> 1410754492448
	1410754493024 [label=NegBackward0]
	1410754493216 -> 1410754493024
	1410754491440 -> 1410754491536
	1410754491440 -> 1401321252784 [dir=none]
	1401321252784 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754491440 -> 1410754796752 [dir=none]
	1410754796752 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754491440 -> 1401321253456 [dir=none]
	1401321253456 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754491440 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570847632 -> 1410754491440
	1401321253456 [label="ul_modules.5.input_gated_resnet.0.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321253456 -> 1401570847632
	1401570847632 [label=AccumulateGrad]
	1401570847008 -> 1410754491440
	1401321252784 [label="ul_modules.5.input_gated_resnet.0.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321252784 -> 1401570847008
	1401570847008 [label=AccumulateGrad]
	1401570844608 -> 1410754491536
	1401321253264 [label="ul_modules.5.input_gated_resnet.0.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321253264 -> 1401570844608
	1401570844608 [label=AccumulateGrad]
	1410754491344 -> 1410753896144
	1410754491344 -> 1410754797904 [dir=none]
	1410754797904 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754491344 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753896240 -> 1410754491344
	1410753896000 -> 1410753891584
	1410753896000 -> 1410753980144 [dir=none]
	1410753980144 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753896000 -> 1410753979952 [dir=none]
	1410753979952 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753896000 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753896096 -> 1410753896000
	1410753896096 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754492688 -> 1410753896096
	1410754492688 -> 1410753979760 [dir=none]
	1410753979760 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754492688 -> 1410753974960 [dir=none]
	1410753974960 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754492688 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754491872 -> 1410754492688
	1410754491872 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754492880 -> 1410754491872
	1410754492880 -> 1410754798960 [dir=none]
	1410754798960 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754492880 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754741728 -> 1410754492880
	1410754741728 -> 1410753979664 [dir=none]
	1410753979664 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754741728 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754740624 -> 1410754741728
	1410754740624 [label="CatBackward0
------------
dim: 1"]
	1410754742208 -> 1410754740624
	1410754742208 -> 1410753979472 [dir=none]
	1410753979472 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754742208 -> 1410753979184 [dir=none]
	1410753979184 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754742208 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754742352 -> 1410754742208
	1410754742352 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754742400 -> 1410754742352
	1410754742400 -> 1410753977456 [dir=none]
	1410753977456 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754742400 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754742496 -> 1410754742400
	1410754742496 [label="CatBackward0
------------
dim: 1"]
	1410753896048 -> 1410754742496
	1410754742592 -> 1410754742496
	1410754742592 [label=NegBackward0]
	1410753896048 -> 1410754742592
	1410754741920 -> 1410754742208
	1410754741920 -> 1401321253168 [dir=none]
	1401321253168 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754741920 -> 1410754800880 [dir=none]
	1410754800880 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754741920 -> 1401321254032 [dir=none]
	1401321254032 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754741920 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570851952 -> 1410754741920
	1401321254032 [label="ul_modules.5.input_gated_resnet.1.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321254032 -> 1401570851952
	1401570851952 [label=AccumulateGrad]
	1401570851856 -> 1410754741920
	1401321253168 [label="ul_modules.5.input_gated_resnet.1.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321253168 -> 1401570851856
	1401570851856 [label=AccumulateGrad]
	1401570851520 -> 1410754742208
	1401321253840 [label="ul_modules.5.input_gated_resnet.1.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321253840 -> 1401570851520
	1401570851520 [label=AccumulateGrad]
	1410754741584 -> 1410754740624
	1410754741584 [label=NegBackward0]
	1410754742208 -> 1410754741584
	1410754492496 -> 1410754492688
	1410754492496 -> 1401321253744 [dir=none]
	1401321253744 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754492496 -> 1410754802512 [dir=none]
	1410754802512 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754492496 -> 1401321254416 [dir=none]
	1401321254416 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754492496 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570851328 -> 1410754492496
	1401321254416 [label="ul_modules.5.input_gated_resnet.1.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321254416 -> 1401570851328
	1401570851328 [label=AccumulateGrad]
	1401570851232 -> 1410754492496
	1401321253744 [label="ul_modules.5.input_gated_resnet.1.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321253744 -> 1401570851232
	1401570851232 [label=AccumulateGrad]
	1401570846384 -> 1410754492688
	1401321254224 [label="ul_modules.5.input_gated_resnet.1.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321254224 -> 1401570846384
	1401570846384 [label=AccumulateGrad]
	1410754491728 -> 1410753896000
	1410754491728 -> 1410754803664 [dir=none]
	1410754803664 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754491728 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753896096 -> 1410754491728
	1410753891632 -> 1410753891728
	1410753891632 -> 1410753981008 [dir=none]
	1410753981008 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753891632 -> 1410753980816 [dir=none]
	1410753980816 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753891632 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753895952 -> 1410753891632
	1410753895952 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754478192 -> 1410753895952
	1410754478192 -> 1410753980624 [dir=none]
	1410753980624 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754478192 -> 1410753979376 [dir=none]
	1410753979376 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754478192 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754740576 -> 1410754478192
	1410754740576 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754742112 -> 1410754740576
	1410754742112 -> 1410754804720 [dir=none]
	1410754804720 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754742112 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754742688 -> 1410754742112
	1410754742688 -> 1410753980528 [dir=none]
	1410753980528 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754742688 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754742304 -> 1410754742688
	1410754742304 [label="CatBackward0
------------
dim: 1"]
	1410754742784 -> 1410754742304
	1410754742784 -> 1410753980240 [dir=none]
	1410753980240 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754742784 -> 1410753979568 [dir=none]
	1410753979568 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754742784 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754742928 -> 1410754742784
	1410754742928 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754743072 -> 1410754742928
	1410754743072 -> 1410753980048 [dir=none]
	1410753980048 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754743072 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754743168 -> 1410754743072
	1410754743168 [label="CatBackward0
------------
dim: 1"]
	1410753891584 -> 1410754743168
	1410754743264 -> 1410754743168
	1410754743264 [label=NegBackward0]
	1410753891584 -> 1410754743264
	1410754742880 -> 1410754742784
	1410754742880 -> 1401317232944 [dir=none]
	1401317232944 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754742880 -> 1410754839472 [dir=none]
	1410754839472 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754742880 -> 1401321254896 [dir=none]
	1401321254896 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754742880 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570852672 -> 1410754742880
	1401321254896 [label="ul_modules.5.input_gated_resnet.2.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321254896 -> 1401570852672
	1401570852672 [label=AccumulateGrad]
	1401570852720 -> 1410754742880
	1401317232944 [label="ul_modules.5.input_gated_resnet.2.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401317232944 -> 1401570852720
	1401570852720 [label=AccumulateGrad]
	1401570852480 -> 1410754742784
	1401321254704 [label="ul_modules.5.input_gated_resnet.2.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321254704 -> 1401570852480
	1401570852480 [label=AccumulateGrad]
	1410754742736 -> 1410754742304
	1410754742736 [label=NegBackward0]
	1410754742784 -> 1410754742736
	1410754741392 -> 1410754478192
	1410754741392 -> 1401321254608 [dir=none]
	1401321254608 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754741392 -> 1410754841104 [dir=none]
	1410754841104 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754741392 -> 1401321255280 [dir=none]
	1401321255280 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754741392 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570852288 -> 1410754741392
	1401321255280 [label="ul_modules.5.input_gated_resnet.2.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321255280 -> 1401570852288
	1401570852288 [label=AccumulateGrad]
	1401570852192 -> 1410754741392
	1401321254608 [label="ul_modules.5.input_gated_resnet.2.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321254608 -> 1401570852192
	1401570852192 [label=AccumulateGrad]
	1401570849504 -> 1410754478192
	1401321255088 [label="ul_modules.5.input_gated_resnet.2.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321255088 -> 1401570849504
	1401570849504 [label=AccumulateGrad]
	1410754478528 -> 1410753891632
	1410754478528 -> 1410754842256 [dir=none]
	1410754842256 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754478528 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753895952 -> 1410754478528
	1410753891776 -> 1410753891872
	1410753891776 -> 1410753981872 [dir=none]
	1410753981872 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753891776 -> 1410753981680 [dir=none]
	1410753981680 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753891776 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410754481840 -> 1410753891776
	1410754481840 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754742544 -> 1410754481840
	1410754742544 -> 1410753981488 [dir=none]
	1410753981488 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754742544 -> 1410753980336 [dir=none]
	1410753980336 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754742544 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754742448 -> 1410754742544
	1410754742448 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754742832 -> 1410754742448
	1410754742832 -> 1410754843312 [dir=none]
	1410754843312 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754742832 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754743360 -> 1410754742832
	1410754743360 -> 1410753981392 [dir=none]
	1410753981392 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754743360 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754742976 -> 1410754743360
	1410754742976 [label="CatBackward0
------------
dim: 1"]
	1410754743456 -> 1410754742976
	1410754743456 -> 1410753981104 [dir=none]
	1410753981104 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754743456 -> 1410753980432 [dir=none]
	1410753980432 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754743456 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754743600 -> 1410754743456
	1410754743600 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754743744 -> 1410754743600
	1410754743744 -> 1410753980912 [dir=none]
	1410753980912 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754743744 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754743840 -> 1410754743744
	1410754743840 [label="CatBackward0
------------
dim: 1"]
	1410753891728 -> 1410754743840
	1410754743936 -> 1410754743840
	1410754743936 [label=NegBackward0]
	1410753891728 -> 1410754743936
	1410754743552 -> 1410754743456
	1410754743552 -> 1401321254992 [dir=none]
	1401321254992 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754743552 -> 1410754845232 [dir=none]
	1410754845232 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754743552 -> 1401321255856 [dir=none]
	1401321255856 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754743552 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571034160 -> 1410754743552
	1401321255856 [label="ul_modules.5.input_gated_resnet.3.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321255856 -> 1401571034160
	1401571034160 [label=AccumulateGrad]
	1401571034064 -> 1410754743552
	1401321254992 [label="ul_modules.5.input_gated_resnet.3.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321254992 -> 1401571034064
	1401571034064 [label=AccumulateGrad]
	1401571033728 -> 1410754743456
	1401321255664 [label="ul_modules.5.input_gated_resnet.3.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321255664 -> 1401571033728
	1401571033728 [label=AccumulateGrad]
	1410754743408 -> 1410754742976
	1410754743408 [label=NegBackward0]
	1410754743456 -> 1410754743408
	1410754742640 -> 1410754742544
	1410754742640 -> 1401321255568 [dir=none]
	1401321255568 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754742640 -> 1410754846864 [dir=none]
	1410754846864 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754742640 -> 1401321256240 [dir=none]
	1401321256240 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754742640 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571033536 -> 1410754742640
	1401321256240 [label="ul_modules.5.input_gated_resnet.3.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321256240 -> 1401571033536
	1401571033536 [label=AccumulateGrad]
	1401571033440 -> 1410754742640
	1401321255568 [label="ul_modules.5.input_gated_resnet.3.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321255568 -> 1401571033440
	1401571033440 [label=AccumulateGrad]
	1401571033344 -> 1410754742544
	1401321256048 [label="ul_modules.5.input_gated_resnet.3.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321256048 -> 1401571033344
	1401571033344 [label=AccumulateGrad]
	1410753891680 -> 1410753891776
	1410753891680 -> 1410754848016 [dir=none]
	1410754848016 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410753891680 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410754481840 -> 1410753891680
	1410753891920 -> 1410753895472
	1410753891920 -> 1410753982736 [dir=none]
	1410753982736 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753891920 -> 1410753982544 [dir=none]
	1410753982544 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753891920 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753891824 -> 1410753891920
	1410753891824 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754743216 -> 1410753891824
	1410754743216 -> 1410753982352 [dir=none]
	1410753982352 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754743216 -> 1410753981200 [dir=none]
	1410753981200 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754743216 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754743120 -> 1410754743216
	1410754743120 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754743504 -> 1410754743120
	1410754743504 -> 1410754849072 [dir=none]
	1410754849072 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754743504 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754744032 -> 1410754743504
	1410754744032 -> 1410753982256 [dir=none]
	1410753982256 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754744032 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754743648 -> 1410754744032
	1410754743648 [label="CatBackward0
------------
dim: 1"]
	1410754744128 -> 1410754743648
	1410754744128 -> 1410753981968 [dir=none]
	1410753981968 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754744128 -> 1410753981296 [dir=none]
	1410753981296 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754744128 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754744272 -> 1410754744128
	1410754744272 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754744416 -> 1410754744272
	1410754744416 -> 1410753981776 [dir=none]
	1410753981776 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754744416 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754744512 -> 1410754744416
	1410754744512 [label="CatBackward0
------------
dim: 1"]
	1410753891872 -> 1410754744512
	1410754744608 -> 1410754744512
	1410754744608 [label=NegBackward0]
	1410753891872 -> 1410754744608
	1410754744224 -> 1410754744128
	1410754744224 -> 1401321255952 [dir=none]
	1401321255952 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754744224 -> 1410754850992 [dir=none]
	1410754850992 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754744224 -> 1401321256816 [dir=none]
	1401321256816 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754744224 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571035120 -> 1410754744224
	1401321256816 [label="ul_modules.5.input_gated_resnet.4.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321256816 -> 1401571035120
	1401571035120 [label=AccumulateGrad]
	1401571035024 -> 1410754744224
	1401321255952 [label="ul_modules.5.input_gated_resnet.4.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321255952 -> 1401571035024
	1401571035024 [label=AccumulateGrad]
	1401571034688 -> 1410754744128
	1401321256624 [label="ul_modules.5.input_gated_resnet.4.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321256624 -> 1401571034688
	1401571034688 [label=AccumulateGrad]
	1410754744080 -> 1410754743648
	1410754744080 [label=NegBackward0]
	1410754744128 -> 1410754744080
	1410754743312 -> 1410754743216
	1410754743312 -> 1401321256528 [dir=none]
	1401321256528 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754743312 -> 1410754852624 [dir=none]
	1410754852624 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754743312 -> 1401321257200 [dir=none]
	1401321257200 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754743312 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571034496 -> 1410754743312
	1401321257200 [label="ul_modules.5.input_gated_resnet.4.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321257200 -> 1401571034496
	1401571034496 [label=AccumulateGrad]
	1401571034400 -> 1410754743312
	1401321256528 [label="ul_modules.5.input_gated_resnet.4.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321256528 -> 1401571034400
	1401571034400 [label=AccumulateGrad]
	1401571034304 -> 1410754743216
	1401321257008 [label="ul_modules.5.input_gated_resnet.4.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321257008 -> 1401571034304
	1401571034304 [label=AccumulateGrad]
	1410754743024 -> 1410753891920
	1410754743024 -> 1410754853776 [dir=none]
	1410754853776 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754743024 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753891824 -> 1410754743024
	1410753895616 -> 1410753891200
	1410753895616 -> 1410753987120 [dir=none]
	1410753987120 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753895616 -> 1410753986928 [dir=none]
	1410753986928 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753895616 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753895520 -> 1410753895616
	1410753895520 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754743888 -> 1410753895520
	1410754743888 -> 1410753986736 [dir=none]
	1410753986736 [label="input
 (1, 512, 28, 28)" fillcolor=orange]
	1410754743888 -> 1410753986192 [dir=none]
	1410753986192 [label="weight
 (512, 512, 1, 1)" fillcolor=orange]
	1410754743888 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754743792 -> 1410754743888
	1410754743792 -> 1410754854832 [dir=none]
	1410754854832 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754743792 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754744176 -> 1410754743792
	1410754744176 -> 1410753986640 [dir=none]
	1410753986640 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754744176 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754744704 -> 1410754744176
	1410754744704 [label="CatBackward0
------------
dim: 1"]
	1410754744320 -> 1410754744704
	1410754744320 [label="AddBackward0
------------
alpha: 1"]
	1410754744848 -> 1410754744320
	1410754744848 -> 1410753985712 [dir=none]
	1410753985712 [label="input
 (1, 512, 28, 28)" fillcolor=orange]
	1410754744848 -> 1410753986096 [dir=none]
	1410753986096 [label="weight
 (256, 512, 1, 1)" fillcolor=orange]
	1410754744848 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754744992 -> 1410754744848
	1410754744992 -> 1410753986000 [dir=none]
	1410753986000 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754744992 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754745136 -> 1410754744992
	1410754745136 [label="CatBackward0
------------
dim: 1"]
	1410753895472 -> 1410754745136
	1410754745232 -> 1410754745136
	1410754745232 [label=NegBackward0]
	1410753895472 -> 1410754745232
	1410754744944 -> 1410754744848
	1410754744944 -> 1401321522000 [dir=none]
	1401321522000 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754744944 -> 1410754856720 [dir=none]
	1410754856720 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754744944 -> 1401321522672 [dir=none]
	1401321522672 [label="v
 (256, 512, 1, 1)" fillcolor=orange]
	1410754744944 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571036176 -> 1410754744944
	1401321522672 [label="ul_modules.5.out_proj.conv_1.weight_v
 (256, 512, 1, 1)" fillcolor=lightblue]
	1401321522672 -> 1401571036176
	1401571036176 [label=AccumulateGrad]
	1401571036032 -> 1410754744944
	1401321522000 [label="ul_modules.5.out_proj.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321522000 -> 1401571036032
	1401571036032 [label=AccumulateGrad]
	1401571035696 -> 1410754744848
	1401321522480 [label="ul_modules.5.out_proj.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321522480 -> 1401571035696
	1401571035696 [label=AccumulateGrad]
	1410754744800 -> 1410754744320
	1410754744800 -> 1410753986288 [dir=none]
	1410753986288 [label="input
 (1, 256, 28, 28)" fillcolor=orange]
	1410754744800 -> 1410753986480 [dir=none]
	1410753986480 [label="weight
 (256, 256, 1, 1)" fillcolor=orange]
	1410754744800 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754745184 -> 1410754744800
	1410754745184 -> 1410753986384 [dir=none]
	1410753986384 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410754745184 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754745040 -> 1410754745184
	1410754745040 [label="CatBackward0
------------
dim: 1"]
	1410754745424 -> 1410754745040
	1410754745424 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (1, 1, 128, 784)"]
	1410754745568 -> 1410754745424
	1410754745568 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410754745664 -> 1410754745568
	1410754745664 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (1, 784, 128)"]
	1410754745760 -> 1410754745664
	1410754745760 -> 1410754858736 [dir=none]
	1410754858736 [label="mat2
 (1, 784, 128)" fillcolor=orange]
	1410754745760 -> 1410754859024 [dir=none]
	1410754859024 [label="self
 (1, 784, 784)" fillcolor=orange]
	1410754745760 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	1410754745856 -> 1410754745760
	1410754745856 [label="ViewBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 784)"]
	1410754746000 -> 1410754745856
	1410754746000 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 784)"]
	1410754746096 -> 1410754746000
	1410754746096 -> 1410754859312 [dir=none]
	1410754859312 [label="result
 (1, 1, 784, 784)" fillcolor=orange]
	1410754746096 [label="SoftmaxBackward0
----------------------
dim   :     4294967295
result: [saved tensor]"]
	1410754746192 -> 1410754746096
	1410754746192 -> 1410753985232 [dir=none]
	1410753985232 [label="mask
 (1, 1, 784, 784)" fillcolor=orange]
	1410754746192 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	1410754746288 -> 1410754746192
	1410754746288 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (1, 784, 784)"]
	1410754746384 -> 1410754746288
	1410754746384 -> 1410754859984 [dir=none]
	1410754859984 [label="mat2
 (1, 16, 784)" fillcolor=orange]
	1410754746384 -> 1410754860272 [dir=none]
	1410754860272 [label="self
 (1, 784, 16)" fillcolor=orange]
	1410754746384 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	1410754746480 -> 1410754746384
	1410754746480 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (1, 1, 784, 16)"]
	1410754746624 -> 1410754746480
	1410754746624 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 784, 16)"]
	1410754746720 -> 1410754746624
	1410754746720 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410754746816 -> 1410754746720
	1410754746816 -> 1410754860656 [dir=none]
	1410754860656 [label="other
 ()" fillcolor=orange]
	1410754746816 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	1410754746912 -> 1410754746816
	1410754746912 [label="ViewBackward0
----------------------------------
self_sym_sizes: (1, 1, 16, 28, 28)"]
	1410754747008 -> 1410754746912
	1410754747008 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 16, 28, 28)"]
	1410754747104 -> 1410754747008
	1410754747104 -> 1410753985136 [dir=none]
	1410753985136 [label="input
 (1, 258, 28, 28)" fillcolor=orange]
	1410754747104 -> 1410753984848 [dir=none]
	1410753984848 [label="weight
 (16, 258, 1, 1)" fillcolor=orange]
	1410754747104 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754747200 -> 1410754747104
	1410754747200 [label="AddBackward0
------------
alpha: 1"]
	1410754747344 -> 1410754747200
	1410754747344 [label="CatBackward0
------------
dim: 1"]
	1410753895472 -> 1410754747344
	1410754747296 -> 1410754747200
	1410754747296 -> 1410753984944 [dir=none]
	1410753984944 [label="other
 (1, 258, 28, 28)" fillcolor=orange]
	1410754747296 -> 1410753984752 [dir=none]
	1410753984752 [label="self
 (1, 258, 28, 28)" fillcolor=orange]
	1410754747296 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410754747392 -> 1410754747296
	1410754747392 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 516, 28, 28)
split_size    :              258"]
	1410754747632 -> 1410754747392
	1410754747632 -> 1410753984368 [dir=none]
	1410753984368 [label="input
 (1, 516, 28, 28)" fillcolor=orange]
	1410754747632 -> 1410753984464 [dir=none]
	1410753984464 [label="weight
 (516, 516, 1, 1)" fillcolor=orange]
	1410754747632 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (516,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754747728 -> 1410754747632
	1410754747728 -> 1410754862480 [dir=none]
	1410754862480 [label="result1
 (1, 516, 28, 28)" fillcolor=orange]
	1410754747728 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754747872 -> 1410754747728
	1410754747872 -> 1410753984560 [dir=none]
	1410753984560 [label="self
 (1, 516, 28, 28)" fillcolor=orange]
	1410754747872 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754747968 -> 1410754747872
	1410754747968 [label="CatBackward0
------------
dim: 1"]
	1410754748064 -> 1410754747968
	1410754748064 -> 1410753984080 [dir=none]
	1410753984080 [label="input
 (1, 516, 28, 28)" fillcolor=orange]
	1410754748064 -> 1410753984272 [dir=none]
	1410753984272 [label="weight
 (258, 516, 1, 1)" fillcolor=orange]
	1410754748064 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (258,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754748208 -> 1410754748064
	1410754748208 -> 1410753984176 [dir=none]
	1410753984176 [label="self
 (1, 516, 28, 28)" fillcolor=orange]
	1410754748208 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754748352 -> 1410754748208
	1410754748352 [label="CatBackward0
------------
dim: 1"]
	1410754747344 -> 1410754748352
	1410754748448 -> 1410754748352
	1410754748448 [label=NegBackward0]
	1410754747344 -> 1410754748448
	1410754748160 -> 1410754748064
	1410754748160 -> 1401321258448 [dir=none]
	1401321258448 [label="g
 (258, 1, 1, 1)" fillcolor=orange]
	1410754748160 -> 1410754864208 [dir=none]
	1410754864208 [label="result1
 (258, 1, 1, 1)" fillcolor=orange]
	1410754748160 -> 1401321521328 [dir=none]
	1401321521328 [label="v
 (258, 516, 1, 1)" fillcolor=orange]
	1410754748160 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571039680 -> 1410754748160
	1401321521328 [label="ul_modules.5.in_proj_q.0.conv_1.weight_v
 (258, 516, 1, 1)" fillcolor=lightblue]
	1401321521328 -> 1401571039680
	1401571039680 [label=AccumulateGrad]
	1401571039536 -> 1410754748160
	1401321258448 [label="ul_modules.5.in_proj_q.0.conv_1.weight_g
 (258, 1, 1, 1)" fillcolor=lightblue]
	1401321258448 -> 1401571039536
	1401571039536 [label=AccumulateGrad]
	1401571039200 -> 1410754748064
	1401321258928 [label="ul_modules.5.in_proj_q.0.conv_1.bias
 (258)" fillcolor=lightblue]
	1401321258928 -> 1401571039200
	1401571039200 [label=AccumulateGrad]
	1410754748016 -> 1410754747968
	1410754748016 [label=NegBackward0]
	1410754748064 -> 1410754748016
	1410754747680 -> 1410754747632
	1410754747680 -> 1401321258832 [dir=none]
	1401321258832 [label="g
 (516, 1, 1, 1)" fillcolor=orange]
	1410754747680 -> 1410754865840 [dir=none]
	1410754865840 [label="result1
 (516, 1, 1, 1)" fillcolor=orange]
	1410754747680 -> 1401321521712 [dir=none]
	1401321521712 [label="v
 (516, 516, 1, 1)" fillcolor=orange]
	1410754747680 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571038864 -> 1410754747680
	1401321521712 [label="ul_modules.5.in_proj_q.0.conv_2.weight_v
 (516, 516, 1, 1)" fillcolor=lightblue]
	1401321521712 -> 1401571038864
	1401571038864 [label=AccumulateGrad]
	1401571039008 -> 1410754747680
	1401321258832 [label="ul_modules.5.in_proj_q.0.conv_2.weight_g
 (516, 1, 1, 1)" fillcolor=lightblue]
	1401321258832 -> 1401571039008
	1401571039008 [label=AccumulateGrad]
	1401571038576 -> 1410754747632
	1401321521520 [label="ul_modules.5.in_proj_q.0.conv_2.bias
 (516)" fillcolor=lightblue]
	1401321521520 -> 1401571038576
	1401571038576 [label=AccumulateGrad]
	1410754747440 -> 1410754747296
	1410754747440 -> 1410754866992 [dir=none]
	1410754866992 [label="result
 (1, 258, 28, 28)" fillcolor=orange]
	1410754747440 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410754747392 -> 1410754747440
	1410754747152 -> 1410754747104
	1410754747152 -> 1401321521424 [dir=none]
	1401321521424 [label="g
 (16, 1, 1, 1)" fillcolor=orange]
	1410754747152 -> 1410754867568 [dir=none]
	1410754867568 [label="result1
 (16, 1, 1, 1)" fillcolor=orange]
	1410754747152 -> 1401321522288 [dir=none]
	1401321522288 [label="v
 (16, 258, 1, 1)" fillcolor=orange]
	1410754747152 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571039392 -> 1410754747152
	1401321522288 [label="ul_modules.5.in_proj_q.1.weight_v
 (16, 258, 1, 1)" fillcolor=lightblue]
	1401321522288 -> 1401571039392
	1401571039392 [label=AccumulateGrad]
	1401571038528 -> 1410754747152
	1401321521424 [label="ul_modules.5.in_proj_q.1.weight_g
 (16, 1, 1, 1)" fillcolor=lightblue]
	1401321521424 -> 1401571038528
	1401571038528 [label=AccumulateGrad]
	1401571037520 -> 1410754747104
	1401321522096 [label="ul_modules.5.in_proj_q.1.bias
 (16)" fillcolor=lightblue]
	1401321522096 -> 1401571037520
	1401571037520 [label=AccumulateGrad]
	1410754746432 -> 1410754746384
	1410754746432 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 1, 16, 784)"]
	1410754746768 -> 1410754746432
	1410754746768 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 16, 784)"]
	1410754746960 -> 1410754746768
	1410754746960 [label="ViewBackward0
----------------------------------
self_sym_sizes: (1, 1, 16, 28, 28)"]
	1410754746528 -> 1410754746960
	1410754746528 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 16, 28, 28)"]
	1410754747488 -> 1410754746528
	1410754747488 [label="SplitWithSizesBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 144, 28, 28)
split_sizes   :        (16, 128)"]
	1410754747536 -> 1410754747488
	1410754747536 -> 1410753983792 [dir=none]
	1410753983792 [label="input
 (1, 514, 28, 28)" fillcolor=orange]
	1410754747536 -> 1410753983504 [dir=none]
	1410753983504 [label="weight
 (144, 514, 1, 1)" fillcolor=orange]
	1410754747536 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (144,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754747920 -> 1410754747536
	1410754747920 [label="AddBackward0
------------
alpha: 1"]
	1410754748304 -> 1410754747920
	1410754748304 [label="CatBackward0
------------
dim: 1"]
	1410753896192 -> 1410754748304
	1410753895472 -> 1410754748304
	1410754748400 -> 1410754747920
	1410754748400 -> 1410753983600 [dir=none]
	1410753983600 [label="other
 (1, 514, 28, 28)" fillcolor=orange]
	1410754748400 -> 1410753983408 [dir=none]
	1410753983408 [label="self
 (1, 514, 28, 28)" fillcolor=orange]
	1410754748400 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410754748112 -> 1410754748400
	1410754748112 [label="SplitBackward0
---------------------------------
dim           :                 1
self_sym_sizes: (1, 1028, 28, 28)
split_size    :               514"]
	1410754748640 -> 1410754748112
	1410754748640 -> 1410753983024 [dir=none]
	1410753983024 [label="input
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754748640 -> 1410753983120 [dir=none]
	1410753983120 [label="weight
 (1028, 1028, 1, 1)" fillcolor=orange]
	1410754748640 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1028,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754748736 -> 1410754748640
	1410754748736 -> 1410754870544 [dir=none]
	1410754870544 [label="result1
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754748736 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754748880 -> 1410754748736
	1410754748880 -> 1410753983216 [dir=none]
	1410753983216 [label="self
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754748880 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754748976 -> 1410754748880
	1410754748976 [label="CatBackward0
------------
dim: 1"]
	1410754749072 -> 1410754748976
	1410754749072 -> 1410753982640 [dir=none]
	1410753982640 [label="input
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754749072 -> 1410753982832 [dir=none]
	1410753982832 [label="weight
 (514, 1028, 1, 1)" fillcolor=orange]
	1410754749072 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (514,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754749216 -> 1410754749072
	1410754749216 -> 1410753982160 [dir=none]
	1410753982160 [label="self
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754749216 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754749360 -> 1410754749216
	1410754749360 [label="CatBackward0
------------
dim: 1"]
	1410754748304 -> 1410754749360
	1410754749456 -> 1410754749360
	1410754749456 [label=NegBackward0]
	1410754748304 -> 1410754749456
	1410754749168 -> 1410754749072
	1410754749168 -> 1401321256912 [dir=none]
	1401321256912 [label="g
 (514, 1, 1, 1)" fillcolor=orange]
	1410754749168 -> 1410956215312 [dir=none]
	1410956215312 [label="result1
 (514, 1, 1, 1)" fillcolor=orange]
	1410754749168 -> 1401321257776 [dir=none]
	1401321257776 [label="v
 (514, 1028, 1, 1)" fillcolor=orange]
	1410754749168 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571041120 -> 1410754749168
	1401321257776 [label="ul_modules.5.in_proj_kv.0.conv_1.weight_v
 (514, 1028, 1, 1)" fillcolor=lightblue]
	1401321257776 -> 1401571041120
	1401571041120 [label=AccumulateGrad]
	1401571040976 -> 1410754749168
	1401321256912 [label="ul_modules.5.in_proj_kv.0.conv_1.weight_g
 (514, 1, 1, 1)" fillcolor=lightblue]
	1401321256912 -> 1401571040976
	1401571040976 [label=AccumulateGrad]
	1401571040640 -> 1410754749072
	1401321257584 [label="ul_modules.5.in_proj_kv.0.conv_1.bias
 (514)" fillcolor=lightblue]
	1401321257584 -> 1401571040640
	1401571040640 [label=AccumulateGrad]
	1410754749024 -> 1410754748976
	1410754749024 [label=NegBackward0]
	1410754749072 -> 1410754749024
	1410754748688 -> 1410754748640
	1410754748688 -> 1401321257488 [dir=none]
	1401321257488 [label="g
 (1028, 1, 1, 1)" fillcolor=orange]
	1410754748688 -> 1410956216944 [dir=none]
	1410956216944 [label="result1
 (1028, 1, 1, 1)" fillcolor=orange]
	1410754748688 -> 1401321258160 [dir=none]
	1401321258160 [label="v
 (1028, 1028, 1, 1)" fillcolor=orange]
	1410754748688 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571040304 -> 1410754748688
	1401321258160 [label="ul_modules.5.in_proj_kv.0.conv_2.weight_v
 (1028, 1028, 1, 1)" fillcolor=lightblue]
	1401321258160 -> 1401571040304
	1401571040304 [label=AccumulateGrad]
	1401571040448 -> 1410754748688
	1401321257488 [label="ul_modules.5.in_proj_kv.0.conv_2.weight_g
 (1028, 1, 1, 1)" fillcolor=lightblue]
	1401321257488 -> 1401571040448
	1401571040448 [label=AccumulateGrad]
	1401571040016 -> 1410754748640
	1401321257968 [label="ul_modules.5.in_proj_kv.0.conv_2.bias
 (1028)" fillcolor=lightblue]
	1401321257968 -> 1401571040016
	1401571040016 [label=AccumulateGrad]
	1410754748544 -> 1410754748400
	1410754748544 -> 1410956218096 [dir=none]
	1410956218096 [label="result
 (1, 514, 28, 28)" fillcolor=orange]
	1410754748544 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410754748112 -> 1410754748544
	1410754747584 -> 1410754747536
	1410754747584 -> 1401321257872 [dir=none]
	1401321257872 [label="g
 (144, 1, 1, 1)" fillcolor=orange]
	1410754747584 -> 1410956218672 [dir=none]
	1410956218672 [label="result1
 (144, 1, 1, 1)" fillcolor=orange]
	1410754747584 -> 1401321258736 [dir=none]
	1401321258736 [label="v
 (144, 514, 1, 1)" fillcolor=orange]
	1410754747584 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571040832 -> 1410754747584
	1401321258736 [label="ul_modules.5.in_proj_kv.1.weight_v
 (144, 514, 1, 1)" fillcolor=lightblue]
	1401321258736 -> 1401571040832
	1401571040832 [label=AccumulateGrad]
	1401571039968 -> 1410754747584
	1401321257872 [label="ul_modules.5.in_proj_kv.1.weight_g
 (144, 1, 1, 1)" fillcolor=lightblue]
	1401321257872 -> 1401571039968
	1401571039968 [label=AccumulateGrad]
	1401571037568 -> 1410754747536
	1401321258544 [label="ul_modules.5.in_proj_kv.1.bias
 (144)" fillcolor=lightblue]
	1401321258544 -> 1401571037568
	1401571037568 [label=AccumulateGrad]
	1410754745808 -> 1410754745760
	1410754745808 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 128)"]
	1410754746144 -> 1410754745808
	1410754746144 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 128)"]
	1410754746336 -> 1410754746144
	1410754746336 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410754746672 -> 1410754746336
	1410754746672 [label="ViewBackward0
-----------------------------------
self_sym_sizes: (1, 1, 128, 28, 28)"]
	1410754747056 -> 1410754746672
	1410754747056 [label="ViewBackward0
--------------------------------
self_sym_sizes: (1, 128, 28, 28)"]
	1410754747488 -> 1410754747056
	1410754745376 -> 1410754745040
	1410754745376 [label=NegBackward0]
	1410754745424 -> 1410754745376
	1410754745088 -> 1410754744800
	1410754745088 -> 1401321522384 [dir=none]
	1401321522384 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754745088 -> 1410956220880 [dir=none]
	1410956220880 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754745088 -> 1401321523056 [dir=none]
	1401321523056 [label="v
 (256, 256, 1, 1)" fillcolor=orange]
	1410754745088 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571036704 -> 1410754745088
	1401321523056 [label="ul_modules.5.out_proj.conv_1_sc.weight_v
 (256, 256, 1, 1)" fillcolor=lightblue]
	1401321523056 -> 1401571036704
	1401571036704 [label=AccumulateGrad]
	1401571036224 -> 1410754745088
	1401321522384 [label="ul_modules.5.out_proj.conv_1_sc.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321522384 -> 1401571036224
	1401571036224 [label=AccumulateGrad]
	1401571035840 -> 1410754744800
	1401321522864 [label="ul_modules.5.out_proj.conv_1_sc.bias
 (256)" fillcolor=lightblue]
	1401321522864 -> 1401571035840
	1401571035840 [label=AccumulateGrad]
	1410754744656 -> 1410754744704
	1410754744656 [label=NegBackward0]
	1410754744320 -> 1410754744656
	1410754743984 -> 1410754743888
	1410754743984 -> 1401321522768 [dir=none]
	1401321522768 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754743984 -> 1410956222512 [dir=none]
	1410956222512 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754743984 -> 1401321523440 [dir=none]
	1401321523440 [label="v
 (512, 512, 1, 1)" fillcolor=orange]
	1410754743984 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571034928 -> 1410754743984
	1401321523440 [label="ul_modules.5.out_proj.conv_2.weight_v
 (512, 512, 1, 1)" fillcolor=lightblue]
	1401321523440 -> 1401571034928
	1401571034928 [label=AccumulateGrad]
	1401571035360 -> 1410754743984
	1401321522768 [label="ul_modules.5.out_proj.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321522768 -> 1401571035360
	1401571035360 [label=AccumulateGrad]
	1401571033872 -> 1410754743888
	1401321523248 [label="ul_modules.5.out_proj.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321523248 -> 1401571033872
	1401571033872 [label=AccumulateGrad]
	1410754743696 -> 1410753895616
	1410754743696 -> 1410956223664 [dir=none]
	1410956223664 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754743696 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753895520 -> 1410754743696
	1410753891248 -> 1410753891344
	1410753891248 -> 1410753987024 [dir=none]
	1410753987024 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753891248 -> 1410753982064 [dir=none]
	1410753982064 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753891248 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753895568 -> 1410753891248
	1410753895568 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754744560 -> 1410753895568
	1410754744560 -> 1410753985040 [dir=none]
	1410753985040 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754744560 -> 1410753979280 [dir=none]
	1410753979280 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754744560 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754745280 -> 1410754744560
	1410754745280 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754745328 -> 1410754745280
	1410754745328 -> 1410956224720 [dir=none]
	1410956224720 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754745328 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754745616 -> 1410754745328
	1410754745616 -> 1410753985328 [dir=none]
	1410753985328 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754745616 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754745472 -> 1410754745616
	1410754745472 [label="CatBackward0
------------
dim: 1"]
	1410754746240 -> 1410754745472
	1410754746240 -> 1410753985616 [dir=none]
	1410753985616 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754746240 -> 1410753985808 [dir=none]
	1410753985808 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754746240 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754747824 -> 1410754746240
	1410754747824 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754746576 -> 1410754747824
	1410754746576 -> 1410753985904 [dir=none]
	1410753985904 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754746576 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754748496 -> 1410754746576
	1410754748496 [label="CatBackward0
------------
dim: 1"]
	1410753891200 -> 1410754748496
	1410754748256 -> 1410754748496
	1410754748256 [label=NegBackward0]
	1410753891200 -> 1410754748256
	1410754746864 -> 1410754746240
	1410754746864 -> 1401321523152 [dir=none]
	1401321523152 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754746864 -> 1410956226640 [dir=none]
	1410956226640 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754746864 -> 1401321524016 [dir=none]
	1401321524016 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754746864 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571041216 -> 1410754746864
	1401321524016 [label="ul_modules.6.input_gated_resnet.0.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321524016 -> 1401571041216
	1401571041216 [label=AccumulateGrad]
	1401571040880 -> 1410754746864
	1401321523152 [label="ul_modules.6.input_gated_resnet.0.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321523152 -> 1401571040880
	1401571040880 [label=AccumulateGrad]
	1401571039440 -> 1410754746240
	1401321523824 [label="ul_modules.6.input_gated_resnet.0.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321523824 -> 1401571039440
	1401571039440 [label=AccumulateGrad]
	1410754746048 -> 1410754745472
	1410754746048 [label=NegBackward0]
	1410754746240 -> 1410754746048
	1410754744464 -> 1410754744560
	1410754744464 -> 1401321523728 [dir=none]
	1401321523728 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754744464 -> 1410956228272 [dir=none]
	1410956228272 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754744464 -> 1401321524400 [dir=none]
	1401321524400 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754744464 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571037856 -> 1410754744464
	1401321524400 [label="ul_modules.6.input_gated_resnet.0.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321524400 -> 1401571037856
	1401571037856 [label=AccumulateGrad]
	1401571037232 -> 1410754744464
	1401321523728 [label="ul_modules.6.input_gated_resnet.0.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321523728 -> 1401571037232
	1401571037232 [label=AccumulateGrad]
	1401571034832 -> 1410754744560
	1401321524208 [label="ul_modules.6.input_gated_resnet.0.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321524208 -> 1401571034832
	1401571034832 [label=AccumulateGrad]
	1410754744368 -> 1410753891248
	1410754744368 -> 1410956229424 [dir=none]
	1410956229424 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754744368 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753895568 -> 1410754744368
	1410753891392 -> 1410753891488
	1410753891392 -> 1410753988176 [dir=none]
	1410753988176 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753891392 -> 1410753987984 [dir=none]
	1410753987984 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753891392 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753891296 -> 1410753891392
	1410753891296 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754745712 -> 1410753891296
	1410754745712 -> 1410753987792 [dir=none]
	1410753987792 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754745712 -> 1410753982928 [dir=none]
	1410753982928 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754745712 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754744896 -> 1410754745712
	1410754744896 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754745904 -> 1410754744896
	1410754745904 -> 1410956230480 [dir=none]
	1410956230480 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754745904 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754748928 -> 1410754745904
	1410754748928 -> 1410753987696 [dir=none]
	1410753987696 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754748928 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754747248 -> 1410754748928
	1410754747248 [label="CatBackward0
------------
dim: 1"]
	1410754749408 -> 1410754747248
	1410754749408 -> 1410753987504 [dir=none]
	1410753987504 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754749408 -> 1410753987216 [dir=none]
	1410753987216 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754749408 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754749552 -> 1410754749408
	1410754749552 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754749600 -> 1410754749552
	1410754749600 -> 1410753985424 [dir=none]
	1410753985424 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754749600 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754749696 -> 1410754749600
	1410754749696 [label="CatBackward0
------------
dim: 1"]
	1410753891344 -> 1410754749696
	1410754749792 -> 1410754749696
	1410754749792 [label=NegBackward0]
	1410753891344 -> 1410754749792
	1410754749120 -> 1410754749408
	1410754749120 -> 1401321524112 [dir=none]
	1401321524112 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754749120 -> 1410956232464 [dir=none]
	1410956232464 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754749120 -> 1401321524976 [dir=none]
	1401321524976 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754749120 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571042176 -> 1410754749120
	1401321524976 [label="ul_modules.6.input_gated_resnet.1.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321524976 -> 1401571042176
	1401571042176 [label=AccumulateGrad]
	1401571042080 -> 1410754749120
	1401321524112 [label="ul_modules.6.input_gated_resnet.1.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321524112 -> 1401571042080
	1401571042080 [label=AccumulateGrad]
	1401571041744 -> 1410754749408
	1401321524784 [label="ul_modules.6.input_gated_resnet.1.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321524784 -> 1401571041744
	1401571041744 [label=AccumulateGrad]
	1410754748784 -> 1410754747248
	1410754748784 [label=NegBackward0]
	1410754749408 -> 1410754748784
	1410754745520 -> 1410754745712
	1410754745520 -> 1401321524688 [dir=none]
	1401321524688 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754745520 -> 1410956234096 [dir=none]
	1410956234096 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754745520 -> 1401321525360 [dir=none]
	1401321525360 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754745520 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571041552 -> 1410754745520
	1401321525360 [label="ul_modules.6.input_gated_resnet.1.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321525360 -> 1401571041552
	1401571041552 [label=AccumulateGrad]
	1401571041456 -> 1410754745520
	1401321524688 [label="ul_modules.6.input_gated_resnet.1.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321524688 -> 1401571041456
	1401571041456 [label=AccumulateGrad]
	1401571036608 -> 1410754745712
	1401321525168 [label="ul_modules.6.input_gated_resnet.1.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321525168 -> 1401571036608
	1401571036608 [label=AccumulateGrad]
	1410754744752 -> 1410753891392
	1410754744752 -> 1410956235248 [dir=none]
	1410956235248 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754744752 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753891296 -> 1410754744752
	1410753891536 -> 1410753892112
	1410753891536 -> 1410753989040 [dir=none]
	1410753989040 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753891536 -> 1410753988848 [dir=none]
	1410753988848 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753891536 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753891440 -> 1410753891536
	1410753891440 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754748832 -> 1410753891440
	1410754748832 -> 1410753988656 [dir=none]
	1410753988656 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754748832 -> 1410753987408 [dir=none]
	1410753987408 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754748832 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754747776 -> 1410754748832
	1410754747776 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754749312 -> 1410754747776
	1410754749312 -> 1410956236304 [dir=none]
	1410956236304 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754749312 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754749888 -> 1410754749312
	1410754749888 -> 1410753988560 [dir=none]
	1410753988560 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754749888 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754749504 -> 1410754749888
	1410754749504 [label="CatBackward0
------------
dim: 1"]
	1410754749984 -> 1410754749504
	1410754749984 -> 1410753988272 [dir=none]
	1410753988272 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754749984 -> 1410753987600 [dir=none]
	1410753987600 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754749984 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754750128 -> 1410754749984
	1410754750128 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754750272 -> 1410754750128
	1410754750272 -> 1410753988080 [dir=none]
	1410753988080 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754750272 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754750368 -> 1410754750272
	1410754750368 [label="CatBackward0
------------
dim: 1"]
	1410753891488 -> 1410754750368
	1410754750464 -> 1410754750368
	1410754750464 [label=NegBackward0]
	1410753891488 -> 1410754750464
	1410754750080 -> 1410754749984
	1410754750080 -> 1401321525072 [dir=none]
	1401321525072 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754750080 -> 1410956238224 [dir=none]
	1410956238224 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754750080 -> 1401321525936 [dir=none]
	1401321525936 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754750080 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571043136 -> 1410754750080
	1401321525936 [label="ul_modules.6.input_gated_resnet.2.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321525936 -> 1401571043136
	1401571043136 [label=AccumulateGrad]
	1401571043040 -> 1410754750080
	1401321525072 [label="ul_modules.6.input_gated_resnet.2.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321525072 -> 1401571043040
	1401571043040 [label=AccumulateGrad]
	1401571042704 -> 1410754749984
	1401321525744 [label="ul_modules.6.input_gated_resnet.2.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321525744 -> 1401571042704
	1401571042704 [label=AccumulateGrad]
	1410754749936 -> 1410754749504
	1410754749936 [label=NegBackward0]
	1410754749984 -> 1410754749936
	1410754748592 -> 1410754748832
	1410754748592 -> 1401321525648 [dir=none]
	1401321525648 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754748592 -> 1410956239856 [dir=none]
	1410956239856 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754748592 -> 1401321526320 [dir=none]
	1401321526320 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754748592 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571042512 -> 1410754748592
	1401321526320 [label="ul_modules.6.input_gated_resnet.2.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321526320 -> 1401571042512
	1401571042512 [label=AccumulateGrad]
	1401571042416 -> 1410754748592
	1401321525648 [label="ul_modules.6.input_gated_resnet.2.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321525648 -> 1401571042416
	1401571042416 [label=AccumulateGrad]
	1401571039728 -> 1410754748832
	1401321526128 [label="ul_modules.6.input_gated_resnet.2.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321526128 -> 1401571039728
	1401571039728 [label=AccumulateGrad]
	1410754745952 -> 1410753891536
	1410754745952 -> 1410956241008 [dir=none]
	1410956241008 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754745952 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753891440 -> 1410754745952
	1410753892160 -> 1410753892016
	1410753892160 -> 1410753989904 [dir=none]
	1410753989904 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753892160 -> 1410753989712 [dir=none]
	1410753989712 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753892160 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753892064 -> 1410753892160
	1410753892064 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754749744 -> 1410753892064
	1410754749744 -> 1410753989520 [dir=none]
	1410753989520 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754749744 -> 1410753988368 [dir=none]
	1410753988368 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754749744 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754749648 -> 1410754749744
	1410754749648 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754750032 -> 1410754749648
	1410754750032 -> 1410956242064 [dir=none]
	1410956242064 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754750032 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754750560 -> 1410754750032
	1410754750560 -> 1410753989424 [dir=none]
	1410753989424 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754750560 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754750176 -> 1410754750560
	1410754750176 [label="CatBackward0
------------
dim: 1"]
	1410754750656 -> 1410754750176
	1410754750656 -> 1410753989136 [dir=none]
	1410753989136 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754750656 -> 1410753988464 [dir=none]
	1410753988464 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754750656 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754750800 -> 1410754750656
	1410754750800 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754750944 -> 1410754750800
	1410754750944 -> 1410753988944 [dir=none]
	1410753988944 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754750944 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754751040 -> 1410754750944
	1410754751040 [label="CatBackward0
------------
dim: 1"]
	1410753892112 -> 1410754751040
	1410754751136 -> 1410754751040
	1410754751136 [label=NegBackward0]
	1410753892112 -> 1410754751136
	1410754750752 -> 1410754750656
	1410754750752 -> 1401321526032 [dir=none]
	1401321526032 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754750752 -> 1410956243984 [dir=none]
	1410956243984 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754750752 -> 1401321526896 [dir=none]
	1401321526896 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754750752 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571044096 -> 1410754750752
	1401321526896 [label="ul_modules.6.input_gated_resnet.3.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321526896 -> 1401571044096
	1401571044096 [label=AccumulateGrad]
	1401571044000 -> 1410754750752
	1401321526032 [label="ul_modules.6.input_gated_resnet.3.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321526032 -> 1401571044000
	1401571044000 [label=AccumulateGrad]
	1401571043664 -> 1410754750656
	1401321526704 [label="ul_modules.6.input_gated_resnet.3.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321526704 -> 1401571043664
	1401571043664 [label=AccumulateGrad]
	1410754750608 -> 1410754750176
	1410754750608 [label=NegBackward0]
	1410754750656 -> 1410754750608
	1410754749840 -> 1410754749744
	1410754749840 -> 1401321526608 [dir=none]
	1401321526608 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754749840 -> 1410956245616 [dir=none]
	1410956245616 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754749840 -> 1401321527280 [dir=none]
	1401321527280 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754749840 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571043472 -> 1410754749840
	1401321527280 [label="ul_modules.6.input_gated_resnet.3.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321527280 -> 1401571043472
	1401571043472 [label=AccumulateGrad]
	1401571043376 -> 1410754749840
	1401321526608 [label="ul_modules.6.input_gated_resnet.3.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321526608 -> 1401571043376
	1401571043376 [label=AccumulateGrad]
	1401571041888 -> 1410754749744
	1401321527088 [label="ul_modules.6.input_gated_resnet.3.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321527088 -> 1401571041888
	1401571041888 [label=AccumulateGrad]
	1410754749264 -> 1410753892160
	1410754749264 -> 1410956246768 [dir=none]
	1410956246768 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754749264 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753892064 -> 1410754749264
	1410753895136 -> 1410753894944
	1410753895136 -> 1410753990768 [dir=none]
	1410753990768 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753895136 -> 1410753990576 [dir=none]
	1410753990576 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753895136 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753891968 -> 1410753895136
	1410753891968 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754750416 -> 1410753891968
	1410754750416 -> 1410753990384 [dir=none]
	1410753990384 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754750416 -> 1410753989232 [dir=none]
	1410753989232 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754750416 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754750320 -> 1410754750416
	1410754750320 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754750704 -> 1410754750320
	1410754750704 -> 1410956280656 [dir=none]
	1410956280656 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754750704 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754751232 -> 1410754750704
	1410754751232 -> 1410753990288 [dir=none]
	1410753990288 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754751232 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754750848 -> 1410754751232
	1410754750848 [label="CatBackward0
------------
dim: 1"]
	1410754751328 -> 1410754750848
	1410754751328 -> 1410753990000 [dir=none]
	1410753990000 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754751328 -> 1410753989328 [dir=none]
	1410753989328 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754751328 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754751472 -> 1410754751328
	1410754751472 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754751616 -> 1410754751472
	1410754751616 -> 1410753989808 [dir=none]
	1410753989808 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754751616 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754751712 -> 1410754751616
	1410754751712 [label="CatBackward0
------------
dim: 1"]
	1410753892016 -> 1410754751712
	1410754751808 -> 1410754751712
	1410754751808 [label=NegBackward0]
	1410753892016 -> 1410754751808
	1410754751424 -> 1410754751328
	1410754751424 -> 1401321526992 [dir=none]
	1401321526992 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754751424 -> 1410956282576 [dir=none]
	1410956282576 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754751424 -> 1401321527856 [dir=none]
	1401321527856 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754751424 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571045056 -> 1410754751424
	1401321527856 [label="ul_modules.6.input_gated_resnet.4.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321527856 -> 1401571045056
	1401571045056 [label=AccumulateGrad]
	1401571044960 -> 1410754751424
	1401321526992 [label="ul_modules.6.input_gated_resnet.4.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321526992 -> 1401571044960
	1401571044960 [label=AccumulateGrad]
	1401571044624 -> 1410754751328
	1401321527664 [label="ul_modules.6.input_gated_resnet.4.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321527664 -> 1401571044624
	1401571044624 [label=AccumulateGrad]
	1410754751280 -> 1410754750848
	1410754751280 [label=NegBackward0]
	1410754751328 -> 1410754751280
	1410754750512 -> 1410754750416
	1410754750512 -> 1401321527568 [dir=none]
	1401321527568 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754750512 -> 1410956284208 [dir=none]
	1410956284208 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754750512 -> 1401321528240 [dir=none]
	1401321528240 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754750512 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571044432 -> 1410754750512
	1401321528240 [label="ul_modules.6.input_gated_resnet.4.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321528240 -> 1401571044432
	1401571044432 [label=AccumulateGrad]
	1401571044336 -> 1410754750512
	1401321527568 [label="ul_modules.6.input_gated_resnet.4.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321527568 -> 1401571044336
	1401571044336 [label=AccumulateGrad]
	1401571042848 -> 1410754750416
	1401321528048 [label="ul_modules.6.input_gated_resnet.4.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321528048 -> 1401571042848
	1401571042848 [label=AccumulateGrad]
	1410754750224 -> 1410753895136
	1410754750224 -> 1410956285360 [dir=none]
	1410956285360 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754750224 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753891968 -> 1410754750224
	1410753894896 -> 1410753894800
	1410753894896 -> 1410753994896 [dir=none]
	1410753994896 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753894896 -> 1410539648688 [dir=none]
	1410539648688 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753894896 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753895184 -> 1410753894896
	1410753895184 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754751088 -> 1410753895184
	1410754751088 -> 1410753994704 [dir=none]
	1410753994704 [label="input
 (1, 512, 28, 28)" fillcolor=orange]
	1410754751088 -> 1410753994224 [dir=none]
	1410753994224 [label="weight
 (512, 512, 1, 1)" fillcolor=orange]
	1410754751088 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754750992 -> 1410754751088
	1410754750992 -> 1410956286416 [dir=none]
	1410956286416 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754750992 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754751376 -> 1410754750992
	1410754751376 -> 1410753994608 [dir=none]
	1410753994608 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754751376 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754751904 -> 1410754751376
	1410754751904 [label="CatBackward0
------------
dim: 1"]
	1410754751520 -> 1410754751904
	1410754751520 [label="AddBackward0
------------
alpha: 1"]
	1410754752048 -> 1410754751520
	1410754752048 -> 1410753993744 [dir=none]
	1410753993744 [label="input
 (1, 512, 28, 28)" fillcolor=orange]
	1410754752048 -> 1410753994128 [dir=none]
	1410753994128 [label="weight
 (256, 512, 1, 1)" fillcolor=orange]
	1410754752048 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754752192 -> 1410754752048
	1410754752192 -> 1410753994032 [dir=none]
	1410753994032 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754752192 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754752336 -> 1410754752192
	1410754752336 [label="CatBackward0
------------
dim: 1"]
	1410753894944 -> 1410754752336
	1410754752432 -> 1410754752336
	1410754752432 [label=NegBackward0]
	1410753894944 -> 1410754752432
	1410754752144 -> 1410754752048
	1410754752144 -> 1401321530832 [dir=none]
	1401321530832 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754752144 -> 1410956288240 [dir=none]
	1410956288240 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754752144 -> 1401321531504 [dir=none]
	1401321531504 [label="v
 (256, 512, 1, 1)" fillcolor=orange]
	1410754752144 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571046112 -> 1410754752144
	1401321531504 [label="ul_modules.6.out_proj.conv_1.weight_v
 (256, 512, 1, 1)" fillcolor=lightblue]
	1401321531504 -> 1401571046112
	1401571046112 [label=AccumulateGrad]
	1401571045968 -> 1410754752144
	1401321530832 [label="ul_modules.6.out_proj.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321530832 -> 1401571045968
	1401571045968 [label=AccumulateGrad]
	1401571045632 -> 1410754752048
	1401321531312 [label="ul_modules.6.out_proj.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321531312 -> 1401571045632
	1401571045632 [label=AccumulateGrad]
	1410754752000 -> 1410754751520
	1410754752000 -> 1410753994320 [dir=none]
	1410753994320 [label="input
 (1, 256, 28, 28)" fillcolor=orange]
	1410754752000 -> 1410753994512 [dir=none]
	1410753994512 [label="weight
 (256, 256, 1, 1)" fillcolor=orange]
	1410754752000 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754752384 -> 1410754752000
	1410754752384 -> 1410753994416 [dir=none]
	1410753994416 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410754752384 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754752240 -> 1410754752384
	1410754752240 [label="CatBackward0
------------
dim: 1"]
	1410754752624 -> 1410754752240
	1410754752624 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (1, 1, 128, 784)"]
	1410754752768 -> 1410754752624
	1410754752768 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410754752864 -> 1410754752768
	1410754752864 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (1, 784, 128)"]
	1410754752960 -> 1410754752864
	1410754752960 -> 1410956290256 [dir=none]
	1410956290256 [label="mat2
 (1, 784, 128)" fillcolor=orange]
	1410754752960 -> 1410956290544 [dir=none]
	1410956290544 [label="self
 (1, 784, 784)" fillcolor=orange]
	1410754752960 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	1410754753056 -> 1410754752960
	1410754753056 [label="ViewBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 784)"]
	1410754753200 -> 1410754753056
	1410754753200 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 784)"]
	1410754753296 -> 1410754753200
	1410754753296 -> 1410956290832 [dir=none]
	1410956290832 [label="result
 (1, 1, 784, 784)" fillcolor=orange]
	1410754753296 [label="SoftmaxBackward0
----------------------
dim   :     4294967295
result: [saved tensor]"]
	1410754753392 -> 1410754753296
	1410754753392 -> 1410753993264 [dir=none]
	1410753993264 [label="mask
 (1, 1, 784, 784)" fillcolor=orange]
	1410754753392 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	1410754753488 -> 1410754753392
	1410754753488 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (1, 784, 784)"]
	1410754753584 -> 1410754753488
	1410754753584 -> 1410956291504 [dir=none]
	1410956291504 [label="mat2
 (1, 16, 784)" fillcolor=orange]
	1410754753584 -> 1410956291792 [dir=none]
	1410956291792 [label="self
 (1, 784, 16)" fillcolor=orange]
	1410754753584 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	1410754753680 -> 1410754753584
	1410754753680 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (1, 1, 784, 16)"]
	1410754753824 -> 1410754753680
	1410754753824 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 784, 16)"]
	1410754753920 -> 1410754753824
	1410754753920 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410754754016 -> 1410754753920
	1410754754016 -> 1410956292176 [dir=none]
	1410956292176 [label="other
 ()" fillcolor=orange]
	1410754754016 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	1410754754112 -> 1410754754016
	1410754754112 [label="ViewBackward0
----------------------------------
self_sym_sizes: (1, 1, 16, 28, 28)"]
	1410754754208 -> 1410754754112
	1410754754208 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 16, 28, 28)"]
	1410754754304 -> 1410754754208
	1410754754304 -> 1410753993168 [dir=none]
	1410753993168 [label="input
 (1, 258, 28, 28)" fillcolor=orange]
	1410754754304 -> 1410753992880 [dir=none]
	1410753992880 [label="weight
 (16, 258, 1, 1)" fillcolor=orange]
	1410754754304 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754754400 -> 1410754754304
	1410754754400 [label="AddBackward0
------------
alpha: 1"]
	1410754754544 -> 1410754754400
	1410754754544 [label="CatBackward0
------------
dim: 1"]
	1410753894944 -> 1410754754544
	1410754754496 -> 1410754754400
	1410754754496 -> 1410753992976 [dir=none]
	1410753992976 [label="other
 (1, 258, 28, 28)" fillcolor=orange]
	1410754754496 -> 1410753992784 [dir=none]
	1410753992784 [label="self
 (1, 258, 28, 28)" fillcolor=orange]
	1410754754496 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410754754592 -> 1410754754496
	1410754754592 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 516, 28, 28)
split_size    :              258"]
	1410754754832 -> 1410754754592
	1410754754832 -> 1410753992400 [dir=none]
	1410753992400 [label="input
 (1, 516, 28, 28)" fillcolor=orange]
	1410754754832 -> 1410753992496 [dir=none]
	1410753992496 [label="weight
 (516, 516, 1, 1)" fillcolor=orange]
	1410754754832 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (516,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754754928 -> 1410754754832
	1410754754928 -> 1410956294000 [dir=none]
	1410956294000 [label="result1
 (1, 516, 28, 28)" fillcolor=orange]
	1410754754928 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754755072 -> 1410754754928
	1410754755072 -> 1410753992592 [dir=none]
	1410753992592 [label="self
 (1, 516, 28, 28)" fillcolor=orange]
	1410754755072 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754755168 -> 1410754755072
	1410754755168 [label="CatBackward0
------------
dim: 1"]
	1410754755264 -> 1410754755168
	1410754755264 -> 1410753992112 [dir=none]
	1410753992112 [label="input
 (1, 516, 28, 28)" fillcolor=orange]
	1410754755264 -> 1410753992304 [dir=none]
	1410753992304 [label="weight
 (258, 516, 1, 1)" fillcolor=orange]
	1410754755264 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (258,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754755408 -> 1410754755264
	1410754755408 -> 1410753992208 [dir=none]
	1410753992208 [label="self
 (1, 516, 28, 28)" fillcolor=orange]
	1410754755408 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754755552 -> 1410754755408
	1410754755552 [label="CatBackward0
------------
dim: 1"]
	1410754754544 -> 1410754755552
	1410754755648 -> 1410754755552
	1410754755648 [label=NegBackward0]
	1410754754544 -> 1410754755648
	1410754755360 -> 1410754755264
	1410754755360 -> 1401321529488 [dir=none]
	1401321529488 [label="g
 (258, 1, 1, 1)" fillcolor=orange]
	1410754755360 -> 1410956295728 [dir=none]
	1410956295728 [label="result1
 (258, 1, 1, 1)" fillcolor=orange]
	1410754755360 -> 1401321530160 [dir=none]
	1401321530160 [label="v
 (258, 516, 1, 1)" fillcolor=orange]
	1410754755360 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571049376 -> 1410754755360
	1401321530160 [label="ul_modules.6.in_proj_q.0.conv_1.weight_v
 (258, 516, 1, 1)" fillcolor=lightblue]
	1401321530160 -> 1401571049376
	1401571049376 [label=AccumulateGrad]
	1401571213520 -> 1410754755360
	1401321529488 [label="ul_modules.6.in_proj_q.0.conv_1.weight_g
 (258, 1, 1, 1)" fillcolor=lightblue]
	1401321529488 -> 1401571213520
	1401571213520 [label=AccumulateGrad]
	1401571049136 -> 1410754755264
	1401321529968 [label="ul_modules.6.in_proj_q.0.conv_1.bias
 (258)" fillcolor=lightblue]
	1401321529968 -> 1401571049136
	1401571049136 [label=AccumulateGrad]
	1410754755216 -> 1410754755168
	1410754755216 [label=NegBackward0]
	1410754755264 -> 1410754755216
	1410754754880 -> 1410754754832
	1410754754880 -> 1401321529872 [dir=none]
	1401321529872 [label="g
 (516, 1, 1, 1)" fillcolor=orange]
	1410754754880 -> 1410956297424 [dir=none]
	1410956297424 [label="result1
 (516, 1, 1, 1)" fillcolor=orange]
	1410754754880 -> 1401321530544 [dir=none]
	1401321530544 [label="v
 (516, 516, 1, 1)" fillcolor=orange]
	1410754754880 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571048800 -> 1410754754880
	1401321530544 [label="ul_modules.6.in_proj_q.0.conv_2.weight_v
 (516, 516, 1, 1)" fillcolor=lightblue]
	1401321530544 -> 1401571048800
	1401571048800 [label=AccumulateGrad]
	1401571048944 -> 1410754754880
	1401321529872 [label="ul_modules.6.in_proj_q.0.conv_2.weight_g
 (516, 1, 1, 1)" fillcolor=lightblue]
	1401321529872 -> 1401571048944
	1401571048944 [label=AccumulateGrad]
	1401571048512 -> 1410754754832
	1401321530352 [label="ul_modules.6.in_proj_q.0.conv_2.bias
 (516)" fillcolor=lightblue]
	1401321530352 -> 1401571048512
	1401571048512 [label=AccumulateGrad]
	1410754754640 -> 1410754754496
	1410754754640 -> 1410956298576 [dir=none]
	1410956298576 [label="result
 (1, 258, 28, 28)" fillcolor=orange]
	1410754754640 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410754754592 -> 1410754754640
	1410754754352 -> 1410754754304
	1410754754352 -> 1401321530256 [dir=none]
	1401321530256 [label="g
 (16, 1, 1, 1)" fillcolor=orange]
	1410754754352 -> 1410956299152 [dir=none]
	1410956299152 [label="result1
 (16, 1, 1, 1)" fillcolor=orange]
	1410754754352 -> 1401321531120 [dir=none]
	1401321531120 [label="v
 (16, 258, 1, 1)" fillcolor=orange]
	1410754754352 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571049280 -> 1410754754352
	1401321531120 [label="ul_modules.6.in_proj_q.1.weight_v
 (16, 258, 1, 1)" fillcolor=lightblue]
	1401321531120 -> 1401571049280
	1401571049280 [label=AccumulateGrad]
	1401571048464 -> 1410754754352
	1401321530256 [label="ul_modules.6.in_proj_q.1.weight_g
 (16, 1, 1, 1)" fillcolor=lightblue]
	1401321530256 -> 1401571048464
	1401571048464 [label=AccumulateGrad]
	1401571047456 -> 1410754754304
	1401321530928 [label="ul_modules.6.in_proj_q.1.bias
 (16)" fillcolor=lightblue]
	1401321530928 -> 1401571047456
	1401571047456 [label=AccumulateGrad]
	1410754753632 -> 1410754753584
	1410754753632 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 1, 16, 784)"]
	1410754753968 -> 1410754753632
	1410754753968 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 16, 784)"]
	1410754754160 -> 1410754753968
	1410754754160 [label="ViewBackward0
----------------------------------
self_sym_sizes: (1, 1, 16, 28, 28)"]
	1410754753728 -> 1410754754160
	1410754753728 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 16, 28, 28)"]
	1410754754688 -> 1410754753728
	1410754754688 [label="SplitWithSizesBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 144, 28, 28)
split_sizes   :        (16, 128)"]
	1410754754736 -> 1410754754688
	1410754754736 -> 1410753991824 [dir=none]
	1410753991824 [label="input
 (1, 514, 28, 28)" fillcolor=orange]
	1410754754736 -> 1410753991536 [dir=none]
	1410753991536 [label="weight
 (144, 514, 1, 1)" fillcolor=orange]
	1410754754736 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (144,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754755120 -> 1410754754736
	1410754755120 [label="AddBackward0
------------
alpha: 1"]
	1410754755504 -> 1410754755120
	1410754755504 [label="CatBackward0
------------
dim: 1"]
	1410753891200 -> 1410754755504
	1410753894944 -> 1410754755504
	1410754755600 -> 1410754755120
	1410754755600 -> 1410753991632 [dir=none]
	1410753991632 [label="other
 (1, 514, 28, 28)" fillcolor=orange]
	1410754755600 -> 1410753991440 [dir=none]
	1410753991440 [label="self
 (1, 514, 28, 28)" fillcolor=orange]
	1410754755600 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410754755312 -> 1410754755600
	1410754755312 [label="SplitBackward0
---------------------------------
dim           :                 1
self_sym_sizes: (1, 1028, 28, 28)
split_size    :               514"]
	1410754755840 -> 1410754755312
	1410754755840 -> 1410753991056 [dir=none]
	1410753991056 [label="input
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754755840 -> 1410753991152 [dir=none]
	1410753991152 [label="weight
 (1028, 1028, 1, 1)" fillcolor=orange]
	1410754755840 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1028,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754755936 -> 1410754755840
	1410754755936 -> 1410956302128 [dir=none]
	1410956302128 [label="result1
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754755936 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754756080 -> 1410754755936
	1410754756080 -> 1410753991248 [dir=none]
	1410753991248 [label="self
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754756080 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754756176 -> 1410754756080
	1410754756176 [label="CatBackward0
------------
dim: 1"]
	1410754756272 -> 1410754756176
	1410754756272 -> 1410753990672 [dir=none]
	1410753990672 [label="input
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754756272 -> 1410753990864 [dir=none]
	1410753990864 [label="weight
 (514, 1028, 1, 1)" fillcolor=orange]
	1410754756272 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (514,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754756416 -> 1410754756272
	1410754756416 -> 1410753990192 [dir=none]
	1410753990192 [label="self
 (1, 1028, 28, 28)" fillcolor=orange]
	1410754756416 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754756560 -> 1410754756416
	1410754756560 [label="CatBackward0
------------
dim: 1"]
	1410754755504 -> 1410754756560
	1410754756464 -> 1410754756560
	1410754756464 [label=NegBackward0]
	1410754755504 -> 1410754756464
	1410754756368 -> 1410754756272
	1410754756368 -> 1401321527952 [dir=none]
	1401321527952 [label="g
 (514, 1, 1, 1)" fillcolor=orange]
	1410754756368 -> 1410956303856 [dir=none]
	1410956303856 [label="result1
 (514, 1, 1, 1)" fillcolor=orange]
	1410754756368 -> 1401321528816 [dir=none]
	1401321528816 [label="v
 (514, 1028, 1, 1)" fillcolor=orange]
	1410754756368 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571214960 -> 1410754756368
	1401321528816 [label="ul_modules.6.in_proj_kv.0.conv_1.weight_v
 (514, 1028, 1, 1)" fillcolor=lightblue]
	1401321528816 -> 1401571214960
	1401571214960 [label=AccumulateGrad]
	1401571214816 -> 1410754756368
	1401321527952 [label="ul_modules.6.in_proj_kv.0.conv_1.weight_g
 (514, 1, 1, 1)" fillcolor=lightblue]
	1401321527952 -> 1401571214816
	1401571214816 [label=AccumulateGrad]
	1401571214480 -> 1410754756272
	1401321528624 [label="ul_modules.6.in_proj_kv.0.conv_1.bias
 (514)" fillcolor=lightblue]
	1401321528624 -> 1401571214480
	1401571214480 [label=AccumulateGrad]
	1410754756224 -> 1410754756176
	1410754756224 [label=NegBackward0]
	1410754756272 -> 1410754756224
	1410754755888 -> 1410754755840
	1410754755888 -> 1401321528528 [dir=none]
	1401321528528 [label="g
 (1028, 1, 1, 1)" fillcolor=orange]
	1410754755888 -> 1410956305488 [dir=none]
	1410956305488 [label="result1
 (1028, 1, 1, 1)" fillcolor=orange]
	1410754755888 -> 1401321529200 [dir=none]
	1401321529200 [label="v
 (1028, 1028, 1, 1)" fillcolor=orange]
	1410754755888 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571214144 -> 1410754755888
	1401321529200 [label="ul_modules.6.in_proj_kv.0.conv_2.weight_v
 (1028, 1028, 1, 1)" fillcolor=lightblue]
	1401321529200 -> 1401571214144
	1401571214144 [label=AccumulateGrad]
	1401571214288 -> 1410754755888
	1401321528528 [label="ul_modules.6.in_proj_kv.0.conv_2.weight_g
 (1028, 1, 1, 1)" fillcolor=lightblue]
	1401321528528 -> 1401571214288
	1401571214288 [label=AccumulateGrad]
	1401571213856 -> 1410754755840
	1401321529008 [label="ul_modules.6.in_proj_kv.0.conv_2.bias
 (1028)" fillcolor=lightblue]
	1401321529008 -> 1401571213856
	1401571213856 [label=AccumulateGrad]
	1410754755744 -> 1410754755600
	1410754755744 -> 1410956306640 [dir=none]
	1410956306640 [label="result
 (1, 514, 28, 28)" fillcolor=orange]
	1410754755744 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410754755312 -> 1410754755744
	1410754754784 -> 1410754754736
	1410754754784 -> 1401321528912 [dir=none]
	1401321528912 [label="g
 (144, 1, 1, 1)" fillcolor=orange]
	1410754754784 -> 1410956307216 [dir=none]
	1410956307216 [label="result1
 (144, 1, 1, 1)" fillcolor=orange]
	1410754754784 -> 1401321529776 [dir=none]
	1401321529776 [label="v
 (144, 514, 1, 1)" fillcolor=orange]
	1410754754784 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571214672 -> 1410754754784
	1401321529776 [label="ul_modules.6.in_proj_kv.1.weight_v
 (144, 514, 1, 1)" fillcolor=lightblue]
	1401321529776 -> 1401571214672
	1401571214672 [label=AccumulateGrad]
	1401571213808 -> 1410754754784
	1401321528912 [label="ul_modules.6.in_proj_kv.1.weight_g
 (144, 1, 1, 1)" fillcolor=lightblue]
	1401321528912 -> 1401571213808
	1401571213808 [label=AccumulateGrad]
	1401571213472 -> 1410754754736
	1401321529584 [label="ul_modules.6.in_proj_kv.1.bias
 (144)" fillcolor=lightblue]
	1401321529584 -> 1401571213472
	1401571213472 [label=AccumulateGrad]
	1410754753008 -> 1410754752960
	1410754753008 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 128)"]
	1410754753344 -> 1410754753008
	1410754753344 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 128)"]
	1410754753536 -> 1410754753344
	1410754753536 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410754753872 -> 1410754753536
	1410754753872 [label="ViewBackward0
-----------------------------------
self_sym_sizes: (1, 1, 128, 28, 28)"]
	1410754754256 -> 1410754753872
	1410754754256 [label="ViewBackward0
--------------------------------
self_sym_sizes: (1, 128, 28, 28)"]
	1410754754688 -> 1410754754256
	1410754752576 -> 1410754752240
	1410754752576 [label=NegBackward0]
	1410754752624 -> 1410754752576
	1410754752288 -> 1410754752000
	1410754752288 -> 1401321531216 [dir=none]
	1401321531216 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754752288 -> 1410956309424 [dir=none]
	1410956309424 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754752288 -> 1401321531888 [dir=none]
	1401321531888 [label="v
 (256, 256, 1, 1)" fillcolor=orange]
	1410754752288 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571046640 -> 1410754752288
	1401321531888 [label="ul_modules.6.out_proj.conv_1_sc.weight_v
 (256, 256, 1, 1)" fillcolor=lightblue]
	1401321531888 -> 1401571046640
	1401571046640 [label=AccumulateGrad]
	1401571046160 -> 1410754752288
	1401321531216 [label="ul_modules.6.out_proj.conv_1_sc.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321531216 -> 1401571046160
	1401571046160 [label=AccumulateGrad]
	1401571045776 -> 1410754752000
	1401321531696 [label="ul_modules.6.out_proj.conv_1_sc.bias
 (256)" fillcolor=lightblue]
	1401321531696 -> 1401571045776
	1401571045776 [label=AccumulateGrad]
	1410754751856 -> 1410754751904
	1410754751856 [label=NegBackward0]
	1410754751520 -> 1410754751856
	1410754751184 -> 1410754751088
	1410754751184 -> 1401321531600 [dir=none]
	1401321531600 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754751184 -> 1410956311056 [dir=none]
	1410956311056 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754751184 -> 1401321532272 [dir=none]
	1401321532272 [label="v
 (512, 512, 1, 1)" fillcolor=orange]
	1410754751184 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571044864 -> 1410754751184
	1401321532272 [label="ul_modules.6.out_proj.conv_2.weight_v
 (512, 512, 1, 1)" fillcolor=lightblue]
	1401321532272 -> 1401571044864
	1401571044864 [label=AccumulateGrad]
	1401571045296 -> 1410754751184
	1401321531600 [label="ul_modules.6.out_proj.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321531600 -> 1401571045296
	1401571045296 [label=AccumulateGrad]
	1401571043808 -> 1410754751088
	1401321532080 [label="ul_modules.6.out_proj.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321532080 -> 1401571043808
	1401571043808 [label=AccumulateGrad]
	1410754750896 -> 1410753894896
	1410754750896 -> 1410956312208 [dir=none]
	1410956312208 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754750896 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753895184 -> 1410754750896
	1410753894992 -> 1410753894704
	1410753894992 -> 1410753994800 [dir=none]
	1410753994800 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753894992 -> 1410753990096 [dir=none]
	1410753990096 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753894992 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753894848 -> 1410753894992
	1410753894848 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754751760 -> 1410753894848
	1410754751760 -> 1410753993072 [dir=none]
	1410753993072 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754751760 -> 1410753987312 [dir=none]
	1410753987312 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754751760 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754752480 -> 1410754751760
	1410754752480 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754752528 -> 1410754752480
	1410754752528 -> 1410956329712 [dir=none]
	1410956329712 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754752528 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754752816 -> 1410754752528
	1410754752816 -> 1410753993360 [dir=none]
	1410753993360 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754752816 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754752672 -> 1410754752816
	1410754752672 [label="CatBackward0
------------
dim: 1"]
	1410754753440 -> 1410754752672
	1410754753440 -> 1410753993648 [dir=none]
	1410753993648 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754753440 -> 1410753993840 [dir=none]
	1410753993840 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754753440 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754755024 -> 1410754753440
	1410754755024 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754753776 -> 1410754755024
	1410754753776 -> 1410753993936 [dir=none]
	1410753993936 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754753776 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754755696 -> 1410754753776
	1410754755696 [label="CatBackward0
------------
dim: 1"]
	1410753894800 -> 1410754755696
	1410754755456 -> 1410754755696
	1410754755456 [label=NegBackward0]
	1410753894800 -> 1410754755456
	1410754754064 -> 1410754753440
	1410754754064 -> 1401321531984 [dir=none]
	1401321531984 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410754754064 -> 1410956331632 [dir=none]
	1410956331632 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410754754064 -> 1401321532848 [dir=none]
	1401321532848 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410754754064 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571215056 -> 1410754754064
	1401321532848 [label="ul_modules.7.input_gated_resnet.0.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321532848 -> 1401571215056
	1401571215056 [label=AccumulateGrad]
	1401571214720 -> 1410754754064
	1401321531984 [label="ul_modules.7.input_gated_resnet.0.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321531984 -> 1401571214720
	1401571214720 [label=AccumulateGrad]
	1401571213424 -> 1410754753440
	1401321532656 [label="ul_modules.7.input_gated_resnet.0.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321532656 -> 1401571213424
	1401571213424 [label=AccumulateGrad]
	1410754753248 -> 1410754752672
	1410754753248 [label=NegBackward0]
	1410754753440 -> 1410754753248
	1410754751664 -> 1410754751760
	1410754751664 -> 1401321532560 [dir=none]
	1401321532560 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754751664 -> 1410956333264 [dir=none]
	1410956333264 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754751664 -> 1401321533232 [dir=none]
	1401321533232 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754751664 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571047792 -> 1410754751664
	1401321533232 [label="ul_modules.7.input_gated_resnet.0.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321533232 -> 1401571047792
	1401571047792 [label=AccumulateGrad]
	1401571047168 -> 1410754751664
	1401321532560 [label="ul_modules.7.input_gated_resnet.0.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321532560 -> 1401571047168
	1401571047168 [label=AccumulateGrad]
	1401571044768 -> 1410754751760
	1401321533040 [label="ul_modules.7.input_gated_resnet.0.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321533040 -> 1401571044768
	1401571044768 [label=AccumulateGrad]
	1410754751568 -> 1410753894992
	1410754751568 -> 1410956334416 [dir=none]
	1410956334416 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754751568 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753894848 -> 1410754751568
	1410753894176 -> 1410753894128
	1410753894176 -> 1410753995952 [dir=none]
	1410753995952 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753894176 -> 1410753995760 [dir=none]
	1410753995760 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753894176 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753894752 -> 1410753894176
	1410753894752 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754752912 -> 1410753894752
	1410754752912 -> 1410753995568 [dir=none]
	1410753995568 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754752912 -> 1410753990960 [dir=none]
	1410753990960 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754752912 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754752096 -> 1410754752912
	1410754752096 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410754753104 -> 1410754752096
	1410754753104 -> 1410956335472 [dir=none]
	1410956335472 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410754753104 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410754756128 -> 1410754753104
	1410754756128 -> 1410753995472 [dir=none]
	1410753995472 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410754756128 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410754754448 -> 1410754756128
	1410754754448 [label="CatBackward0
------------
dim: 1"]
	1410754756512 -> 1410754754448
	1410754756512 -> 1410753995280 [dir=none]
	1410753995280 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754756512 -> 1410753994992 [dir=none]
	1410753994992 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410754756512 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754756320 -> 1410754756512
	1410754756320 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956361984 -> 1410754756320
	1410956361984 -> 1410753993456 [dir=none]
	1410753993456 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956361984 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956362080 -> 1410956361984
	1410956362080 [label="CatBackward0
------------
dim: 1"]
	1410753894704 -> 1410956362080
	1410956362176 -> 1410956362080
	1410956362176 [label=NegBackward0]
	1410753894704 -> 1410956362176
	1410956361840 -> 1410754756512
	1410956361840 -> 1401321532944 [dir=none]
	1401321532944 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410956361840 -> 1410956337392 [dir=none]
	1410956337392 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410956361840 -> 1401321533808 [dir=none]
	1401321533808 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410956361840 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571216016 -> 1410956361840
	1401321533808 [label="ul_modules.7.input_gated_resnet.1.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321533808 -> 1401571216016
	1401571216016 [label=AccumulateGrad]
	1401571215920 -> 1410956361840
	1401321532944 [label="ul_modules.7.input_gated_resnet.1.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321532944 -> 1401571215920
	1401571215920 [label=AccumulateGrad]
	1401571215584 -> 1410754756512
	1401321533616 [label="ul_modules.7.input_gated_resnet.1.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321533616 -> 1401571215584
	1401571215584 [label=AccumulateGrad]
	1410754755984 -> 1410754754448
	1410754755984 [label=NegBackward0]
	1410754756512 -> 1410754755984
	1410754752720 -> 1410754752912
	1410754752720 -> 1401321533520 [dir=none]
	1401321533520 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754752720 -> 1410956339024 [dir=none]
	1410956339024 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754752720 -> 1401321534192 [dir=none]
	1401321534192 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754752720 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571215392 -> 1410754752720
	1401321534192 [label="ul_modules.7.input_gated_resnet.1.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321534192 -> 1401571215392
	1401571215392 [label=AccumulateGrad]
	1401571215296 -> 1410754752720
	1401321533520 [label="ul_modules.7.input_gated_resnet.1.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321533520 -> 1401571215296
	1401571215296 [label=AccumulateGrad]
	1401571215200 -> 1410754752912
	1401321534000 [label="ul_modules.7.input_gated_resnet.1.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321534000 -> 1401571215200
	1401571215200 [label=AccumulateGrad]
	1410754751952 -> 1410753894176
	1410754751952 -> 1410956340176 [dir=none]
	1410956340176 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754751952 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753894752 -> 1410754751952
	1410753894080 -> 1410753893936
	1410753894080 -> 1410753996816 [dir=none]
	1410753996816 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753894080 -> 1410753996624 [dir=none]
	1410753996624 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753894080 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753894032 -> 1410753894080
	1410753894032 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754756032 -> 1410753894032
	1410754756032 -> 1410753996432 [dir=none]
	1410753996432 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754756032 -> 1410753995184 [dir=none]
	1410753995184 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754756032 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410754754976 -> 1410754756032
	1410754754976 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956361792 -> 1410754754976
	1410956361792 -> 1410956341232 [dir=none]
	1410956341232 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410956361792 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956362272 -> 1410956361792
	1410956362272 -> 1410753996336 [dir=none]
	1410753996336 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956362272 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956361888 -> 1410956362272
	1410956361888 [label="CatBackward0
------------
dim: 1"]
	1410956362368 -> 1410956361888
	1410956362368 -> 1410753996048 [dir=none]
	1410753996048 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956362368 -> 1410753995376 [dir=none]
	1410753995376 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410956362368 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956362512 -> 1410956362368
	1410956362512 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956362656 -> 1410956362512
	1410956362656 -> 1410753995856 [dir=none]
	1410753995856 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956362656 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956362752 -> 1410956362656
	1410956362752 [label="CatBackward0
------------
dim: 1"]
	1410753894128 -> 1410956362752
	1410956362848 -> 1410956362752
	1410956362848 [label=NegBackward0]
	1410753894128 -> 1410956362848
	1410956362464 -> 1410956362368
	1410956362464 -> 1401321533904 [dir=none]
	1401321533904 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410956362464 -> 1410956343152 [dir=none]
	1410956343152 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410956362464 -> 1401321534768 [dir=none]
	1401321534768 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410956362464 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571216976 -> 1410956362464
	1401321534768 [label="ul_modules.7.input_gated_resnet.2.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321534768 -> 1401571216976
	1401571216976 [label=AccumulateGrad]
	1401571216880 -> 1410956362464
	1401321533904 [label="ul_modules.7.input_gated_resnet.2.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321533904 -> 1401571216880
	1401571216880 [label=AccumulateGrad]
	1401571216544 -> 1410956362368
	1401321534576 [label="ul_modules.7.input_gated_resnet.2.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321534576 -> 1401571216544
	1401571216544 [label=AccumulateGrad]
	1410956362320 -> 1410956361888
	1410956362320 [label=NegBackward0]
	1410956362368 -> 1410956362320
	1410754755792 -> 1410754756032
	1410754755792 -> 1401321534480 [dir=none]
	1401321534480 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410754755792 -> 1410956344784 [dir=none]
	1410956344784 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410754755792 -> 1401321535152 [dir=none]
	1401321535152 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410754755792 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571216352 -> 1410754755792
	1401321535152 [label="ul_modules.7.input_gated_resnet.2.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321535152 -> 1401571216352
	1401571216352 [label=AccumulateGrad]
	1401571216256 -> 1410754755792
	1401321534480 [label="ul_modules.7.input_gated_resnet.2.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321534480 -> 1401571216256
	1401571216256 [label=AccumulateGrad]
	1401571216160 -> 1410754756032
	1401321534960 [label="ul_modules.7.input_gated_resnet.2.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321534960 -> 1401571216160
	1401571216160 [label=AccumulateGrad]
	1410754753152 -> 1410753894080
	1410754753152 -> 1410956395152 [dir=none]
	1410956395152 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754753152 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753894032 -> 1410754753152
	1410753893888 -> 1410753890912
	1410753893888 -> 1410753997680 [dir=none]
	1410753997680 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753893888 -> 1410753997488 [dir=none]
	1410753997488 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753893888 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753893984 -> 1410753893888
	1410753893984 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410754741632 -> 1410753893984
	1410754741632 -> 1410753997296 [dir=none]
	1410753997296 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410754741632 -> 1410753996144 [dir=none]
	1410753996144 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410754741632 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956362032 -> 1410754741632
	1410956362032 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956362416 -> 1410956362032
	1410956362416 -> 1410956396208 [dir=none]
	1410956396208 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410956362416 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956362944 -> 1410956362416
	1410956362944 -> 1410753997200 [dir=none]
	1410753997200 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956362944 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956362560 -> 1410956362944
	1410956362560 [label="CatBackward0
------------
dim: 1"]
	1410956363040 -> 1410956362560
	1410956363040 -> 1410753996912 [dir=none]
	1410753996912 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956363040 -> 1410753996240 [dir=none]
	1410753996240 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410956363040 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956363184 -> 1410956363040
	1410956363184 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956363328 -> 1410956363184
	1410956363328 -> 1410753996720 [dir=none]
	1410753996720 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956363328 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956363424 -> 1410956363328
	1410956363424 [label="CatBackward0
------------
dim: 1"]
	1410753893936 -> 1410956363424
	1410956363520 -> 1410956363424
	1410956363520 [label=NegBackward0]
	1410753893936 -> 1410956363520
	1410956363136 -> 1410956363040
	1410956363136 -> 1401321534864 [dir=none]
	1401321534864 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410956363136 -> 1410956398128 [dir=none]
	1410956398128 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410956363136 -> 1401321535728 [dir=none]
	1401321535728 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410956363136 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571217936 -> 1410956363136
	1401321535728 [label="ul_modules.7.input_gated_resnet.3.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321535728 -> 1401571217936
	1401571217936 [label=AccumulateGrad]
	1401571217840 -> 1410956363136
	1401321534864 [label="ul_modules.7.input_gated_resnet.3.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321534864 -> 1401571217840
	1401571217840 [label=AccumulateGrad]
	1401571217504 -> 1410956363040
	1401321535536 [label="ul_modules.7.input_gated_resnet.3.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321535536 -> 1401571217504
	1401571217504 [label=AccumulateGrad]
	1410956362992 -> 1410956362560
	1410956362992 [label=NegBackward0]
	1410956363040 -> 1410956362992
	1410956362224 -> 1410754741632
	1410956362224 -> 1401321535440 [dir=none]
	1401321535440 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410956362224 -> 1410956399760 [dir=none]
	1410956399760 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410956362224 -> 1401321536112 [dir=none]
	1401321536112 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410956362224 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571217312 -> 1410956362224
	1401321536112 [label="ul_modules.7.input_gated_resnet.3.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321536112 -> 1401571217312
	1401571217312 [label=AccumulateGrad]
	1401571217216 -> 1410956362224
	1401321535440 [label="ul_modules.7.input_gated_resnet.3.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321535440 -> 1401571217216
	1401571217216 [label=AccumulateGrad]
	1401571215728 -> 1410754741632
	1401321535920 [label="ul_modules.7.input_gated_resnet.3.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321535920 -> 1401571215728
	1401571215728 [label=AccumulateGrad]
	1410754740288 -> 1410753893888
	1410754740288 -> 1410956400912 [dir=none]
	1410956400912 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410754740288 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753893984 -> 1410754740288
	1410753890960 -> 1410753895088
	1410753890960 -> 1410753998544 [dir=none]
	1410753998544 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753890960 -> 1410753998352 [dir=none]
	1410753998352 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753890960 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410754742064 -> 1410753890960
	1410754742064 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410956362800 -> 1410754742064
	1410956362800 -> 1410753998160 [dir=none]
	1410753998160 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956362800 -> 1410753997008 [dir=none]
	1410753997008 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410956362800 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956362704 -> 1410956362800
	1410956362704 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956363088 -> 1410956362704
	1410956363088 -> 1410956401968 [dir=none]
	1410956401968 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410956363088 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956363616 -> 1410956363088
	1410956363616 -> 1410753998064 [dir=none]
	1410753998064 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956363616 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956363232 -> 1410956363616
	1410956363232 [label="CatBackward0
------------
dim: 1"]
	1410956363712 -> 1410956363232
	1410956363712 -> 1410753997776 [dir=none]
	1410753997776 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956363712 -> 1410753997104 [dir=none]
	1410753997104 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410956363712 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956363856 -> 1410956363712
	1410956363856 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956364000 -> 1410956363856
	1410956364000 -> 1410753997584 [dir=none]
	1410753997584 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956364000 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956364096 -> 1410956364000
	1410956364096 [label="CatBackward0
------------
dim: 1"]
	1410753890912 -> 1410956364096
	1410956364192 -> 1410956364096
	1410956364192 [label=NegBackward0]
	1410753890912 -> 1410956364192
	1410956363808 -> 1410956363712
	1410956363808 -> 1401321535824 [dir=none]
	1401321535824 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410956363808 -> 1410956403888 [dir=none]
	1410956403888 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410956363808 -> 1401321536688 [dir=none]
	1401321536688 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410956363808 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571218896 -> 1410956363808
	1401321536688 [label="ul_modules.7.input_gated_resnet.4.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321536688 -> 1401571218896
	1401571218896 [label=AccumulateGrad]
	1401571218800 -> 1410956363808
	1401321535824 [label="ul_modules.7.input_gated_resnet.4.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321535824 -> 1401571218800
	1401571218800 [label=AccumulateGrad]
	1401571218464 -> 1410956363712
	1401321536496 [label="ul_modules.7.input_gated_resnet.4.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321536496 -> 1401571218464
	1401571218464 [label=AccumulateGrad]
	1410956363664 -> 1410956363232
	1410956363664 [label=NegBackward0]
	1410956363712 -> 1410956363664
	1410956362896 -> 1410956362800
	1410956362896 -> 1401321536400 [dir=none]
	1401321536400 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410956362896 -> 1410956405520 [dir=none]
	1410956405520 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410956362896 -> 1401321537072 [dir=none]
	1401321537072 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410956362896 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571218272 -> 1410956362896
	1401321537072 [label="ul_modules.7.input_gated_resnet.4.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321537072 -> 1401571218272
	1401571218272 [label=AccumulateGrad]
	1401571218176 -> 1410956362896
	1401321536400 [label="ul_modules.7.input_gated_resnet.4.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321536400 -> 1401571218176
	1401571218176 [label=AccumulateGrad]
	1401571216688 -> 1410956362800
	1401321536880 [label="ul_modules.7.input_gated_resnet.4.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321536880 -> 1401571216688
	1401571216688 [label=AccumulateGrad]
	1410753893840 -> 1410753890960
	1410753893840 -> 1410956406672 [dir=none]
	1410956406672 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410753893840 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410754742064 -> 1410753893840
	1410753895712 -> 1410753893552
	1410753895712 -> 1410754002864 [dir=none]
	1410754002864 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753895712 -> 1410754002672 [dir=none]
	1410754002672 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753895712 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753895040 -> 1410753895712
	1410753895040 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410956363472 -> 1410753895040
	1410956363472 -> 1410754002480 [dir=none]
	1410754002480 [label="input
 (1, 512, 28, 28)" fillcolor=orange]
	1410956363472 -> 1410754002000 [dir=none]
	1410754002000 [label="weight
 (512, 512, 1, 1)" fillcolor=orange]
	1410956363472 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956363376 -> 1410956363472
	1410956363376 -> 1410956407728 [dir=none]
	1410956407728 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410956363376 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956363760 -> 1410956363376
	1410956363760 -> 1410754002384 [dir=none]
	1410754002384 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956363760 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956364288 -> 1410956363760
	1410956364288 [label="CatBackward0
------------
dim: 1"]
	1410956363904 -> 1410956364288
	1410956363904 [label="AddBackward0
------------
alpha: 1"]
	1410956364432 -> 1410956363904
	1410956364432 -> 1410754001520 [dir=none]
	1410754001520 [label="input
 (1, 512, 28, 28)" fillcolor=orange]
	1410956364432 -> 1410754001904 [dir=none]
	1410754001904 [label="weight
 (256, 512, 1, 1)" fillcolor=orange]
	1410956364432 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956364576 -> 1410956364432
	1410956364576 -> 1410754001808 [dir=none]
	1410754001808 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956364576 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956364720 -> 1410956364576
	1410956364720 [label="CatBackward0
------------
dim: 1"]
	1410753895088 -> 1410956364720
	1410956364816 -> 1410956364720
	1410956364816 [label=NegBackward0]
	1410753895088 -> 1410956364816
	1410956364528 -> 1410956364432
	1410956364528 -> 1401321785488 [dir=none]
	1401321785488 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410956364528 -> 1410956409552 [dir=none]
	1410956409552 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410956364528 -> 1401321786160 [dir=none]
	1401321786160 [label="v
 (256, 512, 1, 1)" fillcolor=orange]
	1410956364528 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571219952 -> 1410956364528
	1401321786160 [label="ul_modules.7.out_proj.conv_1.weight_v
 (256, 512, 1, 1)" fillcolor=lightblue]
	1401321786160 -> 1401571219952
	1401571219952 [label=AccumulateGrad]
	1401571219808 -> 1410956364528
	1401321785488 [label="ul_modules.7.out_proj.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321785488 -> 1401571219808
	1401571219808 [label=AccumulateGrad]
	1401571219472 -> 1410956364432
	1401321785968 [label="ul_modules.7.out_proj.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321785968 -> 1401571219472
	1401571219472 [label=AccumulateGrad]
	1410956364384 -> 1410956363904
	1410956364384 -> 1410754002096 [dir=none]
	1410754002096 [label="input
 (1, 256, 28, 28)" fillcolor=orange]
	1410956364384 -> 1410754002288 [dir=none]
	1410754002288 [label="weight
 (256, 256, 1, 1)" fillcolor=orange]
	1410956364384 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956364768 -> 1410956364384
	1410956364768 -> 1410754002192 [dir=none]
	1410754002192 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410956364768 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956364624 -> 1410956364768
	1410956364624 [label="CatBackward0
------------
dim: 1"]
	1410956365008 -> 1410956364624
	1410956365008 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (1, 1, 128, 784)"]
	1410956365152 -> 1410956365008
	1410956365152 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410956365248 -> 1410956365152
	1410956365248 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (1, 784, 128)"]
	1410956365344 -> 1410956365248
	1410956365344 -> 1410956444400 [dir=none]
	1410956444400 [label="mat2
 (1, 784, 128)" fillcolor=orange]
	1410956365344 -> 1410956444688 [dir=none]
	1410956444688 [label="self
 (1, 784, 784)" fillcolor=orange]
	1410956365344 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	1410956365440 -> 1410956365344
	1410956365440 [label="ViewBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 784)"]
	1410956365584 -> 1410956365440
	1410956365584 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 784)"]
	1410956365680 -> 1410956365584
	1410956365680 -> 1410956444976 [dir=none]
	1410956444976 [label="result
 (1, 1, 784, 784)" fillcolor=orange]
	1410956365680 [label="SoftmaxBackward0
----------------------
dim   :     4294967295
result: [saved tensor]"]
	1410956365776 -> 1410956365680
	1410956365776 -> 1410754001040 [dir=none]
	1410754001040 [label="mask
 (1, 1, 784, 784)" fillcolor=orange]
	1410956365776 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	1410956365872 -> 1410956365776
	1410956365872 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (1, 784, 784)"]
	1410956365968 -> 1410956365872
	1410956365968 -> 1410956445648 [dir=none]
	1410956445648 [label="mat2
 (1, 16, 784)" fillcolor=orange]
	1410956365968 -> 1410956445936 [dir=none]
	1410956445936 [label="self
 (1, 784, 16)" fillcolor=orange]
	1410956365968 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	1410956366064 -> 1410956365968
	1410956366064 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (1, 1, 784, 16)"]
	1410956366208 -> 1410956366064
	1410956366208 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 784, 16)"]
	1410956366304 -> 1410956366208
	1410956366304 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410956366400 -> 1410956366304
	1410956366400 -> 1410956446320 [dir=none]
	1410956446320 [label="other
 ()" fillcolor=orange]
	1410956366400 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	1410956366496 -> 1410956366400
	1410956366496 [label="ViewBackward0
----------------------------------
self_sym_sizes: (1, 1, 16, 28, 28)"]
	1410956366592 -> 1410956366496
	1410956366592 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 16, 28, 28)"]
	1410956366688 -> 1410956366592
	1410956366688 -> 1410754000944 [dir=none]
	1410754000944 [label="input
 (1, 258, 28, 28)" fillcolor=orange]
	1410956366688 -> 1410754000656 [dir=none]
	1410754000656 [label="weight
 (16, 258, 1, 1)" fillcolor=orange]
	1410956366688 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956366784 -> 1410956366688
	1410956366784 [label="AddBackward0
------------
alpha: 1"]
	1410956366928 -> 1410956366784
	1410956366928 [label="CatBackward0
------------
dim: 1"]
	1410753895088 -> 1410956366928
	1410956366880 -> 1410956366784
	1410956366880 -> 1410754000752 [dir=none]
	1410754000752 [label="other
 (1, 258, 28, 28)" fillcolor=orange]
	1410956366880 -> 1410754000560 [dir=none]
	1410754000560 [label="self
 (1, 258, 28, 28)" fillcolor=orange]
	1410956366880 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410956366976 -> 1410956366880
	1410956366976 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 516, 28, 28)
split_size    :              258"]
	1410956367216 -> 1410956366976
	1410956367216 -> 1410754000176 [dir=none]
	1410754000176 [label="input
 (1, 516, 28, 28)" fillcolor=orange]
	1410956367216 -> 1410754000272 [dir=none]
	1410754000272 [label="weight
 (516, 516, 1, 1)" fillcolor=orange]
	1410956367216 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (516,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956367312 -> 1410956367216
	1410956367312 -> 1410956448144 [dir=none]
	1410956448144 [label="result1
 (1, 516, 28, 28)" fillcolor=orange]
	1410956367312 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956367456 -> 1410956367312
	1410956367456 -> 1410754000368 [dir=none]
	1410754000368 [label="self
 (1, 516, 28, 28)" fillcolor=orange]
	1410956367456 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956367552 -> 1410956367456
	1410956367552 [label="CatBackward0
------------
dim: 1"]
	1410956367648 -> 1410956367552
	1410956367648 -> 1410753999888 [dir=none]
	1410753999888 [label="input
 (1, 516, 28, 28)" fillcolor=orange]
	1410956367648 -> 1410754000080 [dir=none]
	1410754000080 [label="weight
 (258, 516, 1, 1)" fillcolor=orange]
	1410956367648 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (258,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956367792 -> 1410956367648
	1410956367792 -> 1410753999984 [dir=none]
	1410753999984 [label="self
 (1, 516, 28, 28)" fillcolor=orange]
	1410956367792 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956367936 -> 1410956367792
	1410956367936 [label="CatBackward0
------------
dim: 1"]
	1410956366928 -> 1410956367936
	1410956368032 -> 1410956367936
	1410956368032 [label=NegBackward0]
	1410956366928 -> 1410956368032
	1410956367744 -> 1410956367648
	1410956367744 -> 1401321784144 [dir=none]
	1401321784144 [label="g
 (258, 1, 1, 1)" fillcolor=orange]
	1410956367744 -> 1410956449872 [dir=none]
	1410956449872 [label="result1
 (258, 1, 1, 1)" fillcolor=orange]
	1410956367744 -> 1401321784816 [dir=none]
	1401321784816 [label="v
 (258, 516, 1, 1)" fillcolor=orange]
	1410956367744 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571223456 -> 1410956367744
	1401321784816 [label="ul_modules.7.in_proj_q.0.conv_1.weight_v
 (258, 516, 1, 1)" fillcolor=lightblue]
	1401321784816 -> 1401571223456
	1401571223456 [label=AccumulateGrad]
	1401571223312 -> 1410956367744
	1401321784144 [label="ul_modules.7.in_proj_q.0.conv_1.weight_g
 (258, 1, 1, 1)" fillcolor=lightblue]
	1401321784144 -> 1401571223312
	1401571223312 [label=AccumulateGrad]
	1401571222976 -> 1410956367648
	1401321784624 [label="ul_modules.7.in_proj_q.0.conv_1.bias
 (258)" fillcolor=lightblue]
	1401321784624 -> 1401571222976
	1401571222976 [label=AccumulateGrad]
	1410956367600 -> 1410956367552
	1410956367600 [label=NegBackward0]
	1410956367648 -> 1410956367600
	1410956367264 -> 1410956367216
	1410956367264 -> 1401321784528 [dir=none]
	1401321784528 [label="g
 (516, 1, 1, 1)" fillcolor=orange]
	1410956367264 -> 1410956451504 [dir=none]
	1410956451504 [label="result1
 (516, 1, 1, 1)" fillcolor=orange]
	1410956367264 -> 1401321785200 [dir=none]
	1401321785200 [label="v
 (516, 516, 1, 1)" fillcolor=orange]
	1410956367264 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571222640 -> 1410956367264
	1401321785200 [label="ul_modules.7.in_proj_q.0.conv_2.weight_v
 (516, 516, 1, 1)" fillcolor=lightblue]
	1401321785200 -> 1401571222640
	1401571222640 [label=AccumulateGrad]
	1401571222784 -> 1410956367264
	1401321784528 [label="ul_modules.7.in_proj_q.0.conv_2.weight_g
 (516, 1, 1, 1)" fillcolor=lightblue]
	1401321784528 -> 1401571222784
	1401571222784 [label=AccumulateGrad]
	1401571222352 -> 1410956367216
	1401321785008 [label="ul_modules.7.in_proj_q.0.conv_2.bias
 (516)" fillcolor=lightblue]
	1401321785008 -> 1401571222352
	1401571222352 [label=AccumulateGrad]
	1410956367024 -> 1410956366880
	1410956367024 -> 1410956452656 [dir=none]
	1410956452656 [label="result
 (1, 258, 28, 28)" fillcolor=orange]
	1410956367024 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410956366976 -> 1410956367024
	1410956366736 -> 1410956366688
	1410956366736 -> 1401321784912 [dir=none]
	1401321784912 [label="g
 (16, 1, 1, 1)" fillcolor=orange]
	1410956366736 -> 1410956453232 [dir=none]
	1410956453232 [label="result1
 (16, 1, 1, 1)" fillcolor=orange]
	1410956366736 -> 1401321785776 [dir=none]
	1401321785776 [label="v
 (16, 258, 1, 1)" fillcolor=orange]
	1410956366736 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571223168 -> 1410956366736
	1401321785776 [label="ul_modules.7.in_proj_q.1.weight_v
 (16, 258, 1, 1)" fillcolor=lightblue]
	1401321785776 -> 1401571223168
	1401571223168 [label=AccumulateGrad]
	1401571222304 -> 1410956366736
	1401321784912 [label="ul_modules.7.in_proj_q.1.weight_g
 (16, 1, 1, 1)" fillcolor=lightblue]
	1401321784912 -> 1401571222304
	1401571222304 [label=AccumulateGrad]
	1401571221296 -> 1410956366688
	1401321785584 [label="ul_modules.7.in_proj_q.1.bias
 (16)" fillcolor=lightblue]
	1401321785584 -> 1401571221296
	1401571221296 [label=AccumulateGrad]
	1410956366016 -> 1410956365968
	1410956366016 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 1, 16, 784)"]
	1410956366352 -> 1410956366016
	1410956366352 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 16, 784)"]
	1410956366544 -> 1410956366352
	1410956366544 [label="ViewBackward0
----------------------------------
self_sym_sizes: (1, 1, 16, 28, 28)"]
	1410956366112 -> 1410956366544
	1410956366112 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 16, 28, 28)"]
	1410956367072 -> 1410956366112
	1410956367072 [label="SplitWithSizesBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 144, 28, 28)
split_sizes   :        (16, 128)"]
	1410956367120 -> 1410956367072
	1410956367120 -> 1410753999600 [dir=none]
	1410753999600 [label="input
 (1, 514, 28, 28)" fillcolor=orange]
	1410956367120 -> 1410753999312 [dir=none]
	1410753999312 [label="weight
 (144, 514, 1, 1)" fillcolor=orange]
	1410956367120 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (144,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956367504 -> 1410956367120
	1410956367504 [label="AddBackward0
------------
alpha: 1"]
	1410956367888 -> 1410956367504
	1410956367888 [label="CatBackward0
------------
dim: 1"]
	1410753894800 -> 1410956367888
	1410753895088 -> 1410956367888
	1410956367984 -> 1410956367504
	1410956367984 -> 1410753999408 [dir=none]
	1410753999408 [label="other
 (1, 514, 28, 28)" fillcolor=orange]
	1410956367984 -> 1410753999216 [dir=none]
	1410753999216 [label="self
 (1, 514, 28, 28)" fillcolor=orange]
	1410956367984 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410956367696 -> 1410956367984
	1410956367696 [label="SplitBackward0
---------------------------------
dim           :                 1
self_sym_sizes: (1, 1028, 28, 28)
split_size    :               514"]
	1410956368224 -> 1410956367696
	1410956368224 -> 1410753998832 [dir=none]
	1410753998832 [label="input
 (1, 1028, 28, 28)" fillcolor=orange]
	1410956368224 -> 1410753998928 [dir=none]
	1410753998928 [label="weight
 (1028, 1028, 1, 1)" fillcolor=orange]
	1410956368224 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1028,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956368320 -> 1410956368224
	1410956368320 -> 1410956456208 [dir=none]
	1410956456208 [label="result1
 (1, 1028, 28, 28)" fillcolor=orange]
	1410956368320 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956368464 -> 1410956368320
	1410956368464 -> 1410753999024 [dir=none]
	1410753999024 [label="self
 (1, 1028, 28, 28)" fillcolor=orange]
	1410956368464 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956368560 -> 1410956368464
	1410956368560 [label="CatBackward0
------------
dim: 1"]
	1410956368656 -> 1410956368560
	1410956368656 -> 1410753998448 [dir=none]
	1410753998448 [label="input
 (1, 1028, 28, 28)" fillcolor=orange]
	1410956368656 -> 1410753998640 [dir=none]
	1410753998640 [label="weight
 (514, 1028, 1, 1)" fillcolor=orange]
	1410956368656 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (514,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956368800 -> 1410956368656
	1410956368800 -> 1410753997968 [dir=none]
	1410753997968 [label="self
 (1, 1028, 28, 28)" fillcolor=orange]
	1410956368800 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956368944 -> 1410956368800
	1410956368944 [label="CatBackward0
------------
dim: 1"]
	1410956367888 -> 1410956368944
	1410956369040 -> 1410956368944
	1410956369040 [label=NegBackward0]
	1410956367888 -> 1410956369040
	1410956368752 -> 1410956368656
	1410956368752 -> 1401321536784 [dir=none]
	1401321536784 [label="g
 (514, 1, 1, 1)" fillcolor=orange]
	1410956368752 -> 1410956457936 [dir=none]
	1410956457936 [label="result1
 (514, 1, 1, 1)" fillcolor=orange]
	1410956368752 -> 1401321783472 [dir=none]
	1401321783472 [label="v
 (514, 1028, 1, 1)" fillcolor=orange]
	1410956368752 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571224896 -> 1410956368752
	1401321783472 [label="ul_modules.7.in_proj_kv.0.conv_1.weight_v
 (514, 1028, 1, 1)" fillcolor=lightblue]
	1401321783472 -> 1401571224896
	1401571224896 [label=AccumulateGrad]
	1401571224752 -> 1410956368752
	1401321536784 [label="ul_modules.7.in_proj_kv.0.conv_1.weight_g
 (514, 1, 1, 1)" fillcolor=lightblue]
	1401321536784 -> 1401571224752
	1401571224752 [label=AccumulateGrad]
	1401571224416 -> 1410956368656
	1401321537456 [label="ul_modules.7.in_proj_kv.0.conv_1.bias
 (514)" fillcolor=lightblue]
	1401321537456 -> 1401571224416
	1401571224416 [label=AccumulateGrad]
	1410956368608 -> 1410956368560
	1410956368608 [label=NegBackward0]
	1410956368656 -> 1410956368608
	1410956368272 -> 1410956368224
	1410956368272 -> 1401321537360 [dir=none]
	1401321537360 [label="g
 (1028, 1, 1, 1)" fillcolor=orange]
	1410956368272 -> 1410956459568 [dir=none]
	1410956459568 [label="result1
 (1028, 1, 1, 1)" fillcolor=orange]
	1410956368272 -> 1401321783856 [dir=none]
	1401321783856 [label="v
 (1028, 1028, 1, 1)" fillcolor=orange]
	1410956368272 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571224080 -> 1410956368272
	1401321783856 [label="ul_modules.7.in_proj_kv.0.conv_2.weight_v
 (1028, 1028, 1, 1)" fillcolor=lightblue]
	1401321783856 -> 1401571224080
	1401571224080 [label=AccumulateGrad]
	1401571224224 -> 1410956368272
	1401321537360 [label="ul_modules.7.in_proj_kv.0.conv_2.weight_g
 (1028, 1, 1, 1)" fillcolor=lightblue]
	1401321537360 -> 1401571224224
	1401571224224 [label=AccumulateGrad]
	1401571223792 -> 1410956368224
	1401321783664 [label="ul_modules.7.in_proj_kv.0.conv_2.bias
 (1028)" fillcolor=lightblue]
	1401321783664 -> 1401571223792
	1401571223792 [label=AccumulateGrad]
	1410956368128 -> 1410956367984
	1410956368128 -> 1410956477168 [dir=none]
	1410956477168 [label="result
 (1, 514, 28, 28)" fillcolor=orange]
	1410956368128 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410956367696 -> 1410956368128
	1410956367168 -> 1410956367120
	1410956367168 -> 1401321783568 [dir=none]
	1401321783568 [label="g
 (144, 1, 1, 1)" fillcolor=orange]
	1410956367168 -> 1410956477744 [dir=none]
	1410956477744 [label="result1
 (144, 1, 1, 1)" fillcolor=orange]
	1410956367168 -> 1401321784432 [dir=none]
	1401321784432 [label="v
 (144, 514, 1, 1)" fillcolor=orange]
	1410956367168 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571224608 -> 1410956367168
	1401321784432 [label="ul_modules.7.in_proj_kv.1.weight_v
 (144, 514, 1, 1)" fillcolor=lightblue]
	1401321784432 -> 1401571224608
	1401571224608 [label=AccumulateGrad]
	1401571223744 -> 1410956367168
	1401321783568 [label="ul_modules.7.in_proj_kv.1.weight_g
 (144, 1, 1, 1)" fillcolor=lightblue]
	1401321783568 -> 1401571223744
	1401571223744 [label=AccumulateGrad]
	1401571221344 -> 1410956367120
	1401321784240 [label="ul_modules.7.in_proj_kv.1.bias
 (144)" fillcolor=lightblue]
	1401321784240 -> 1401571221344
	1401571221344 [label=AccumulateGrad]
	1410956365392 -> 1410956365344
	1410956365392 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 128)"]
	1410956365728 -> 1410956365392
	1410956365728 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 128)"]
	1410956365920 -> 1410956365728
	1410956365920 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410956366256 -> 1410956365920
	1410956366256 [label="ViewBackward0
-----------------------------------
self_sym_sizes: (1, 1, 128, 28, 28)"]
	1410956366640 -> 1410956366256
	1410956366640 [label="ViewBackward0
--------------------------------
self_sym_sizes: (1, 128, 28, 28)"]
	1410956367072 -> 1410956366640
	1410956364960 -> 1410956364624
	1410956364960 [label=NegBackward0]
	1410956365008 -> 1410956364960
	1410956364672 -> 1410956364384
	1410956364672 -> 1401321785872 [dir=none]
	1401321785872 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410956364672 -> 1410956479952 [dir=none]
	1410956479952 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410956364672 -> 1401321786544 [dir=none]
	1401321786544 [label="v
 (256, 256, 1, 1)" fillcolor=orange]
	1410956364672 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571220480 -> 1410956364672
	1401321786544 [label="ul_modules.7.out_proj.conv_1_sc.weight_v
 (256, 256, 1, 1)" fillcolor=lightblue]
	1401321786544 -> 1401571220480
	1401571220480 [label=AccumulateGrad]
	1401571220000 -> 1410956364672
	1401321785872 [label="ul_modules.7.out_proj.conv_1_sc.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321785872 -> 1401571220000
	1401571220000 [label=AccumulateGrad]
	1401571219616 -> 1410956364384
	1401321786352 [label="ul_modules.7.out_proj.conv_1_sc.bias
 (256)" fillcolor=lightblue]
	1401321786352 -> 1401571219616
	1401571219616 [label=AccumulateGrad]
	1410956364240 -> 1410956364288
	1410956364240 [label=NegBackward0]
	1410956363904 -> 1410956364240
	1410956363568 -> 1410956363472
	1410956363568 -> 1401321786256 [dir=none]
	1401321786256 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410956363568 -> 1410956481584 [dir=none]
	1410956481584 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410956363568 -> 1401321786928 [dir=none]
	1401321786928 [label="v
 (512, 512, 1, 1)" fillcolor=orange]
	1410956363568 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571218704 -> 1410956363568
	1401321786928 [label="ul_modules.7.out_proj.conv_2.weight_v
 (512, 512, 1, 1)" fillcolor=lightblue]
	1401321786928 -> 1401571218704
	1401571218704 [label=AccumulateGrad]
	1401571219136 -> 1410956363568
	1401321786256 [label="ul_modules.7.out_proj.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321786256 -> 1401571219136
	1401571219136 [label=AccumulateGrad]
	1401571217648 -> 1410956363472
	1401321786736 [label="ul_modules.7.out_proj.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321786736 -> 1401571217648
	1401571217648 [label=AccumulateGrad]
	1410956363280 -> 1410753895712
	1410956363280 -> 1410956482736 [dir=none]
	1410956482736 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410956363280 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753895040 -> 1410956363280
	1410753893504 -> 1410753893408
	1410753893504 -> 1410754002768 [dir=none]
	1410754002768 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753893504 -> 1410753997872 [dir=none]
	1410753997872 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753893504 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753893600 -> 1410753893504
	1410753893600 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410956364144 -> 1410753893600
	1410956364144 -> 1410754000848 [dir=none]
	1410754000848 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956364144 -> 1410753995088 [dir=none]
	1410753995088 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410956364144 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956364864 -> 1410956364144
	1410956364864 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956364912 -> 1410956364864
	1410956364912 -> 1410956483792 [dir=none]
	1410956483792 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410956364912 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956365200 -> 1410956364912
	1410956365200 -> 1410754001136 [dir=none]
	1410754001136 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956365200 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956365056 -> 1410956365200
	1410956365056 [label="CatBackward0
------------
dim: 1"]
	1410956365824 -> 1410956365056
	1410956365824 -> 1410754001424 [dir=none]
	1410754001424 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956365824 -> 1410754001616 [dir=none]
	1410754001616 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410956365824 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956367408 -> 1410956365824
	1410956367408 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956366160 -> 1410956367408
	1410956366160 -> 1410754001712 [dir=none]
	1410754001712 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956366160 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956368080 -> 1410956366160
	1410956368080 [label="CatBackward0
------------
dim: 1"]
	1410753893552 -> 1410956368080
	1410956367840 -> 1410956368080
	1410956367840 [label=NegBackward0]
	1410753893552 -> 1410956367840
	1410956366448 -> 1410956365824
	1410956366448 -> 1401321786640 [dir=none]
	1401321786640 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410956366448 -> 1410956485712 [dir=none]
	1410956485712 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410956366448 -> 1401321787504 [dir=none]
	1401321787504 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410956366448 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571224992 -> 1410956366448
	1401321787504 [label="ul_modules.8.input_gated_resnet.0.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321787504 -> 1401571224992
	1401571224992 [label=AccumulateGrad]
	1401571224656 -> 1410956366448
	1401321786640 [label="ul_modules.8.input_gated_resnet.0.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321786640 -> 1401571224656
	1401571224656 [label=AccumulateGrad]
	1401571223216 -> 1410956365824
	1401321787312 [label="ul_modules.8.input_gated_resnet.0.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321787312 -> 1401571223216
	1401571223216 [label=AccumulateGrad]
	1410956365632 -> 1410956365056
	1410956365632 [label=NegBackward0]
	1410956365824 -> 1410956365632
	1410956364048 -> 1410956364144
	1410956364048 -> 1401321787216 [dir=none]
	1401321787216 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410956364048 -> 1410956487344 [dir=none]
	1410956487344 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410956364048 -> 1401321787888 [dir=none]
	1401321787888 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410956364048 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571221632 -> 1410956364048
	1401321787888 [label="ul_modules.8.input_gated_resnet.0.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321787888 -> 1401571221632
	1401571221632 [label=AccumulateGrad]
	1401571221008 -> 1410956364048
	1401321787216 [label="ul_modules.8.input_gated_resnet.0.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321787216 -> 1401571221008
	1401571221008 [label=AccumulateGrad]
	1401571218608 -> 1410956364144
	1401321787696 [label="ul_modules.8.input_gated_resnet.0.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321787696 -> 1401571218608
	1401571218608 [label=AccumulateGrad]
	1410956363952 -> 1410753893504
	1410956363952 -> 1410956488496 [dir=none]
	1410956488496 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410956363952 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753893600 -> 1410956363952
	1410753893360 -> 1410753893264
	1410753893360 -> 1410754003984 [dir=none]
	1410754003984 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753893360 -> 1410754003792 [dir=none]
	1410754003792 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753893360 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753893456 -> 1410753893360
	1410753893456 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410956365296 -> 1410753893456
	1410956365296 -> 1410754003600 [dir=none]
	1410754003600 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956365296 -> 1410753998736 [dir=none]
	1410753998736 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410956365296 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956364480 -> 1410956365296
	1410956364480 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956365488 -> 1410956364480
	1410956365488 -> 1410956489552 [dir=none]
	1410956489552 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410956365488 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956368512 -> 1410956365488
	1410956368512 -> 1410754003504 [dir=none]
	1410754003504 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956368512 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956366832 -> 1410956368512
	1410956366832 [label="CatBackward0
------------
dim: 1"]
	1410956368992 -> 1410956366832
	1410956368992 -> 1410754003312 [dir=none]
	1410754003312 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956368992 -> 1410754003024 [dir=none]
	1410754003024 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410956368992 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956369136 -> 1410956368992
	1410956369136 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956369184 -> 1410956369136
	1410956369184 -> 1410754001232 [dir=none]
	1410754001232 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956369184 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956369280 -> 1410956369184
	1410956369280 [label="CatBackward0
------------
dim: 1"]
	1410753893408 -> 1410956369280
	1410956369376 -> 1410956369280
	1410956369376 [label=NegBackward0]
	1410753893408 -> 1410956369376
	1410956368704 -> 1410956368992
	1410956368704 -> 1401321787600 [dir=none]
	1401321787600 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410956368704 -> 1410956491472 [dir=none]
	1410956491472 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410956368704 -> 1401321788464 [dir=none]
	1401321788464 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410956368704 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571225952 -> 1410956368704
	1401321788464 [label="ul_modules.8.input_gated_resnet.1.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321788464 -> 1401571225952
	1401571225952 [label=AccumulateGrad]
	1401571225856 -> 1410956368704
	1401321787600 [label="ul_modules.8.input_gated_resnet.1.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321787600 -> 1401571225856
	1401571225856 [label=AccumulateGrad]
	1401571225520 -> 1410956368992
	1401321788272 [label="ul_modules.8.input_gated_resnet.1.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321788272 -> 1401571225520
	1401571225520 [label=AccumulateGrad]
	1410956368368 -> 1410956366832
	1410956368368 [label=NegBackward0]
	1410956368992 -> 1410956368368
	1410956365104 -> 1410956365296
	1410956365104 -> 1401321788176 [dir=none]
	1401321788176 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410956365104 -> 1410956509552 [dir=none]
	1410956509552 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410956365104 -> 1401321788848 [dir=none]
	1401321788848 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410956365104 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571225328 -> 1410956365104
	1401321788848 [label="ul_modules.8.input_gated_resnet.1.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321788848 -> 1401571225328
	1401571225328 [label=AccumulateGrad]
	1401571225232 -> 1410956365104
	1401321788176 [label="ul_modules.8.input_gated_resnet.1.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321788176 -> 1401571225232
	1401571225232 [label=AccumulateGrad]
	1401571220384 -> 1410956365296
	1401321788656 [label="ul_modules.8.input_gated_resnet.1.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321788656 -> 1401571220384
	1401571220384 [label=AccumulateGrad]
	1410956364336 -> 1410753893360
	1410956364336 -> 1410956510704 [dir=none]
	1410956510704 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410956364336 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753893456 -> 1410956364336
	1410753893216 -> 1410753893120
	1410753893216 -> 1410754004848 [dir=none]
	1410754004848 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753893216 -> 1410754004656 [dir=none]
	1410754004656 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753893216 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753893312 -> 1410753893216
	1410753893312 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410956368416 -> 1410753893312
	1410956368416 -> 1410754004464 [dir=none]
	1410754004464 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956368416 -> 1410754003216 [dir=none]
	1410754003216 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410956368416 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956367360 -> 1410956368416
	1410956367360 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956368896 -> 1410956367360
	1410956368896 -> 1410956511760 [dir=none]
	1410956511760 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410956368896 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956369472 -> 1410956368896
	1410956369472 -> 1410754004368 [dir=none]
	1410754004368 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956369472 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956369088 -> 1410956369472
	1410956369088 [label="CatBackward0
------------
dim: 1"]
	1410956369568 -> 1410956369088
	1410956369568 -> 1410754004080 [dir=none]
	1410754004080 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956369568 -> 1410754003408 [dir=none]
	1410754003408 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410956369568 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956369712 -> 1410956369568
	1410956369712 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956369856 -> 1410956369712
	1410956369856 -> 1410754003888 [dir=none]
	1410754003888 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956369856 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956369952 -> 1410956369856
	1410956369952 [label="CatBackward0
------------
dim: 1"]
	1410753893264 -> 1410956369952
	1410956370048 -> 1410956369952
	1410956370048 [label=NegBackward0]
	1410753893264 -> 1410956370048
	1410956369664 -> 1410956369568
	1410956369664 -> 1401321788560 [dir=none]
	1401321788560 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410956369664 -> 1410956513680 [dir=none]
	1410956513680 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410956369664 -> 1401321789424 [dir=none]
	1401321789424 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410956369664 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571226912 -> 1410956369664
	1401321789424 [label="ul_modules.8.input_gated_resnet.2.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321789424 -> 1401571226912
	1401571226912 [label=AccumulateGrad]
	1401571226816 -> 1410956369664
	1401321788560 [label="ul_modules.8.input_gated_resnet.2.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321788560 -> 1401571226816
	1401571226816 [label=AccumulateGrad]
	1401571226480 -> 1410956369568
	1401321789232 [label="ul_modules.8.input_gated_resnet.2.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321789232 -> 1401571226480
	1401571226480 [label=AccumulateGrad]
	1410956369520 -> 1410956369088
	1410956369520 [label=NegBackward0]
	1410956369568 -> 1410956369520
	1410956368176 -> 1410956368416
	1410956368176 -> 1401321789136 [dir=none]
	1401321789136 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410956368176 -> 1410956515312 [dir=none]
	1410956515312 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410956368176 -> 1401321789808 [dir=none]
	1401321789808 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410956368176 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571226288 -> 1410956368176
	1401321789808 [label="ul_modules.8.input_gated_resnet.2.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321789808 -> 1401571226288
	1401571226288 [label=AccumulateGrad]
	1401571226192 -> 1410956368176
	1401321789136 [label="ul_modules.8.input_gated_resnet.2.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321789136 -> 1401571226192
	1401571226192 [label=AccumulateGrad]
	1401571223504 -> 1410956368416
	1401321789616 [label="ul_modules.8.input_gated_resnet.2.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321789616 -> 1401571223504
	1401571223504 [label=AccumulateGrad]
	1410956365536 -> 1410753893216
	1410956365536 -> 1410956516464 [dir=none]
	1410956516464 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410956365536 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753893312 -> 1410956365536
	1410753893072 -> 1410753892976
	1410753893072 -> 1410754005712 [dir=none]
	1410754005712 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753893072 -> 1410754005520 [dir=none]
	1410754005520 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753893072 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753893168 -> 1410753893072
	1410753893168 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410956369328 -> 1410753893168
	1410956369328 -> 1410754005328 [dir=none]
	1410754005328 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956369328 -> 1410754004176 [dir=none]
	1410754004176 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410956369328 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956369232 -> 1410956369328
	1410956369232 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956369616 -> 1410956369232
	1410956369616 -> 1410956517520 [dir=none]
	1410956517520 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410956369616 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956370144 -> 1410956369616
	1410956370144 -> 1410754005232 [dir=none]
	1410754005232 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956370144 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956369760 -> 1410956370144
	1410956369760 [label="CatBackward0
------------
dim: 1"]
	1410956370240 -> 1410956369760
	1410956370240 -> 1410754004944 [dir=none]
	1410754004944 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956370240 -> 1410754004272 [dir=none]
	1410754004272 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410956370240 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956370384 -> 1410956370240
	1410956370384 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956370528 -> 1410956370384
	1410956370528 -> 1410754004752 [dir=none]
	1410754004752 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956370528 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956370624 -> 1410956370528
	1410956370624 [label="CatBackward0
------------
dim: 1"]
	1410753893120 -> 1410956370624
	1410956370720 -> 1410956370624
	1410956370720 [label=NegBackward0]
	1410753893120 -> 1410956370720
	1410956370336 -> 1410956370240
	1410956370336 -> 1401321789520 [dir=none]
	1401321789520 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410956370336 -> 1410956519440 [dir=none]
	1410956519440 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410956370336 -> 1401321790384 [dir=none]
	1401321790384 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410956370336 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571227872 -> 1410956370336
	1401321790384 [label="ul_modules.8.input_gated_resnet.3.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321790384 -> 1401571227872
	1401571227872 [label=AccumulateGrad]
	1401571227776 -> 1410956370336
	1401321789520 [label="ul_modules.8.input_gated_resnet.3.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321789520 -> 1401571227776
	1401571227776 [label=AccumulateGrad]
	1401571227440 -> 1410956370240
	1401321790192 [label="ul_modules.8.input_gated_resnet.3.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321790192 -> 1401571227440
	1401571227440 [label=AccumulateGrad]
	1410956370192 -> 1410956369760
	1410956370192 [label=NegBackward0]
	1410956370240 -> 1410956370192
	1410956369424 -> 1410956369328
	1410956369424 -> 1401321790096 [dir=none]
	1401321790096 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410956369424 -> 1410956521072 [dir=none]
	1410956521072 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410956369424 -> 1401321790768 [dir=none]
	1401321790768 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410956369424 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571227248 -> 1410956369424
	1401321790768 [label="ul_modules.8.input_gated_resnet.3.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321790768 -> 1401571227248
	1401571227248 [label=AccumulateGrad]
	1401571227152 -> 1410956369424
	1401321790096 [label="ul_modules.8.input_gated_resnet.3.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321790096 -> 1401571227152
	1401571227152 [label=AccumulateGrad]
	1401571225664 -> 1410956369328
	1401321790576 [label="ul_modules.8.input_gated_resnet.3.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321790576 -> 1401571225664
	1401571225664 [label=AccumulateGrad]
	1410956368848 -> 1410753893072
	1410956368848 -> 1410956522224 [dir=none]
	1410956522224 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410956368848 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753893168 -> 1410956368848
	1410753892928 -> 1410753892832
	1410753892928 -> 1410754006576 [dir=none]
	1410754006576 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753892928 -> 1410754006384 [dir=none]
	1410754006384 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753892928 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753893024 -> 1410753892928
	1410753893024 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410956370000 -> 1410753893024
	1410956370000 -> 1410754006192 [dir=none]
	1410754006192 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956370000 -> 1410754005040 [dir=none]
	1410754005040 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410956370000 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956369904 -> 1410956370000
	1410956369904 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956370288 -> 1410956369904
	1410956370288 -> 1410956523280 [dir=none]
	1410956523280 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410956370288 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956370816 -> 1410956370288
	1410956370816 -> 1410754006096 [dir=none]
	1410754006096 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956370816 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956370432 -> 1410956370816
	1410956370432 [label="CatBackward0
------------
dim: 1"]
	1410956370912 -> 1410956370432
	1410956370912 -> 1410754005808 [dir=none]
	1410754005808 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956370912 -> 1410754005136 [dir=none]
	1410754005136 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410956370912 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956371056 -> 1410956370912
	1410956371056 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956371200 -> 1410956371056
	1410956371200 -> 1410754005616 [dir=none]
	1410754005616 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956371200 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956371296 -> 1410956371200
	1410956371296 [label="CatBackward0
------------
dim: 1"]
	1410753892976 -> 1410956371296
	1410956371392 -> 1410956371296
	1410956371392 [label=NegBackward0]
	1410753892976 -> 1410956371392
	1410956371008 -> 1410956370912
	1410956371008 -> 1401321790480 [dir=none]
	1401321790480 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410956371008 -> 1410956525200 [dir=none]
	1410956525200 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410956371008 -> 1401321791344 [dir=none]
	1401321791344 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410956371008 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571228832 -> 1410956371008
	1401321791344 [label="ul_modules.8.input_gated_resnet.4.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321791344 -> 1401571228832
	1401571228832 [label=AccumulateGrad]
	1401571228736 -> 1410956371008
	1401321790480 [label="ul_modules.8.input_gated_resnet.4.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321790480 -> 1401571228736
	1401571228736 [label=AccumulateGrad]
	1401571228400 -> 1410956370912
	1401321791152 [label="ul_modules.8.input_gated_resnet.4.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321791152 -> 1401571228400
	1401571228400 [label=AccumulateGrad]
	1410956370864 -> 1410956370432
	1410956370864 [label=NegBackward0]
	1410956370912 -> 1410956370864
	1410956370096 -> 1410956370000
	1410956370096 -> 1401321791056 [dir=none]
	1401321791056 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410956370096 -> 1410956543280 [dir=none]
	1410956543280 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410956370096 -> 1401321791728 [dir=none]
	1401321791728 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410956370096 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571228208 -> 1410956370096
	1401321791728 [label="ul_modules.8.input_gated_resnet.4.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321791728 -> 1401571228208
	1401571228208 [label=AccumulateGrad]
	1401571228112 -> 1410956370096
	1401321791056 [label="ul_modules.8.input_gated_resnet.4.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321791056 -> 1401571228112
	1401571228112 [label=AccumulateGrad]
	1401571226624 -> 1410956370000
	1401321791536 [label="ul_modules.8.input_gated_resnet.4.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321791536 -> 1401571226624
	1401571226624 [label=AccumulateGrad]
	1410956369808 -> 1410753892928
	1410956369808 -> 1410956544432 [dir=none]
	1410956544432 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410956369808 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753893024 -> 1410956369808
	1410753892784 -> 1410753892688
	1410753892784 -> 1410754010896 [dir=none]
	1410754010896 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753892784 -> 1410754010704 [dir=none]
	1410754010704 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753892784 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753892880 -> 1410753892784
	1410753892880 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410956370672 -> 1410753892880
	1410956370672 -> 1410754010512 [dir=none]
	1410754010512 [label="input
 (1, 512, 28, 28)" fillcolor=orange]
	1410956370672 -> 1410754010032 [dir=none]
	1410754010032 [label="weight
 (512, 512, 1, 1)" fillcolor=orange]
	1410956370672 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956370576 -> 1410956370672
	1410956370576 -> 1410956545488 [dir=none]
	1410956545488 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410956370576 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956370960 -> 1410956370576
	1410956370960 -> 1410754010416 [dir=none]
	1410754010416 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956370960 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956371488 -> 1410956370960
	1410956371488 [label="CatBackward0
------------
dim: 1"]
	1410956371104 -> 1410956371488
	1410956371104 [label="AddBackward0
------------
alpha: 1"]
	1410956371632 -> 1410956371104
	1410956371632 -> 1410754009552 [dir=none]
	1410754009552 [label="input
 (1, 512, 28, 28)" fillcolor=orange]
	1410956371632 -> 1410754009936 [dir=none]
	1410754009936 [label="weight
 (256, 512, 1, 1)" fillcolor=orange]
	1410956371632 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956371776 -> 1410956371632
	1410956371776 -> 1410754009840 [dir=none]
	1410754009840 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956371776 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956371920 -> 1410956371776
	1410956371920 [label="CatBackward0
------------
dim: 1"]
	1410753892832 -> 1410956371920
	1410956372016 -> 1410956371920
	1410956372016 [label=NegBackward0]
	1410753892832 -> 1410956372016
	1410956371728 -> 1410956371632
	1410956371728 -> 1401321794320 [dir=none]
	1401321794320 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410956371728 -> 1410956547312 [dir=none]
	1410956547312 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410956371728 -> 1401321794992 [dir=none]
	1401321794992 [label="v
 (256, 512, 1, 1)" fillcolor=orange]
	1410956371728 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571229600 -> 1410956371728
	1401321794992 [label="ul_modules.8.out_proj.conv_1.weight_v
 (256, 512, 1, 1)" fillcolor=lightblue]
	1401321794992 -> 1401571229600
	1401571229600 [label=AccumulateGrad]
	1401571459328 -> 1410956371728
	1401321794320 [label="ul_modules.8.out_proj.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321794320 -> 1401571459328
	1401571459328 [label=AccumulateGrad]
	1401571229408 -> 1410956371632
	1401321794800 [label="ul_modules.8.out_proj.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321794800 -> 1401571229408
	1401571229408 [label=AccumulateGrad]
	1410956371584 -> 1410956371104
	1410956371584 -> 1410754010128 [dir=none]
	1410754010128 [label="input
 (1, 256, 28, 28)" fillcolor=orange]
	1410956371584 -> 1410754010320 [dir=none]
	1410754010320 [label="weight
 (256, 256, 1, 1)" fillcolor=orange]
	1410956371584 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956371968 -> 1410956371584
	1410956371968 -> 1410754010224 [dir=none]
	1410754010224 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410956371968 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956371824 -> 1410956371968
	1410956371824 [label="CatBackward0
------------
dim: 1"]
	1410956372208 -> 1410956371824
	1410956372208 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (1, 1, 128, 784)"]
	1410956372352 -> 1410956372208
	1410956372352 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410956372448 -> 1410956372352
	1410956372448 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (1, 784, 128)"]
	1410956372544 -> 1410956372448
	1410956372544 -> 1410956549328 [dir=none]
	1410956549328 [label="mat2
 (1, 784, 128)" fillcolor=orange]
	1410956372544 -> 1410956549616 [dir=none]
	1410956549616 [label="self
 (1, 784, 784)" fillcolor=orange]
	1410956372544 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	1410956372640 -> 1410956372544
	1410956372640 [label="ViewBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 784)"]
	1410956372784 -> 1410956372640
	1410956372784 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 784)"]
	1410956372880 -> 1410956372784
	1410956372880 -> 1410956549904 [dir=none]
	1410956549904 [label="result
 (1, 1, 784, 784)" fillcolor=orange]
	1410956372880 [label="SoftmaxBackward0
----------------------
dim   :     4294967295
result: [saved tensor]"]
	1410956372976 -> 1410956372880
	1410956372976 -> 1410754009072 [dir=none]
	1410754009072 [label="mask
 (1, 1, 784, 784)" fillcolor=orange]
	1410956372976 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	1410956373072 -> 1410956372976
	1410956373072 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (1, 784, 784)"]
	1410956373168 -> 1410956373072
	1410956373168 -> 1410956550576 [dir=none]
	1410956550576 [label="mat2
 (1, 16, 784)" fillcolor=orange]
	1410956373168 -> 1410956550864 [dir=none]
	1410956550864 [label="self
 (1, 784, 16)" fillcolor=orange]
	1410956373168 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	1410956373264 -> 1410956373168
	1410956373264 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (1, 1, 784, 16)"]
	1410956373408 -> 1410956373264
	1410956373408 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 784, 16)"]
	1410956373504 -> 1410956373408
	1410956373504 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410956373600 -> 1410956373504
	1410956373600 -> 1410956551248 [dir=none]
	1410956551248 [label="other
 ()" fillcolor=orange]
	1410956373600 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	1410956373696 -> 1410956373600
	1410956373696 [label="ViewBackward0
----------------------------------
self_sym_sizes: (1, 1, 16, 28, 28)"]
	1410956373792 -> 1410956373696
	1410956373792 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 16, 28, 28)"]
	1410956373888 -> 1410956373792
	1410956373888 -> 1410754008976 [dir=none]
	1410754008976 [label="input
 (1, 258, 28, 28)" fillcolor=orange]
	1410956373888 -> 1410754008688 [dir=none]
	1410754008688 [label="weight
 (16, 258, 1, 1)" fillcolor=orange]
	1410956373888 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956373984 -> 1410956373888
	1410956373984 [label="AddBackward0
------------
alpha: 1"]
	1410956374128 -> 1410956373984
	1410956374128 [label="CatBackward0
------------
dim: 1"]
	1410753892832 -> 1410956374128
	1410956374080 -> 1410956373984
	1410956374080 -> 1410754008784 [dir=none]
	1410754008784 [label="other
 (1, 258, 28, 28)" fillcolor=orange]
	1410956374080 -> 1410754008592 [dir=none]
	1410754008592 [label="self
 (1, 258, 28, 28)" fillcolor=orange]
	1410956374080 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410956374176 -> 1410956374080
	1410956374176 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 516, 28, 28)
split_size    :              258"]
	1410956374416 -> 1410956374176
	1410956374416 -> 1410754008208 [dir=none]
	1410754008208 [label="input
 (1, 516, 28, 28)" fillcolor=orange]
	1410956374416 -> 1410754008304 [dir=none]
	1410754008304 [label="weight
 (516, 516, 1, 1)" fillcolor=orange]
	1410956374416 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (516,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956374512 -> 1410956374416
	1410956374512 -> 1410956553072 [dir=none]
	1410956553072 [label="result1
 (1, 516, 28, 28)" fillcolor=orange]
	1410956374512 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956374656 -> 1410956374512
	1410956374656 -> 1410754008400 [dir=none]
	1410754008400 [label="self
 (1, 516, 28, 28)" fillcolor=orange]
	1410956374656 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956374752 -> 1410956374656
	1410956374752 [label="CatBackward0
------------
dim: 1"]
	1410956374848 -> 1410956374752
	1410956374848 -> 1410754007920 [dir=none]
	1410754007920 [label="input
 (1, 516, 28, 28)" fillcolor=orange]
	1410956374848 -> 1410754008112 [dir=none]
	1410754008112 [label="weight
 (258, 516, 1, 1)" fillcolor=orange]
	1410956374848 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (258,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956374992 -> 1410956374848
	1410956374992 -> 1410754008016 [dir=none]
	1410754008016 [label="self
 (1, 516, 28, 28)" fillcolor=orange]
	1410956374992 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956375136 -> 1410956374992
	1410956375136 [label="CatBackward0
------------
dim: 1"]
	1410956374128 -> 1410956375136
	1410956375232 -> 1410956375136
	1410956375232 [label=NegBackward0]
	1410956374128 -> 1410956375232
	1410956374944 -> 1410956374848
	1410956374944 -> 1401321792976 [dir=none]
	1401321792976 [label="g
 (258, 1, 1, 1)" fillcolor=orange]
	1410956374944 -> 1410956554800 [dir=none]
	1410956554800 [label="result1
 (258, 1, 1, 1)" fillcolor=orange]
	1410956374944 -> 1401321793648 [dir=none]
	1401321793648 [label="v
 (258, 516, 1, 1)" fillcolor=orange]
	1410956374944 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571462832 -> 1410956374944
	1401321793648 [label="ul_modules.8.in_proj_q.0.conv_1.weight_v
 (258, 516, 1, 1)" fillcolor=lightblue]
	1401321793648 -> 1401571462832
	1401571462832 [label=AccumulateGrad]
	1401571462688 -> 1410956374944
	1401321792976 [label="ul_modules.8.in_proj_q.0.conv_1.weight_g
 (258, 1, 1, 1)" fillcolor=lightblue]
	1401321792976 -> 1401571462688
	1401571462688 [label=AccumulateGrad]
	1401571462352 -> 1410956374848
	1401321793456 [label="ul_modules.8.in_proj_q.0.conv_1.bias
 (258)" fillcolor=lightblue]
	1401321793456 -> 1401571462352
	1401571462352 [label=AccumulateGrad]
	1410956374800 -> 1410956374752
	1410956374800 [label=NegBackward0]
	1410956374848 -> 1410956374800
	1410956374464 -> 1410956374416
	1410956374464 -> 1401321793360 [dir=none]
	1401321793360 [label="g
 (516, 1, 1, 1)" fillcolor=orange]
	1410956374464 -> 1410956556432 [dir=none]
	1410956556432 [label="result1
 (516, 1, 1, 1)" fillcolor=orange]
	1410956374464 -> 1401321794032 [dir=none]
	1401321794032 [label="v
 (516, 516, 1, 1)" fillcolor=orange]
	1410956374464 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571462016 -> 1410956374464
	1401321794032 [label="ul_modules.8.in_proj_q.0.conv_2.weight_v
 (516, 516, 1, 1)" fillcolor=lightblue]
	1401321794032 -> 1401571462016
	1401571462016 [label=AccumulateGrad]
	1401571462160 -> 1410956374464
	1401321793360 [label="ul_modules.8.in_proj_q.0.conv_2.weight_g
 (516, 1, 1, 1)" fillcolor=lightblue]
	1401321793360 -> 1401571462160
	1401571462160 [label=AccumulateGrad]
	1401571461728 -> 1410956374416
	1401321793840 [label="ul_modules.8.in_proj_q.0.conv_2.bias
 (516)" fillcolor=lightblue]
	1401321793840 -> 1401571461728
	1401571461728 [label=AccumulateGrad]
	1410956374224 -> 1410956374080
	1410956374224 -> 1410956557584 [dir=none]
	1410956557584 [label="result
 (1, 258, 28, 28)" fillcolor=orange]
	1410956374224 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410956374176 -> 1410956374224
	1410956373936 -> 1410956373888
	1410956373936 -> 1401321793744 [dir=none]
	1401321793744 [label="g
 (16, 1, 1, 1)" fillcolor=orange]
	1410956373936 -> 1410956558160 [dir=none]
	1410956558160 [label="result1
 (16, 1, 1, 1)" fillcolor=orange]
	1410956373936 -> 1401321794608 [dir=none]
	1401321794608 [label="v
 (16, 258, 1, 1)" fillcolor=orange]
	1410956373936 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571462544 -> 1410956373936
	1401321794608 [label="ul_modules.8.in_proj_q.1.weight_v
 (16, 258, 1, 1)" fillcolor=lightblue]
	1401321794608 -> 1401571462544
	1401571462544 [label=AccumulateGrad]
	1401571461680 -> 1410956373936
	1401321793744 [label="ul_modules.8.in_proj_q.1.weight_g
 (16, 1, 1, 1)" fillcolor=lightblue]
	1401321793744 -> 1401571461680
	1401571461680 [label=AccumulateGrad]
	1401571460672 -> 1410956373888
	1401321794416 [label="ul_modules.8.in_proj_q.1.bias
 (16)" fillcolor=lightblue]
	1401321794416 -> 1401571460672
	1401571460672 [label=AccumulateGrad]
	1410956373216 -> 1410956373168
	1410956373216 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 1, 16, 784)"]
	1410956373552 -> 1410956373216
	1410956373552 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 16, 784)"]
	1410956373744 -> 1410956373552
	1410956373744 [label="ViewBackward0
----------------------------------
self_sym_sizes: (1, 1, 16, 28, 28)"]
	1410956373312 -> 1410956373744
	1410956373312 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 16, 28, 28)"]
	1410956374272 -> 1410956373312
	1410956374272 [label="SplitWithSizesBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 144, 28, 28)
split_sizes   :        (16, 128)"]
	1410956374320 -> 1410956374272
	1410956374320 -> 1410754007632 [dir=none]
	1410754007632 [label="input
 (1, 514, 28, 28)" fillcolor=orange]
	1410956374320 -> 1410754007344 [dir=none]
	1410754007344 [label="weight
 (144, 514, 1, 1)" fillcolor=orange]
	1410956374320 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (144,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956374704 -> 1410956374320
	1410956374704 [label="AddBackward0
------------
alpha: 1"]
	1410956375088 -> 1410956374704
	1410956375088 [label="CatBackward0
------------
dim: 1"]
	1410753893552 -> 1410956375088
	1410753892832 -> 1410956375088
	1410956375184 -> 1410956374704
	1410956375184 -> 1410754007440 [dir=none]
	1410754007440 [label="other
 (1, 514, 28, 28)" fillcolor=orange]
	1410956375184 -> 1410754007248 [dir=none]
	1410754007248 [label="self
 (1, 514, 28, 28)" fillcolor=orange]
	1410956375184 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410956374896 -> 1410956375184
	1410956374896 [label="SplitBackward0
---------------------------------
dim           :                 1
self_sym_sizes: (1, 1028, 28, 28)
split_size    :               514"]
	1410956375424 -> 1410956374896
	1410956375424 -> 1410754006864 [dir=none]
	1410754006864 [label="input
 (1, 1028, 28, 28)" fillcolor=orange]
	1410956375424 -> 1410754006960 [dir=none]
	1410754006960 [label="weight
 (1028, 1028, 1, 1)" fillcolor=orange]
	1410956375424 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1028,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956375520 -> 1410956375424
	1410956375520 -> 1410956577584 [dir=none]
	1410956577584 [label="result1
 (1, 1028, 28, 28)" fillcolor=orange]
	1410956375520 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956375664 -> 1410956375520
	1410956375664 -> 1410754007056 [dir=none]
	1410754007056 [label="self
 (1, 1028, 28, 28)" fillcolor=orange]
	1410956375664 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956375760 -> 1410956375664
	1410956375760 [label="CatBackward0
------------
dim: 1"]
	1410956375856 -> 1410956375760
	1410956375856 -> 1410754006480 [dir=none]
	1410754006480 [label="input
 (1, 1028, 28, 28)" fillcolor=orange]
	1410956375856 -> 1410754006672 [dir=none]
	1410754006672 [label="weight
 (514, 1028, 1, 1)" fillcolor=orange]
	1410956375856 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (514,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956376000 -> 1410956375856
	1410956376000 -> 1410754006000 [dir=none]
	1410754006000 [label="self
 (1, 1028, 28, 28)" fillcolor=orange]
	1410956376000 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956376144 -> 1410956376000
	1410956376144 [label="CatBackward0
------------
dim: 1"]
	1410956375088 -> 1410956376144
	1410956376240 -> 1410956376144
	1410956376240 [label=NegBackward0]
	1410956375088 -> 1410956376240
	1410956375952 -> 1410956375856
	1410956375952 -> 1401321791440 [dir=none]
	1401321791440 [label="g
 (514, 1, 1, 1)" fillcolor=orange]
	1410956375952 -> 1410956579312 [dir=none]
	1410956579312 [label="result1
 (514, 1, 1, 1)" fillcolor=orange]
	1410956375952 -> 1401321792304 [dir=none]
	1401321792304 [label="v
 (514, 1028, 1, 1)" fillcolor=orange]
	1410956375952 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571464272 -> 1410956375952
	1401321792304 [label="ul_modules.8.in_proj_kv.0.conv_1.weight_v
 (514, 1028, 1, 1)" fillcolor=lightblue]
	1401321792304 -> 1401571464272
	1401571464272 [label=AccumulateGrad]
	1401571464128 -> 1410956375952
	1401321791440 [label="ul_modules.8.in_proj_kv.0.conv_1.weight_g
 (514, 1, 1, 1)" fillcolor=lightblue]
	1401321791440 -> 1401571464128
	1401571464128 [label=AccumulateGrad]
	1401571463792 -> 1410956375856
	1401321792112 [label="ul_modules.8.in_proj_kv.0.conv_1.bias
 (514)" fillcolor=lightblue]
	1401321792112 -> 1401571463792
	1401571463792 [label=AccumulateGrad]
	1410956375808 -> 1410956375760
	1410956375808 [label=NegBackward0]
	1410956375856 -> 1410956375808
	1410956375472 -> 1410956375424
	1410956375472 -> 1401321792016 [dir=none]
	1401321792016 [label="g
 (1028, 1, 1, 1)" fillcolor=orange]
	1410956375472 -> 1410956580944 [dir=none]
	1410956580944 [label="result1
 (1028, 1, 1, 1)" fillcolor=orange]
	1410956375472 -> 1401321792688 [dir=none]
	1401321792688 [label="v
 (1028, 1028, 1, 1)" fillcolor=orange]
	1410956375472 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571463456 -> 1410956375472
	1401321792688 [label="ul_modules.8.in_proj_kv.0.conv_2.weight_v
 (1028, 1028, 1, 1)" fillcolor=lightblue]
	1401321792688 -> 1401571463456
	1401571463456 [label=AccumulateGrad]
	1401571463600 -> 1410956375472
	1401321792016 [label="ul_modules.8.in_proj_kv.0.conv_2.weight_g
 (1028, 1, 1, 1)" fillcolor=lightblue]
	1401321792016 -> 1401571463600
	1401571463600 [label=AccumulateGrad]
	1401571463168 -> 1410956375424
	1401321792496 [label="ul_modules.8.in_proj_kv.0.conv_2.bias
 (1028)" fillcolor=lightblue]
	1401321792496 -> 1401571463168
	1401571463168 [label=AccumulateGrad]
	1410956375328 -> 1410956375184
	1410956375328 -> 1410956582096 [dir=none]
	1410956582096 [label="result
 (1, 514, 28, 28)" fillcolor=orange]
	1410956375328 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410956374896 -> 1410956375328
	1410956374368 -> 1410956374320
	1410956374368 -> 1401321792400 [dir=none]
	1401321792400 [label="g
 (144, 1, 1, 1)" fillcolor=orange]
	1410956374368 -> 1410956582672 [dir=none]
	1410956582672 [label="result1
 (144, 1, 1, 1)" fillcolor=orange]
	1410956374368 -> 1401321793264 [dir=none]
	1401321793264 [label="v
 (144, 514, 1, 1)" fillcolor=orange]
	1410956374368 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571463984 -> 1410956374368
	1401321793264 [label="ul_modules.8.in_proj_kv.1.weight_v
 (144, 514, 1, 1)" fillcolor=lightblue]
	1401321793264 -> 1401571463984
	1401571463984 [label=AccumulateGrad]
	1401571463120 -> 1410956374368
	1401321792400 [label="ul_modules.8.in_proj_kv.1.weight_g
 (144, 1, 1, 1)" fillcolor=lightblue]
	1401321792400 -> 1401571463120
	1401571463120 [label=AccumulateGrad]
	1401571460720 -> 1410956374320
	1401321793072 [label="ul_modules.8.in_proj_kv.1.bias
 (144)" fillcolor=lightblue]
	1401321793072 -> 1401571460720
	1401571460720 [label=AccumulateGrad]
	1410956372592 -> 1410956372544
	1410956372592 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 128)"]
	1410956372928 -> 1410956372592
	1410956372928 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 128)"]
	1410956373120 -> 1410956372928
	1410956373120 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410956373456 -> 1410956373120
	1410956373456 [label="ViewBackward0
-----------------------------------
self_sym_sizes: (1, 1, 128, 28, 28)"]
	1410956373840 -> 1410956373456
	1410956373840 [label="ViewBackward0
--------------------------------
self_sym_sizes: (1, 128, 28, 28)"]
	1410956374272 -> 1410956373840
	1410956372160 -> 1410956371824
	1410956372160 [label=NegBackward0]
	1410956372208 -> 1410956372160
	1410956371872 -> 1410956371584
	1410956371872 -> 1401321794704 [dir=none]
	1401321794704 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410956371872 -> 1410956584880 [dir=none]
	1410956584880 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410956371872 -> 1401321795376 [dir=none]
	1401321795376 [label="v
 (256, 256, 1, 1)" fillcolor=orange]
	1410956371872 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571459856 -> 1410956371872
	1401321795376 [label="ul_modules.8.out_proj.conv_1_sc.weight_v
 (256, 256, 1, 1)" fillcolor=lightblue]
	1401321795376 -> 1401571459856
	1401571459856 [label=AccumulateGrad]
	1401571459376 -> 1410956371872
	1401321794704 [label="ul_modules.8.out_proj.conv_1_sc.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321794704 -> 1401571459376
	1401571459376 [label=AccumulateGrad]
	1401571459136 -> 1410956371584
	1401321795184 [label="ul_modules.8.out_proj.conv_1_sc.bias
 (256)" fillcolor=lightblue]
	1401321795184 -> 1401571459136
	1401571459136 [label=AccumulateGrad]
	1410956371440 -> 1410956371488
	1410956371440 [label=NegBackward0]
	1410956371104 -> 1410956371440
	1410956370768 -> 1410956370672
	1410956370768 -> 1401321795088 [dir=none]
	1401321795088 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410956370768 -> 1410956586512 [dir=none]
	1410956586512 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410956370768 -> 1401321795760 [dir=none]
	1401321795760 [label="v
 (512, 512, 1, 1)" fillcolor=orange]
	1410956370768 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571228640 -> 1410956370768
	1401321795760 [label="ul_modules.8.out_proj.conv_2.weight_v
 (512, 512, 1, 1)" fillcolor=lightblue]
	1401321795760 -> 1401571228640
	1401571228640 [label=AccumulateGrad]
	1401571229072 -> 1410956370768
	1401321795088 [label="ul_modules.8.out_proj.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321795088 -> 1401571229072
	1401571229072 [label=AccumulateGrad]
	1401571227584 -> 1410956370672
	1401321795568 [label="ul_modules.8.out_proj.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321795568 -> 1401571227584
	1401571227584 [label=AccumulateGrad]
	1410956370480 -> 1410753892784
	1410956370480 -> 1410956587664 [dir=none]
	1410956587664 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410956370480 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753892880 -> 1410956370480
	1410753892640 -> 1410753892544
	1410753892640 -> 1410754010800 [dir=none]
	1410754010800 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753892640 -> 1410754005904 [dir=none]
	1410754005904 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753892640 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753892736 -> 1410753892640
	1410753892736 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410956371344 -> 1410753892736
	1410956371344 -> 1410754008880 [dir=none]
	1410754008880 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956371344 -> 1410754003120 [dir=none]
	1410754003120 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410956371344 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956372064 -> 1410956371344
	1410956372064 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956372112 -> 1410956372064
	1410956372112 -> 1410956588720 [dir=none]
	1410956588720 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410956372112 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956372400 -> 1410956372112
	1410956372400 -> 1410754009168 [dir=none]
	1410754009168 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956372400 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956372256 -> 1410956372400
	1410956372256 [label="CatBackward0
------------
dim: 1"]
	1410956373024 -> 1410956372256
	1410956373024 -> 1410754009456 [dir=none]
	1410754009456 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956373024 -> 1410754009648 [dir=none]
	1410754009648 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410956373024 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956374608 -> 1410956373024
	1410956374608 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956373360 -> 1410956374608
	1410956373360 -> 1410754009744 [dir=none]
	1410754009744 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956373360 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956375280 -> 1410956373360
	1410956375280 [label="CatBackward0
------------
dim: 1"]
	1410753892688 -> 1410956375280
	1410956375040 -> 1410956375280
	1410956375040 [label=NegBackward0]
	1410753892688 -> 1410956375040
	1410956373648 -> 1410956373024
	1410956373648 -> 1401321795472 [dir=none]
	1401321795472 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410956373648 -> 1410956590640 [dir=none]
	1410956590640 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410956373648 -> 1401321796336 [dir=none]
	1401321796336 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410956373648 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571464368 -> 1410956373648
	1401321796336 [label="ul_modules.9.input_gated_resnet.0.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321796336 -> 1401571464368
	1401571464368 [label=AccumulateGrad]
	1401571464032 -> 1410956373648
	1401321795472 [label="ul_modules.9.input_gated_resnet.0.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321795472 -> 1401571464032
	1401571464032 [label=AccumulateGrad]
	1401571462592 -> 1410956373024
	1401321796144 [label="ul_modules.9.input_gated_resnet.0.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321796144 -> 1401571462592
	1401571462592 [label=AccumulateGrad]
	1410956372832 -> 1410956372256
	1410956372832 [label=NegBackward0]
	1410956373024 -> 1410956372832
	1410956371248 -> 1410956371344
	1410956371248 -> 1401321796048 [dir=none]
	1401321796048 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410956371248 -> 1410956592336 [dir=none]
	1410956592336 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410956371248 -> 1401321796720 [dir=none]
	1401321796720 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410956371248 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571461008 -> 1410956371248
	1401321796720 [label="ul_modules.9.input_gated_resnet.0.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321796720 -> 1401571461008
	1401571461008 [label=AccumulateGrad]
	1401571460384 -> 1410956371248
	1401321796048 [label="ul_modules.9.input_gated_resnet.0.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321796048 -> 1401571460384
	1401571460384 [label=AccumulateGrad]
	1401571459664 -> 1410956371344
	1401321796528 [label="ul_modules.9.input_gated_resnet.0.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321796528 -> 1401571459664
	1401571459664 [label=AccumulateGrad]
	1410956371152 -> 1410753892640
	1410956371152 -> 1410956593488 [dir=none]
	1410956593488 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410956371152 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753892736 -> 1410956371152
	1410753892496 -> 1410753892400
	1410753892496 -> 1410754011952 [dir=none]
	1410754011952 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753892496 -> 1410754011760 [dir=none]
	1410754011760 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753892496 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753892592 -> 1410753892496
	1410753892592 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410956372496 -> 1410753892592
	1410956372496 -> 1410754011568 [dir=none]
	1410754011568 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956372496 -> 1410754006768 [dir=none]
	1410754006768 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410956372496 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956371680 -> 1410956372496
	1410956371680 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956372688 -> 1410956371680
	1410956372688 -> 1410956594544 [dir=none]
	1410956594544 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410956372688 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956375712 -> 1410956372688
	1410956375712 -> 1410754011472 [dir=none]
	1410754011472 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956375712 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956374032 -> 1410956375712
	1410956374032 [label="CatBackward0
------------
dim: 1"]
	1410956376192 -> 1410956374032
	1410956376192 -> 1410754011280 [dir=none]
	1410754011280 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956376192 -> 1410754010992 [dir=none]
	1410754010992 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410956376192 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956376336 -> 1410956376192
	1410956376336 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956376384 -> 1410956376336
	1410956376384 -> 1410754009264 [dir=none]
	1410754009264 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956376384 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956376480 -> 1410956376384
	1410956376480 [label="CatBackward0
------------
dim: 1"]
	1410753892544 -> 1410956376480
	1410956376576 -> 1410956376480
	1410956376576 [label=NegBackward0]
	1410753892544 -> 1410956376576
	1410956375904 -> 1410956376192
	1410956375904 -> 1401321796432 [dir=none]
	1401321796432 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410956375904 -> 1410956596464 [dir=none]
	1410956596464 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410956375904 -> 1401321797296 [dir=none]
	1401321797296 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410956375904 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571465328 -> 1410956375904
	1401321797296 [label="ul_modules.9.input_gated_resnet.1.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321797296 -> 1401571465328
	1401571465328 [label=AccumulateGrad]
	1401571465232 -> 1410956375904
	1401321796432 [label="ul_modules.9.input_gated_resnet.1.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321796432 -> 1401571465232
	1401571465232 [label=AccumulateGrad]
	1401571464896 -> 1410956376192
	1401321797104 [label="ul_modules.9.input_gated_resnet.1.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321797104 -> 1401571464896
	1401571464896 [label=AccumulateGrad]
	1410956375568 -> 1410956374032
	1410956375568 [label=NegBackward0]
	1410956376192 -> 1410956375568
	1410956372304 -> 1410956372496
	1410956372304 -> 1401321797008 [dir=none]
	1401321797008 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410956372304 -> 1410956598096 [dir=none]
	1410956598096 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410956372304 -> 1401321797680 [dir=none]
	1401321797680 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410956372304 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571464704 -> 1410956372304
	1401321797680 [label="ul_modules.9.input_gated_resnet.1.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321797680 -> 1401571464704
	1401571464704 [label=AccumulateGrad]
	1401571464608 -> 1410956372304
	1401321797008 [label="ul_modules.9.input_gated_resnet.1.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321797008 -> 1401571464608
	1401571464608 [label=AccumulateGrad]
	1401571464512 -> 1410956372496
	1401321797488 [label="ul_modules.9.input_gated_resnet.1.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321797488 -> 1401571464512
	1401571464512 [label=AccumulateGrad]
	1410956371536 -> 1410753892496
	1410956371536 -> 1410956599248 [dir=none]
	1410956599248 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410956371536 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753892592 -> 1410956371536
	1410753892352 -> 1410753892256
	1410753892352 -> 1410754012816 [dir=none]
	1410754012816 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753892352 -> 1410754012624 [dir=none]
	1410754012624 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753892352 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753892448 -> 1410753892352
	1410753892448 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410956375616 -> 1410753892448
	1410956375616 -> 1410754012432 [dir=none]
	1410754012432 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956375616 -> 1410754011184 [dir=none]
	1410754011184 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410956375616 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956374560 -> 1410956375616
	1410956374560 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956376096 -> 1410956374560
	1410956376096 -> 1410956600304 [dir=none]
	1410956600304 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410956376096 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956376672 -> 1410956376096
	1410956376672 -> 1410754012336 [dir=none]
	1410754012336 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956376672 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956376288 -> 1410956376672
	1410956376288 [label="CatBackward0
------------
dim: 1"]
	1410956376768 -> 1410956376288
	1410956376768 -> 1410754012048 [dir=none]
	1410754012048 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956376768 -> 1410754011376 [dir=none]
	1410754011376 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410956376768 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956376912 -> 1410956376768
	1410956376912 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956377056 -> 1410956376912
	1410956377056 -> 1410754011856 [dir=none]
	1410754011856 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956377056 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956377152 -> 1410956377056
	1410956377152 [label="CatBackward0
------------
dim: 1"]
	1410753892400 -> 1410956377152
	1410956377248 -> 1410956377152
	1410956377248 [label=NegBackward0]
	1410753892400 -> 1410956377248
	1410956376864 -> 1410956376768
	1410956376864 -> 1401321797392 [dir=none]
	1401321797392 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410956376864 -> 1410956602224 [dir=none]
	1410956602224 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410956376864 -> 1401321798256 [dir=none]
	1401321798256 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410956376864 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571466288 -> 1410956376864
	1401321798256 [label="ul_modules.9.input_gated_resnet.2.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321798256 -> 1401571466288
	1401571466288 [label=AccumulateGrad]
	1401571466192 -> 1410956376864
	1401321797392 [label="ul_modules.9.input_gated_resnet.2.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321797392 -> 1401571466192
	1401571466192 [label=AccumulateGrad]
	1401571465856 -> 1410956376768
	1401321798064 [label="ul_modules.9.input_gated_resnet.2.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321798064 -> 1401571465856
	1401571465856 [label=AccumulateGrad]
	1410956376720 -> 1410956376288
	1410956376720 [label=NegBackward0]
	1410956376768 -> 1410956376720
	1410956375376 -> 1410956375616
	1410956375376 -> 1401321797968 [dir=none]
	1401321797968 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410956375376 -> 1410956603856 [dir=none]
	1410956603856 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410956375376 -> 1401321798640 [dir=none]
	1401321798640 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410956375376 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571465664 -> 1410956375376
	1401321798640 [label="ul_modules.9.input_gated_resnet.2.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321798640 -> 1401571465664
	1401571465664 [label=AccumulateGrad]
	1401571465568 -> 1410956375376
	1401321797968 [label="ul_modules.9.input_gated_resnet.2.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321797968 -> 1401571465568
	1401571465568 [label=AccumulateGrad]
	1401571462880 -> 1410956375616
	1401321798448 [label="ul_modules.9.input_gated_resnet.2.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321798448 -> 1401571462880
	1401571462880 [label=AccumulateGrad]
	1410956372736 -> 1410753892352
	1410956372736 -> 1410956605008 [dir=none]
	1410956605008 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410956372736 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753892448 -> 1410956372736
	1410753892208 -> 1410753891104
	1410753892208 -> 1410754013680 [dir=none]
	1410754013680 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753892208 -> 1410754013488 [dir=none]
	1410754013488 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753892208 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753892304 -> 1410753892208
	1410753892304 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410956376528 -> 1410753892304
	1410956376528 -> 1410754013296 [dir=none]
	1410754013296 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956376528 -> 1410754012144 [dir=none]
	1410754012144 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410956376528 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956376432 -> 1410956376528
	1410956376432 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956376816 -> 1410956376432
	1410956376816 -> 1410956606064 [dir=none]
	1410956606064 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410956376816 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956377344 -> 1410956376816
	1410956377344 -> 1410754013200 [dir=none]
	1410754013200 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956377344 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956376960 -> 1410956377344
	1410956376960 [label="CatBackward0
------------
dim: 1"]
	1410956377440 -> 1410956376960
	1410956377440 -> 1410754012912 [dir=none]
	1410754012912 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956377440 -> 1410754012240 [dir=none]
	1410754012240 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410956377440 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956377584 -> 1410956377440
	1410956377584 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956377728 -> 1410956377584
	1410956377728 -> 1410754012720 [dir=none]
	1410754012720 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956377728 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956377824 -> 1410956377728
	1410956377824 [label="CatBackward0
------------
dim: 1"]
	1410753892256 -> 1410956377824
	1410956377920 -> 1410956377824
	1410956377920 [label=NegBackward0]
	1410753892256 -> 1410956377920
	1410956377536 -> 1410956377440
	1410956377536 -> 1401321798352 [dir=none]
	1401321798352 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410956377536 -> 1410956657200 [dir=none]
	1410956657200 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410956377536 -> 1401321799216 [dir=none]
	1401321799216 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410956377536 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571467248 -> 1410956377536
	1401321799216 [label="ul_modules.9.input_gated_resnet.3.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401321799216 -> 1401571467248
	1401571467248 [label=AccumulateGrad]
	1401571467152 -> 1410956377536
	1401321798352 [label="ul_modules.9.input_gated_resnet.3.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321798352 -> 1401571467152
	1401571467152 [label=AccumulateGrad]
	1401571466816 -> 1410956377440
	1401321799024 [label="ul_modules.9.input_gated_resnet.3.conv_1.bias
 (256)" fillcolor=lightblue]
	1401321799024 -> 1401571466816
	1401571466816 [label=AccumulateGrad]
	1410956377392 -> 1410956376960
	1410956377392 [label=NegBackward0]
	1410956377440 -> 1410956377392
	1410956376624 -> 1410956376528
	1410956376624 -> 1401321798928 [dir=none]
	1401321798928 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410956376624 -> 1410956658832 [dir=none]
	1410956658832 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410956376624 -> 1401321799600 [dir=none]
	1401321799600 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410956376624 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571466624 -> 1410956376624
	1401321799600 [label="ul_modules.9.input_gated_resnet.3.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401321799600 -> 1401571466624
	1401571466624 [label=AccumulateGrad]
	1401571466528 -> 1410956376624
	1401321798928 [label="ul_modules.9.input_gated_resnet.3.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401321798928 -> 1401571466528
	1401571466528 [label=AccumulateGrad]
	1401571465040 -> 1410956376528
	1401321799408 [label="ul_modules.9.input_gated_resnet.3.conv_2.bias
 (512)" fillcolor=lightblue]
	1401321799408 -> 1401571465040
	1401571465040 [label=AccumulateGrad]
	1410956376048 -> 1410753892208
	1410956376048 -> 1410956659984 [dir=none]
	1410956659984 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410956376048 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753892304 -> 1410956376048
	1410753891056 -> 1410753895760
	1410753891056 -> 1410754014544 [dir=none]
	1410754014544 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753891056 -> 1410754014352 [dir=none]
	1410754014352 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753891056 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753891152 -> 1410753891056
	1410753891152 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410956377200 -> 1410753891152
	1410956377200 -> 1410754014160 [dir=none]
	1410754014160 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956377200 -> 1410754013008 [dir=none]
	1410754013008 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410956377200 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956377104 -> 1410956377200
	1410956377104 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956377488 -> 1410956377104
	1410956377488 -> 1410956661040 [dir=none]
	1410956661040 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410956377488 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956378016 -> 1410956377488
	1410956378016 -> 1410754014064 [dir=none]
	1410754014064 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956378016 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956377632 -> 1410956378016
	1410956377632 [label="CatBackward0
------------
dim: 1"]
	1410956378064 -> 1410956377632
	1410956378064 -> 1410754013776 [dir=none]
	1410754013776 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956378064 -> 1410754013104 [dir=none]
	1410754013104 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410956378064 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956640464 -> 1410956378064
	1410956640464 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956640608 -> 1410956640464
	1410956640608 -> 1410754013584 [dir=none]
	1410754013584 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956640608 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956640704 -> 1410956640608
	1410956640704 [label="CatBackward0
------------
dim: 1"]
	1410753891104 -> 1410956640704
	1410956640800 -> 1410956640704
	1410956640800 [label=NegBackward0]
	1410753891104 -> 1410956640800
	1410956640416 -> 1410956378064
	1410956640416 -> 1401321799312 [dir=none]
	1401321799312 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410956640416 -> 1410956662960 [dir=none]
	1410956662960 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410956640416 -> 1401322029616 [dir=none]
	1401322029616 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410956640416 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571468208 -> 1410956640416
	1401322029616 [label="ul_modules.9.input_gated_resnet.4.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401322029616 -> 1401571468208
	1401571468208 [label=AccumulateGrad]
	1401571468112 -> 1410956640416
	1401321799312 [label="ul_modules.9.input_gated_resnet.4.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401321799312 -> 1401571468112
	1401571468112 [label=AccumulateGrad]
	1401571467776 -> 1410956378064
	1401322029424 [label="ul_modules.9.input_gated_resnet.4.conv_1.bias
 (256)" fillcolor=lightblue]
	1401322029424 -> 1401571467776
	1401571467776 [label=AccumulateGrad]
	1410956377776 -> 1410956377632
	1410956377776 [label=NegBackward0]
	1410956378064 -> 1410956377776
	1410956377296 -> 1410956377200
	1410956377296 -> 1401322029328 [dir=none]
	1401322029328 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410956377296 -> 1410956664592 [dir=none]
	1410956664592 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410956377296 -> 1401322030000 [dir=none]
	1401322030000 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410956377296 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571467584 -> 1410956377296
	1401322030000 [label="ul_modules.9.input_gated_resnet.4.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401322030000 -> 1401571467584
	1401571467584 [label=AccumulateGrad]
	1401571467488 -> 1410956377296
	1401322029328 [label="ul_modules.9.input_gated_resnet.4.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401322029328 -> 1401571467488
	1401571467488 [label=AccumulateGrad]
	1401571466000 -> 1410956377200
	1401322029808 [label="ul_modules.9.input_gated_resnet.4.conv_2.bias
 (512)" fillcolor=lightblue]
	1401322029808 -> 1401571466000
	1401571466000 [label=AccumulateGrad]
	1410956377008 -> 1410753891056
	1410956377008 -> 1410956665744 [dir=none]
	1410956665744 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410956377008 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753891152 -> 1410956377008
	1410753888896 -> 1410753889568
	1410753888896 -> 1410754018864 [dir=none]
	1410754018864 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753888896 -> 1410754018672 [dir=none]
	1410754018672 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753888896 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753891008 -> 1410753888896
	1410753891008 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410956377872 -> 1410753891008
	1410956377872 -> 1410754018480 [dir=none]
	1410754018480 [label="input
 (1, 512, 28, 28)" fillcolor=orange]
	1410956377872 -> 1410754018000 [dir=none]
	1410754018000 [label="weight
 (512, 512, 1, 1)" fillcolor=orange]
	1410956377872 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956377968 -> 1410956377872
	1410956377968 -> 1410956666800 [dir=none]
	1410956666800 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410956377968 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956640368 -> 1410956377968
	1410956640368 -> 1410754018384 [dir=none]
	1410754018384 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956640368 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956640896 -> 1410956640368
	1410956640896 [label="CatBackward0
------------
dim: 1"]
	1410956640512 -> 1410956640896
	1410956640512 [label="AddBackward0
------------
alpha: 1"]
	1410956641040 -> 1410956640512
	1410956641040 -> 1410754017520 [dir=none]
	1410754017520 [label="input
 (1, 512, 28, 28)" fillcolor=orange]
	1410956641040 -> 1410754017904 [dir=none]
	1410754017904 [label="weight
 (256, 512, 1, 1)" fillcolor=orange]
	1410956641040 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956641184 -> 1410956641040
	1410956641184 -> 1410754017808 [dir=none]
	1410754017808 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956641184 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956641328 -> 1410956641184
	1410956641328 [label="CatBackward0
------------
dim: 1"]
	1410753895760 -> 1410956641328
	1410956641424 -> 1410956641328
	1410956641424 [label=NegBackward0]
	1410753895760 -> 1410956641424
	1410956641136 -> 1410956641040
	1410956641136 -> 1401322032592 [dir=none]
	1401322032592 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410956641136 -> 1410956668624 [dir=none]
	1410956668624 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410956641136 -> 1401322033264 [dir=none]
	1401322033264 [label="v
 (256, 512, 1, 1)" fillcolor=orange]
	1410956641136 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571469264 -> 1410956641136
	1401322033264 [label="ul_modules.9.out_proj.conv_1.weight_v
 (256, 512, 1, 1)" fillcolor=lightblue]
	1401322033264 -> 1401571469264
	1401571469264 [label=AccumulateGrad]
	1401571469120 -> 1410956641136
	1401322032592 [label="ul_modules.9.out_proj.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401322032592 -> 1401571469120
	1401571469120 [label=AccumulateGrad]
	1401571468784 -> 1410956641040
	1401322033072 [label="ul_modules.9.out_proj.conv_1.bias
 (256)" fillcolor=lightblue]
	1401322033072 -> 1401571468784
	1401571468784 [label=AccumulateGrad]
	1410956640992 -> 1410956640512
	1410956640992 -> 1410754018096 [dir=none]
	1410754018096 [label="input
 (1, 256, 28, 28)" fillcolor=orange]
	1410956640992 -> 1410754018288 [dir=none]
	1410754018288 [label="weight
 (256, 256, 1, 1)" fillcolor=orange]
	1410956640992 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956641376 -> 1410956640992
	1410956641376 -> 1410754018192 [dir=none]
	1410754018192 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410956641376 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956641232 -> 1410956641376
	1410956641232 [label="CatBackward0
------------
dim: 1"]
	1410956641616 -> 1410956641232
	1410956641616 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (1, 1, 128, 784)"]
	1410956641760 -> 1410956641616
	1410956641760 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410956641856 -> 1410956641760
	1410956641856 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (1, 784, 128)"]
	1410956641952 -> 1410956641856
	1410956641952 -> 1410956670640 [dir=none]
	1410956670640 [label="mat2
 (1, 784, 128)" fillcolor=orange]
	1410956641952 -> 1410956670928 [dir=none]
	1410956670928 [label="self
 (1, 784, 784)" fillcolor=orange]
	1410956641952 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	1410956642048 -> 1410956641952
	1410956642048 [label="ViewBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 784)"]
	1410956642192 -> 1410956642048
	1410956642192 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 784)"]
	1410956642288 -> 1410956642192
	1410956642288 -> 1410956671216 [dir=none]
	1410956671216 [label="result
 (1, 1, 784, 784)" fillcolor=orange]
	1410956642288 [label="SoftmaxBackward0
----------------------
dim   :     4294967295
result: [saved tensor]"]
	1410956642384 -> 1410956642288
	1410956642384 -> 1410754017040 [dir=none]
	1410754017040 [label="mask
 (1, 1, 784, 784)" fillcolor=orange]
	1410956642384 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	1410956642480 -> 1410956642384
	1410956642480 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (1, 784, 784)"]
	1410956642576 -> 1410956642480
	1410956642576 -> 1410956671888 [dir=none]
	1410956671888 [label="mat2
 (1, 16, 784)" fillcolor=orange]
	1410956642576 -> 1410956672176 [dir=none]
	1410956672176 [label="self
 (1, 784, 16)" fillcolor=orange]
	1410956642576 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	1410956642672 -> 1410956642576
	1410956642672 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (1, 1, 784, 16)"]
	1410956642816 -> 1410956642672
	1410956642816 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 784, 16)"]
	1410956642912 -> 1410956642816
	1410956642912 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410956643008 -> 1410956642912
	1410956643008 -> 1410956672560 [dir=none]
	1410956672560 [label="other
 ()" fillcolor=orange]
	1410956643008 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	1410956643104 -> 1410956643008
	1410956643104 [label="ViewBackward0
----------------------------------
self_sym_sizes: (1, 1, 16, 28, 28)"]
	1410956643200 -> 1410956643104
	1410956643200 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 16, 28, 28)"]
	1410956643296 -> 1410956643200
	1410956643296 -> 1410754016944 [dir=none]
	1410754016944 [label="input
 (1, 258, 28, 28)" fillcolor=orange]
	1410956643296 -> 1410754016656 [dir=none]
	1410754016656 [label="weight
 (16, 258, 1, 1)" fillcolor=orange]
	1410956643296 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956643392 -> 1410956643296
	1410956643392 [label="AddBackward0
------------
alpha: 1"]
	1410956643536 -> 1410956643392
	1410956643536 [label="CatBackward0
------------
dim: 1"]
	1410753895760 -> 1410956643536
	1410956643488 -> 1410956643392
	1410956643488 -> 1410754016752 [dir=none]
	1410754016752 [label="other
 (1, 258, 28, 28)" fillcolor=orange]
	1410956643488 -> 1410754016560 [dir=none]
	1410754016560 [label="self
 (1, 258, 28, 28)" fillcolor=orange]
	1410956643488 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410956643584 -> 1410956643488
	1410956643584 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 516, 28, 28)
split_size    :              258"]
	1410956643824 -> 1410956643584
	1410956643824 -> 1410754016176 [dir=none]
	1410754016176 [label="input
 (1, 516, 28, 28)" fillcolor=orange]
	1410956643824 -> 1410754016272 [dir=none]
	1410754016272 [label="weight
 (516, 516, 1, 1)" fillcolor=orange]
	1410956643824 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (516,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956643920 -> 1410956643824
	1410956643920 -> 1410956707216 [dir=none]
	1410956707216 [label="result1
 (1, 516, 28, 28)" fillcolor=orange]
	1410956643920 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956644064 -> 1410956643920
	1410956644064 -> 1410754016368 [dir=none]
	1410754016368 [label="self
 (1, 516, 28, 28)" fillcolor=orange]
	1410956644064 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956644160 -> 1410956644064
	1410956644160 [label="CatBackward0
------------
dim: 1"]
	1410956644256 -> 1410956644160
	1410956644256 -> 1410754015888 [dir=none]
	1410754015888 [label="input
 (1, 516, 28, 28)" fillcolor=orange]
	1410956644256 -> 1410754016080 [dir=none]
	1410754016080 [label="weight
 (258, 516, 1, 1)" fillcolor=orange]
	1410956644256 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (258,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956644400 -> 1410956644256
	1410956644400 -> 1410754015984 [dir=none]
	1410754015984 [label="self
 (1, 516, 28, 28)" fillcolor=orange]
	1410956644400 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956644544 -> 1410956644400
	1410956644544 [label="CatBackward0
------------
dim: 1"]
	1410956643536 -> 1410956644544
	1410956644640 -> 1410956644544
	1410956644640 [label=NegBackward0]
	1410956643536 -> 1410956644640
	1410956644352 -> 1410956644256
	1410956644352 -> 1401322031248 [dir=none]
	1401322031248 [label="g
 (258, 1, 1, 1)" fillcolor=orange]
	1410956644352 -> 1410956708944 [dir=none]
	1410956708944 [label="result1
 (258, 1, 1, 1)" fillcolor=orange]
	1410956644352 -> 1401322031920 [dir=none]
	1401322031920 [label="v
 (258, 516, 1, 1)" fillcolor=orange]
	1410956644352 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571472768 -> 1410956644352
	1401322031920 [label="ul_modules.9.in_proj_q.0.conv_1.weight_v
 (258, 516, 1, 1)" fillcolor=lightblue]
	1401322031920 -> 1401571472768
	1401571472768 [label=AccumulateGrad]
	1401571472624 -> 1410956644352
	1401322031248 [label="ul_modules.9.in_proj_q.0.conv_1.weight_g
 (258, 1, 1, 1)" fillcolor=lightblue]
	1401322031248 -> 1401571472624
	1401571472624 [label=AccumulateGrad]
	1401571472288 -> 1410956644256
	1401322031728 [label="ul_modules.9.in_proj_q.0.conv_1.bias
 (258)" fillcolor=lightblue]
	1401322031728 -> 1401571472288
	1401571472288 [label=AccumulateGrad]
	1410956644208 -> 1410956644160
	1410956644208 [label=NegBackward0]
	1410956644256 -> 1410956644208
	1410956643872 -> 1410956643824
	1410956643872 -> 1401322031632 [dir=none]
	1401322031632 [label="g
 (516, 1, 1, 1)" fillcolor=orange]
	1410956643872 -> 1410956710576 [dir=none]
	1410956710576 [label="result1
 (516, 1, 1, 1)" fillcolor=orange]
	1410956643872 -> 1401322032304 [dir=none]
	1401322032304 [label="v
 (516, 516, 1, 1)" fillcolor=orange]
	1410956643872 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571471952 -> 1410956643872
	1401322032304 [label="ul_modules.9.in_proj_q.0.conv_2.weight_v
 (516, 516, 1, 1)" fillcolor=lightblue]
	1401322032304 -> 1401571471952
	1401571471952 [label=AccumulateGrad]
	1401571472096 -> 1410956643872
	1401322031632 [label="ul_modules.9.in_proj_q.0.conv_2.weight_g
 (516, 1, 1, 1)" fillcolor=lightblue]
	1401322031632 -> 1401571472096
	1401571472096 [label=AccumulateGrad]
	1401571471664 -> 1410956643824
	1401322032112 [label="ul_modules.9.in_proj_q.0.conv_2.bias
 (516)" fillcolor=lightblue]
	1401322032112 -> 1401571471664
	1401571471664 [label=AccumulateGrad]
	1410956643632 -> 1410956643488
	1410956643632 -> 1410956711728 [dir=none]
	1410956711728 [label="result
 (1, 258, 28, 28)" fillcolor=orange]
	1410956643632 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410956643584 -> 1410956643632
	1410956643344 -> 1410956643296
	1410956643344 -> 1401322032016 [dir=none]
	1401322032016 [label="g
 (16, 1, 1, 1)" fillcolor=orange]
	1410956643344 -> 1410956712304 [dir=none]
	1410956712304 [label="result1
 (16, 1, 1, 1)" fillcolor=orange]
	1410956643344 -> 1401322032880 [dir=none]
	1401322032880 [label="v
 (16, 258, 1, 1)" fillcolor=orange]
	1410956643344 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571472480 -> 1410956643344
	1401322032880 [label="ul_modules.9.in_proj_q.1.weight_v
 (16, 258, 1, 1)" fillcolor=lightblue]
	1401322032880 -> 1401571472480
	1401571472480 [label=AccumulateGrad]
	1401571471616 -> 1410956643344
	1401322032016 [label="ul_modules.9.in_proj_q.1.weight_g
 (16, 1, 1, 1)" fillcolor=lightblue]
	1401322032016 -> 1401571471616
	1401571471616 [label=AccumulateGrad]
	1401571470608 -> 1410956643296
	1401322032688 [label="ul_modules.9.in_proj_q.1.bias
 (16)" fillcolor=lightblue]
	1401322032688 -> 1401571470608
	1401571470608 [label=AccumulateGrad]
	1410956642624 -> 1410956642576
	1410956642624 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 1, 16, 784)"]
	1410956642960 -> 1410956642624
	1410956642960 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 16, 784)"]
	1410956643152 -> 1410956642960
	1410956643152 [label="ViewBackward0
----------------------------------
self_sym_sizes: (1, 1, 16, 28, 28)"]
	1410956642720 -> 1410956643152
	1410956642720 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 16, 28, 28)"]
	1410956643680 -> 1410956642720
	1410956643680 [label="SplitWithSizesBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 144, 28, 28)
split_sizes   :        (16, 128)"]
	1410956643728 -> 1410956643680
	1410956643728 -> 1410754015600 [dir=none]
	1410754015600 [label="input
 (1, 514, 28, 28)" fillcolor=orange]
	1410956643728 -> 1410754015312 [dir=none]
	1410754015312 [label="weight
 (144, 514, 1, 1)" fillcolor=orange]
	1410956643728 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (144,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956644112 -> 1410956643728
	1410956644112 [label="AddBackward0
------------
alpha: 1"]
	1410956644496 -> 1410956644112
	1410956644496 [label="CatBackward0
------------
dim: 1"]
	1410753892688 -> 1410956644496
	1410753895760 -> 1410956644496
	1410956644592 -> 1410956644112
	1410956644592 -> 1410754015408 [dir=none]
	1410754015408 [label="other
 (1, 514, 28, 28)" fillcolor=orange]
	1410956644592 -> 1410754015216 [dir=none]
	1410754015216 [label="self
 (1, 514, 28, 28)" fillcolor=orange]
	1410956644592 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410956644304 -> 1410956644592
	1410956644304 [label="SplitBackward0
---------------------------------
dim           :                 1
self_sym_sizes: (1, 1028, 28, 28)
split_size    :               514"]
	1410956644832 -> 1410956644304
	1410956644832 -> 1410754014832 [dir=none]
	1410754014832 [label="input
 (1, 1028, 28, 28)" fillcolor=orange]
	1410956644832 -> 1410754014928 [dir=none]
	1410754014928 [label="weight
 (1028, 1028, 1, 1)" fillcolor=orange]
	1410956644832 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1028,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956644928 -> 1410956644832
	1410956644928 -> 1410956715280 [dir=none]
	1410956715280 [label="result1
 (1, 1028, 28, 28)" fillcolor=orange]
	1410956644928 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956645072 -> 1410956644928
	1410956645072 -> 1410754015024 [dir=none]
	1410754015024 [label="self
 (1, 1028, 28, 28)" fillcolor=orange]
	1410956645072 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956645168 -> 1410956645072
	1410956645168 [label="CatBackward0
------------
dim: 1"]
	1410956645264 -> 1410956645168
	1410956645264 -> 1410754014448 [dir=none]
	1410754014448 [label="input
 (1, 1028, 28, 28)" fillcolor=orange]
	1410956645264 -> 1410754014640 [dir=none]
	1410754014640 [label="weight
 (514, 1028, 1, 1)" fillcolor=orange]
	1410956645264 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (514,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956645408 -> 1410956645264
	1410956645408 -> 1410754013968 [dir=none]
	1410754013968 [label="self
 (1, 1028, 28, 28)" fillcolor=orange]
	1410956645408 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956645552 -> 1410956645408
	1410956645552 [label="CatBackward0
------------
dim: 1"]
	1410956644496 -> 1410956645552
	1410956645648 -> 1410956645552
	1410956645648 [label=NegBackward0]
	1410956644496 -> 1410956645648
	1410956645360 -> 1410956645264
	1410956645360 -> 1401322029712 [dir=none]
	1401322029712 [label="g
 (514, 1, 1, 1)" fillcolor=orange]
	1410956645360 -> 1410956717008 [dir=none]
	1410956717008 [label="result1
 (514, 1, 1, 1)" fillcolor=orange]
	1410956645360 -> 1401322030576 [dir=none]
	1401322030576 [label="v
 (514, 1028, 1, 1)" fillcolor=orange]
	1410956645360 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571474208 -> 1410956645360
	1401322030576 [label="ul_modules.9.in_proj_kv.0.conv_1.weight_v
 (514, 1028, 1, 1)" fillcolor=lightblue]
	1401322030576 -> 1401571474208
	1401571474208 [label=AccumulateGrad]
	1401571474064 -> 1410956645360
	1401322029712 [label="ul_modules.9.in_proj_kv.0.conv_1.weight_g
 (514, 1, 1, 1)" fillcolor=lightblue]
	1401322029712 -> 1401571474064
	1401571474064 [label=AccumulateGrad]
	1401571473728 -> 1410956645264
	1401322030384 [label="ul_modules.9.in_proj_kv.0.conv_1.bias
 (514)" fillcolor=lightblue]
	1401322030384 -> 1401571473728
	1401571473728 [label=AccumulateGrad]
	1410956645216 -> 1410956645168
	1410956645216 [label=NegBackward0]
	1410956645264 -> 1410956645216
	1410956644880 -> 1410956644832
	1410956644880 -> 1401322030288 [dir=none]
	1401322030288 [label="g
 (1028, 1, 1, 1)" fillcolor=orange]
	1410956644880 -> 1410956718640 [dir=none]
	1410956718640 [label="result1
 (1028, 1, 1, 1)" fillcolor=orange]
	1410956644880 -> 1401322030960 [dir=none]
	1401322030960 [label="v
 (1028, 1028, 1, 1)" fillcolor=orange]
	1410956644880 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571473392 -> 1410956644880
	1401322030960 [label="ul_modules.9.in_proj_kv.0.conv_2.weight_v
 (1028, 1028, 1, 1)" fillcolor=lightblue]
	1401322030960 -> 1401571473392
	1401571473392 [label=AccumulateGrad]
	1401571473536 -> 1410956644880
	1401322030288 [label="ul_modules.9.in_proj_kv.0.conv_2.weight_g
 (1028, 1, 1, 1)" fillcolor=lightblue]
	1401322030288 -> 1401571473536
	1401571473536 [label=AccumulateGrad]
	1401571473104 -> 1410956644832
	1401322030768 [label="ul_modules.9.in_proj_kv.0.conv_2.bias
 (1028)" fillcolor=lightblue]
	1401322030768 -> 1401571473104
	1401571473104 [label=AccumulateGrad]
	1410956644736 -> 1410956644592
	1410956644736 -> 1410956719792 [dir=none]
	1410956719792 [label="result
 (1, 514, 28, 28)" fillcolor=orange]
	1410956644736 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410956644304 -> 1410956644736
	1410956643776 -> 1410956643728
	1410956643776 -> 1401322030672 [dir=none]
	1401322030672 [label="g
 (144, 1, 1, 1)" fillcolor=orange]
	1410956643776 -> 1410956720368 [dir=none]
	1410956720368 [label="result1
 (144, 1, 1, 1)" fillcolor=orange]
	1410956643776 -> 1401322031536 [dir=none]
	1401322031536 [label="v
 (144, 514, 1, 1)" fillcolor=orange]
	1410956643776 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571473920 -> 1410956643776
	1401322031536 [label="ul_modules.9.in_proj_kv.1.weight_v
 (144, 514, 1, 1)" fillcolor=lightblue]
	1401322031536 -> 1401571473920
	1401571473920 [label=AccumulateGrad]
	1401571473056 -> 1410956643776
	1401322030672 [label="ul_modules.9.in_proj_kv.1.weight_g
 (144, 1, 1, 1)" fillcolor=lightblue]
	1401322030672 -> 1401571473056
	1401571473056 [label=AccumulateGrad]
	1401571470656 -> 1410956643728
	1401322031344 [label="ul_modules.9.in_proj_kv.1.bias
 (144)" fillcolor=lightblue]
	1401322031344 -> 1401571470656
	1401571470656 [label=AccumulateGrad]
	1410956642000 -> 1410956641952
	1410956642000 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 128)"]
	1410956642336 -> 1410956642000
	1410956642336 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 128)"]
	1410956642528 -> 1410956642336
	1410956642528 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1410956642864 -> 1410956642528
	1410956642864 [label="ViewBackward0
-----------------------------------
self_sym_sizes: (1, 1, 128, 28, 28)"]
	1410956643248 -> 1410956642864
	1410956643248 [label="ViewBackward0
--------------------------------
self_sym_sizes: (1, 128, 28, 28)"]
	1410956643680 -> 1410956643248
	1410956641568 -> 1410956641232
	1410956641568 [label=NegBackward0]
	1410956641616 -> 1410956641568
	1410956641280 -> 1410956640992
	1410956641280 -> 1401322032976 [dir=none]
	1401322032976 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410956641280 -> 1410956722640 [dir=none]
	1410956722640 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410956641280 -> 1401322033648 [dir=none]
	1401322033648 [label="v
 (256, 256, 1, 1)" fillcolor=orange]
	1410956641280 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571469792 -> 1410956641280
	1401322033648 [label="ul_modules.9.out_proj.conv_1_sc.weight_v
 (256, 256, 1, 1)" fillcolor=lightblue]
	1401322033648 -> 1401571469792
	1401571469792 [label=AccumulateGrad]
	1401571469312 -> 1410956641280
	1401322032976 [label="ul_modules.9.out_proj.conv_1_sc.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401322032976 -> 1401571469312
	1401571469312 [label=AccumulateGrad]
	1401571468928 -> 1410956640992
	1401322033456 [label="ul_modules.9.out_proj.conv_1_sc.bias
 (256)" fillcolor=lightblue]
	1401322033456 -> 1401571468928
	1401571468928 [label=AccumulateGrad]
	1410956640848 -> 1410956640896
	1410956640848 [label=NegBackward0]
	1410956640512 -> 1410956640848
	1410956361936 -> 1410956377872
	1410956361936 -> 1401322033360 [dir=none]
	1401322033360 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410956361936 -> 1410956724272 [dir=none]
	1410956724272 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410956361936 -> 1401322034032 [dir=none]
	1401322034032 [label="v
 (512, 512, 1, 1)" fillcolor=orange]
	1410956361936 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571468016 -> 1410956361936
	1401322034032 [label="ul_modules.9.out_proj.conv_2.weight_v
 (512, 512, 1, 1)" fillcolor=lightblue]
	1401322034032 -> 1401571468016
	1401571468016 [label=AccumulateGrad]
	1401571468448 -> 1410956361936
	1401322033360 [label="ul_modules.9.out_proj.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401322033360 -> 1401571468448
	1401571468448 [label=AccumulateGrad]
	1401571466960 -> 1410956377872
	1401322033840 [label="ul_modules.9.out_proj.conv_2.bias
 (512)" fillcolor=lightblue]
	1401322033840 -> 1401571466960
	1401571466960 [label=AccumulateGrad]
	1410956377680 -> 1410753888896
	1410956377680 -> 1410956725424 [dir=none]
	1410956725424 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410956377680 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753891008 -> 1410956377680
	1410753889760 -> 1410753890576
	1410753889760 -> 1410754018768 [dir=none]
	1410754018768 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753889760 -> 1410754013872 [dir=none]
	1410754013872 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753889760 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410956362128 -> 1410753889760
	1410956362128 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410956640752 -> 1410956362128
	1410956640752 -> 1410754016848 [dir=none]
	1410754016848 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956640752 -> 1410754011088 [dir=none]
	1410754011088 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410956640752 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956641472 -> 1410956640752
	1410956641472 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956641520 -> 1410956641472
	1410956641520 -> 1410956726480 [dir=none]
	1410956726480 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410956641520 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956641808 -> 1410956641520
	1410956641808 -> 1410754017136 [dir=none]
	1410754017136 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956641808 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956641664 -> 1410956641808
	1410956641664 [label="CatBackward0
------------
dim: 1"]
	1410956642432 -> 1410956641664
	1410956642432 -> 1410754017424 [dir=none]
	1410754017424 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956642432 -> 1410754017616 [dir=none]
	1410754017616 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410956642432 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956644016 -> 1410956642432
	1410956644016 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956642768 -> 1410956644016
	1410956642768 -> 1410754017712 [dir=none]
	1410754017712 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956642768 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956644688 -> 1410956642768
	1410956644688 [label="CatBackward0
------------
dim: 1"]
	1410753889568 -> 1410956644688
	1410956644448 -> 1410956644688
	1410956644448 [label=NegBackward0]
	1410753889568 -> 1410956644448
	1410956643056 -> 1410956642432
	1410956643056 -> 1401322033744 [dir=none]
	1401322033744 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410956643056 -> 1410956728400 [dir=none]
	1410956728400 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410956643056 -> 1401322034608 [dir=none]
	1401322034608 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410956643056 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571474304 -> 1410956643056
	1401322034608 [label="ul_modules.10.input_gated_resnet.0.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401322034608 -> 1401571474304
	1401571474304 [label=AccumulateGrad]
	1401571473968 -> 1410956643056
	1401322033744 [label="ul_modules.10.input_gated_resnet.0.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401322033744 -> 1401571473968
	1401571473968 [label=AccumulateGrad]
	1401571472528 -> 1410956642432
	1401322034416 [label="ul_modules.10.input_gated_resnet.0.conv_1.bias
 (256)" fillcolor=lightblue]
	1401322034416 -> 1401571472528
	1401571472528 [label=AccumulateGrad]
	1410956642240 -> 1410956641664
	1410956642240 [label=NegBackward0]
	1410956642432 -> 1410956642240
	1410956640656 -> 1410956640752
	1410956640656 -> 1401322034320 [dir=none]
	1401322034320 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410956640656 -> 1410956730032 [dir=none]
	1410956730032 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410956640656 -> 1401322034992 [dir=none]
	1401322034992 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410956640656 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571470944 -> 1410956640656
	1401322034992 [label="ul_modules.10.input_gated_resnet.0.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401322034992 -> 1401571470944
	1401571470944 [label=AccumulateGrad]
	1401571470320 -> 1410956640656
	1401322034320 [label="ul_modules.10.input_gated_resnet.0.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401322034320 -> 1401571470320
	1401571470320 [label=AccumulateGrad]
	1401571467920 -> 1410956640752
	1401322034800 [label="ul_modules.10.input_gated_resnet.0.conv_2.bias
 (512)" fillcolor=lightblue]
	1401322034800 -> 1401571467920
	1401571467920 [label=AccumulateGrad]
	1410956362608 -> 1410753889760
	1410956362608 -> 1410956731184 [dir=none]
	1410956731184 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410956362608 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410956362128 -> 1410956362608
	1410753889856 -> 1410753890384
	1410753889856 -> 1410754019984 [dir=none]
	1410754019984 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753889856 -> 1410754019792 [dir=none]
	1410754019792 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753889856 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753888944 -> 1410753889856
	1410753888944 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410956641904 -> 1410753888944
	1410956641904 -> 1410754019600 [dir=none]
	1410754019600 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956641904 -> 1410754014736 [dir=none]
	1410754014736 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410956641904 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956641088 -> 1410956641904
	1410956641088 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956642096 -> 1410956641088
	1410956642096 -> 1410956732240 [dir=none]
	1410956732240 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410956642096 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956645120 -> 1410956642096
	1410956645120 -> 1410754019504 [dir=none]
	1410754019504 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956645120 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956643440 -> 1410956645120
	1410956643440 [label="CatBackward0
------------
dim: 1"]
	1410956645600 -> 1410956643440
	1410956645600 -> 1410754019248 [dir=none]
	1410754019248 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956645600 -> 1410754018960 [dir=none]
	1410754018960 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410956645600 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956645744 -> 1410956645600
	1410956645744 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956645792 -> 1410956645744
	1410956645792 -> 1410754017232 [dir=none]
	1410754017232 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956645792 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956645888 -> 1410956645792
	1410956645888 [label="CatBackward0
------------
dim: 1"]
	1410753890576 -> 1410956645888
	1410956645984 -> 1410956645888
	1410956645984 [label=NegBackward0]
	1410753890576 -> 1410956645984
	1410956645312 -> 1410956645600
	1410956645312 -> 1401322034704 [dir=none]
	1401322034704 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410956645312 -> 1410956734160 [dir=none]
	1410956734160 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410956645312 -> 1401322035568 [dir=none]
	1401322035568 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410956645312 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571475264 -> 1410956645312
	1401322035568 [label="ul_modules.10.input_gated_resnet.1.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401322035568 -> 1401571475264
	1401571475264 [label=AccumulateGrad]
	1401571475168 -> 1410956645312
	1401322034704 [label="ul_modules.10.input_gated_resnet.1.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401322034704 -> 1401571475168
	1401571475168 [label=AccumulateGrad]
	1401571474832 -> 1410956645600
	1401322035376 [label="ul_modules.10.input_gated_resnet.1.conv_1.bias
 (256)" fillcolor=lightblue]
	1401322035376 -> 1401571474832
	1401571474832 [label=AccumulateGrad]
	1410956644976 -> 1410956643440
	1410956644976 [label=NegBackward0]
	1410956645600 -> 1410956644976
	1410956641712 -> 1410956641904
	1410956641712 -> 1401322035280 [dir=none]
	1401322035280 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410956641712 -> 1410956735792 [dir=none]
	1410956735792 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410956641712 -> 1401322035952 [dir=none]
	1401322035952 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410956641712 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571474640 -> 1410956641712
	1401322035952 [label="ul_modules.10.input_gated_resnet.1.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401322035952 -> 1401571474640
	1401571474640 [label=AccumulateGrad]
	1401571474544 -> 1410956641712
	1401322035280 [label="ul_modules.10.input_gated_resnet.1.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401322035280 -> 1401571474544
	1401571474544 [label=AccumulateGrad]
	1401571469696 -> 1410956641904
	1401322035760 [label="ul_modules.10.input_gated_resnet.1.conv_2.bias
 (512)" fillcolor=lightblue]
	1401322035760 -> 1401571469696
	1401571469696 [label=AccumulateGrad]
	1410753889232 -> 1410753889856
	1410753889232 -> 1410956736944 [dir=none]
	1410956736944 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410753889232 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753888944 -> 1410753889232
	1410753894224 -> 1410753894320
	1410753894224 -> 1410754020848 [dir=none]
	1410754020848 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753894224 -> 1410754020656 [dir=none]
	1410754020656 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753894224 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753890288 -> 1410753894224
	1410753890288 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410956645024 -> 1410753890288
	1410956645024 -> 1410754020464 [dir=none]
	1410754020464 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956645024 -> 1410754019152 [dir=none]
	1410754019152 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410956645024 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956643968 -> 1410956645024
	1410956643968 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956645504 -> 1410956643968
	1410956645504 -> 1410956738000 [dir=none]
	1410956738000 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410956645504 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956646080 -> 1410956645504
	1410956646080 -> 1410754020368 [dir=none]
	1410754020368 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956646080 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956645696 -> 1410956646080
	1410956645696 [label="CatBackward0
------------
dim: 1"]
	1410956646176 -> 1410956645696
	1410956646176 -> 1410754020080 [dir=none]
	1410754020080 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956646176 -> 1410754019408 [dir=none]
	1410754019408 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410956646176 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956646320 -> 1410956646176
	1410956646320 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956646464 -> 1410956646320
	1410956646464 -> 1410754019888 [dir=none]
	1410754019888 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956646464 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956646560 -> 1410956646464
	1410956646560 [label="CatBackward0
------------
dim: 1"]
	1410753890384 -> 1410956646560
	1410956646656 -> 1410956646560
	1410956646656 [label=NegBackward0]
	1410753890384 -> 1410956646656
	1410956646272 -> 1410956646176
	1410956646272 -> 1401322035664 [dir=none]
	1401322035664 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410956646272 -> 1410956739984 [dir=none]
	1410956739984 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410956646272 -> 1401322036528 [dir=none]
	1401322036528 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410956646272 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571640128 -> 1410956646272
	1401322036528 [label="ul_modules.10.input_gated_resnet.2.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401322036528 -> 1401571640128
	1401571640128 [label=AccumulateGrad]
	1401571640032 -> 1410956646272
	1401322035664 [label="ul_modules.10.input_gated_resnet.2.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401322035664 -> 1401571640032
	1401571640032 [label=AccumulateGrad]
	1401571639696 -> 1410956646176
	1401322036336 [label="ul_modules.10.input_gated_resnet.2.conv_1.bias
 (256)" fillcolor=lightblue]
	1401322036336 -> 1401571639696
	1401571639696 [label=AccumulateGrad]
	1410956646128 -> 1410956645696
	1410956646128 [label=NegBackward0]
	1410956646176 -> 1410956646128
	1410956644784 -> 1410956645024
	1410956644784 -> 1401322036240 [dir=none]
	1401322036240 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410956644784 -> 1410956741616 [dir=none]
	1410956741616 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410956644784 -> 1401322036912 [dir=none]
	1401322036912 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410956644784 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571475072 -> 1410956644784
	1401322036912 [label="ul_modules.10.input_gated_resnet.2.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401322036912 -> 1401571475072
	1401571475072 [label=AccumulateGrad]
	1401571639504 -> 1410956644784
	1401322036240 [label="ul_modules.10.input_gated_resnet.2.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401322036240 -> 1401571639504
	1401571639504 [label=AccumulateGrad]
	1401571472816 -> 1410956645024
	1401322036720 [label="ul_modules.10.input_gated_resnet.2.conv_2.bias
 (512)" fillcolor=lightblue]
	1401322036720 -> 1401571472816
	1401571472816 [label=AccumulateGrad]
	1410956642144 -> 1410753894224
	1410956642144 -> 1410956742768 [dir=none]
	1410956742768 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410956642144 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753890288 -> 1410956642144
	1410753894368 -> 1410753894464
	1410753894368 -> 1410754021712 [dir=none]
	1410754021712 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753894368 -> 1410754021520 [dir=none]
	1410754021520 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753894368 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753894272 -> 1410753894368
	1410753894272 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410956645936 -> 1410753894272
	1410956645936 -> 1410754021328 [dir=none]
	1410754021328 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956645936 -> 1410754020176 [dir=none]
	1410754020176 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410956645936 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956645840 -> 1410956645936
	1410956645840 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956646224 -> 1410956645840
	1410956646224 -> 1410956743824 [dir=none]
	1410956743824 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410956646224 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956646752 -> 1410956646224
	1410956646752 -> 1410754021232 [dir=none]
	1410754021232 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956646752 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956646368 -> 1410956646752
	1410956646368 [label="CatBackward0
------------
dim: 1"]
	1410956646848 -> 1410956646368
	1410956646848 -> 1410754020944 [dir=none]
	1410754020944 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956646848 -> 1410754020272 [dir=none]
	1410754020272 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410956646848 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956646992 -> 1410956646848
	1410956646992 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956647136 -> 1410956646992
	1410956647136 -> 1410754020752 [dir=none]
	1410754020752 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956647136 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956647232 -> 1410956647136
	1410956647232 [label="CatBackward0
------------
dim: 1"]
	1410753894320 -> 1410956647232
	1410956647328 -> 1410956647232
	1410956647328 [label=NegBackward0]
	1410753894320 -> 1410956647328
	1410956646944 -> 1410956646848
	1410956646944 -> 1401322036624 [dir=none]
	1401322036624 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410956646944 -> 1410956745744 [dir=none]
	1410956745744 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410956646944 -> 1401322037488 [dir=none]
	1401322037488 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410956646944 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571641088 -> 1410956646944
	1401322037488 [label="ul_modules.10.input_gated_resnet.3.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401322037488 -> 1401571641088
	1401571641088 [label=AccumulateGrad]
	1401571640992 -> 1410956646944
	1401322036624 [label="ul_modules.10.input_gated_resnet.3.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401322036624 -> 1401571640992
	1401571640992 [label=AccumulateGrad]
	1401571640656 -> 1410956646848
	1401322037296 [label="ul_modules.10.input_gated_resnet.3.conv_1.bias
 (256)" fillcolor=lightblue]
	1401322037296 -> 1401571640656
	1401571640656 [label=AccumulateGrad]
	1410956646800 -> 1410956646368
	1410956646800 [label=NegBackward0]
	1410956646848 -> 1410956646800
	1410956646032 -> 1410956645936
	1410956646032 -> 1401322037200 [dir=none]
	1401322037200 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410956646032 -> 1410956747376 [dir=none]
	1410956747376 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410956646032 -> 1401322037872 [dir=none]
	1401322037872 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410956646032 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571640464 -> 1410956646032
	1401322037872 [label="ul_modules.10.input_gated_resnet.3.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401322037872 -> 1401571640464
	1401571640464 [label=AccumulateGrad]
	1401571640368 -> 1410956646032
	1401322037200 [label="ul_modules.10.input_gated_resnet.3.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401322037200 -> 1401571640368
	1401571640368 [label=AccumulateGrad]
	1401571640272 -> 1410956645936
	1401322037680 [label="ul_modules.10.input_gated_resnet.3.conv_2.bias
 (512)" fillcolor=lightblue]
	1401322037680 -> 1401571640272
	1401571640272 [label=AccumulateGrad]
	1410956645456 -> 1410753894368
	1410956645456 -> 1410956748528 [dir=none]
	1410956748528 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410956645456 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753894272 -> 1410956645456
	1410753894512 -> 1410753895424
	1410753894512 -> 1410754022576 [dir=none]
	1410754022576 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753894512 -> 1410754022384 [dir=none]
	1410754022384 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753894512 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753894416 -> 1410753894512
	1410753894416 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410956646608 -> 1410753894416
	1410956646608 -> 1410754022192 [dir=none]
	1410754022192 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956646608 -> 1410754021040 [dir=none]
	1410754021040 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1410956646608 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956646512 -> 1410956646608
	1410956646512 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956646896 -> 1410956646512
	1410956646896 -> 1410956749584 [dir=none]
	1410956749584 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410956646896 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956647424 -> 1410956646896
	1410956647424 -> 1410754022096 [dir=none]
	1410754022096 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956647424 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956647040 -> 1410956647424
	1410956647040 [label="CatBackward0
------------
dim: 1"]
	1410956647520 -> 1410956647040
	1410956647520 -> 1410754021808 [dir=none]
	1410754021808 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1410956647520 -> 1410754021136 [dir=none]
	1410754021136 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1410956647520 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956647664 -> 1410956647520
	1410956647664 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1410956647808 -> 1410956647664
	1410956647808 -> 1410754021616 [dir=none]
	1410754021616 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956647808 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956647904 -> 1410956647808
	1410956647904 [label="CatBackward0
------------
dim: 1"]
	1410753894464 -> 1410956647904
	1410956648000 -> 1410956647904
	1410956648000 [label=NegBackward0]
	1410753894464 -> 1410956648000
	1410956647616 -> 1410956647520
	1410956647616 -> 1401322037584 [dir=none]
	1401322037584 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1410956647616 -> 1410956751504 [dir=none]
	1410956751504 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1410956647616 -> 1401322038448 [dir=none]
	1401322038448 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1410956647616 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571642048 -> 1410956647616
	1401322038448 [label="ul_modules.10.input_gated_resnet.4.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401322038448 -> 1401571642048
	1401571642048 [label=AccumulateGrad]
	1401571641952 -> 1410956647616
	1401322037584 [label="ul_modules.10.input_gated_resnet.4.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401322037584 -> 1401571641952
	1401571641952 [label=AccumulateGrad]
	1401571641616 -> 1410956647520
	1401322038256 [label="ul_modules.10.input_gated_resnet.4.conv_1.bias
 (256)" fillcolor=lightblue]
	1401322038256 -> 1401571641616
	1401571641616 [label=AccumulateGrad]
	1410956647472 -> 1410956647040
	1410956647472 [label=NegBackward0]
	1410956647520 -> 1410956647472
	1410956646704 -> 1410956646608
	1410956646704 -> 1401322038160 [dir=none]
	1401322038160 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410956646704 -> 1410956753136 [dir=none]
	1410956753136 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410956646704 -> 1401322038832 [dir=none]
	1401322038832 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1410956646704 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571641424 -> 1410956646704
	1401322038832 [label="ul_modules.10.input_gated_resnet.4.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401322038832 -> 1401571641424
	1401571641424 [label=AccumulateGrad]
	1401571641328 -> 1410956646704
	1401322038160 [label="ul_modules.10.input_gated_resnet.4.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401322038160 -> 1401571641328
	1401571641328 [label=AccumulateGrad]
	1401571639840 -> 1410956646608
	1401322038640 [label="ul_modules.10.input_gated_resnet.4.conv_2.bias
 (512)" fillcolor=lightblue]
	1401322038640 -> 1401571639840
	1401571639840 [label=AccumulateGrad]
	1410956646416 -> 1410753894512
	1410956646416 -> 1410956754288 [dir=none]
	1410956754288 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410956646416 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753894416 -> 1410956646416
	1410753890864 -> 1410753890672
	1410753890864 -> 1410754026896 [dir=none]
	1410754026896 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753890864 -> 1410754026704 [dir=none]
	1410754026704 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753890864 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1410753894560 -> 1410753890864
	1410753894560 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1410956647280 -> 1410753894560
	1410956647280 -> 1410754026512 [dir=none]
	1410754026512 [label="input
 (1, 512, 28, 28)" fillcolor=orange]
	1410956647280 -> 1410754026032 [dir=none]
	1410754026032 [label="weight
 (512, 512, 1, 1)" fillcolor=orange]
	1410956647280 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1410956647184 -> 1410956647280
	1410956647184 -> 1410956755408 [dir=none]
	1410956755408 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1410956647184 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1410956647568 -> 1410956647184
	1410956647568 -> 1410754026416 [dir=none]
	1410754026416 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1410956647568 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1410956648096 -> 1410956647568
	1410956648096 [label="CatBackward0
------------
dim: 1"]
	1401570852816 -> 1410956648096
	1401570852816 [label="AddBackward0
------------
alpha: 1"]
	1401570852384 -> 1401570852816
	1401570852384 -> 1410754025552 [dir=none]
	1410754025552 [label="input
 (1, 512, 28, 28)" fillcolor=orange]
	1401570852384 -> 1410754025936 [dir=none]
	1410754025936 [label="weight
 (256, 512, 1, 1)" fillcolor=orange]
	1401570852384 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401570852144 -> 1401570852384
	1401570852144 -> 1410754025840 [dir=none]
	1410754025840 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1401570852144 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1401570851808 -> 1401570852144
	1401570851808 [label="CatBackward0
------------
dim: 1"]
	1410753895424 -> 1401570851808
	1401570851712 -> 1401570851808
	1401570851712 [label=NegBackward0]
	1410753895424 -> 1401570851712
	1401570852240 -> 1401570852384
	1401570852240 -> 1401322041424 [dir=none]
	1401322041424 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1401570852240 -> 1401709358800 [dir=none]
	1401709358800 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1401570852240 -> 1401322042096 [dir=none]
	1401322042096 [label="v
 (256, 512, 1, 1)" fillcolor=orange]
	1401570852240 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570851616 -> 1401570852240
	1401322042096 [label="ul_modules.10.out_proj.conv_1.weight_v
 (256, 512, 1, 1)" fillcolor=lightblue]
	1401322042096 -> 1401570851616
	1401570851616 [label=AccumulateGrad]
	1401570851760 -> 1401570852240
	1401322041424 [label="ul_modules.10.out_proj.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401322041424 -> 1401570851760
	1401570851760 [label=AccumulateGrad]
	1401570852336 -> 1401570852384
	1401322041904 [label="ul_modules.10.out_proj.conv_1.bias
 (256)" fillcolor=lightblue]
	1401322041904 -> 1401570852336
	1401570852336 [label=AccumulateGrad]
	1401570852432 -> 1401570852816
	1401570852432 -> 1410754026128 [dir=none]
	1410754026128 [label="input
 (1, 256, 28, 28)" fillcolor=orange]
	1401570852432 -> 1410754026320 [dir=none]
	1410754026320 [label="weight
 (256, 256, 1, 1)" fillcolor=orange]
	1401570852432 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401570851664 -> 1401570852432
	1401570851664 -> 1410754026224 [dir=none]
	1410754026224 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1401570851664 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1401570851424 -> 1401570851664
	1401570851424 [label="CatBackward0
------------
dim: 1"]
	1401570851280 -> 1401570851424
	1401570851280 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (1, 1, 128, 784)"]
	1401570851088 -> 1401570851280
	1401570851088 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1401570850944 -> 1401570851088
	1401570850944 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (1, 784, 128)"]
	1401570850800 -> 1401570850944
	1401570850800 -> 1401322646960 [dir=none]
	1401322646960 [label="mat2
 (1, 784, 128)" fillcolor=orange]
	1401570850800 -> 1401322636304 [dir=none]
	1401322636304 [label="self
 (1, 784, 784)" fillcolor=orange]
	1401570850800 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	1401570850560 -> 1401570850800
	1401570850560 [label="ViewBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 784)"]
	1401570850368 -> 1401570850560
	1401570850368 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 784)"]
	1401570850272 -> 1401570850368
	1401570850272 -> 1401323231888 [dir=none]
	1401323231888 [label="result
 (1, 1, 784, 784)" fillcolor=orange]
	1401570850272 [label="SoftmaxBackward0
----------------------
dim   :     4294967295
result: [saved tensor]"]
	1401570850176 -> 1401570850272
	1401570850176 -> 1410754025072 [dir=none]
	1410754025072 [label="mask
 (1, 1, 784, 784)" fillcolor=orange]
	1401570850176 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	1401570849984 -> 1401570850176
	1401570849984 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (1, 784, 784)"]
	1401570849888 -> 1401570849984
	1401570849888 -> 1401323239472 [dir=none]
	1401323239472 [label="mat2
 (1, 16, 784)" fillcolor=orange]
	1401570849888 -> 1401323234096 [dir=none]
	1401323234096 [label="self
 (1, 784, 16)" fillcolor=orange]
	1401570849888 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	1401570849696 -> 1401570849888
	1401570849696 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (1, 1, 784, 16)"]
	1401570849552 -> 1401570849696
	1401570849552 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 784, 16)"]
	1401570849360 -> 1401570849552
	1401570849360 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1401570849120 -> 1401570849360
	1401570849120 -> 1401323231600 [dir=none]
	1401323231600 [label="other
 ()" fillcolor=orange]
	1401570849120 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	1401570849072 -> 1401570849120
	1401570849072 [label="ViewBackward0
----------------------------------
self_sym_sizes: (1, 1, 16, 28, 28)"]
	1401570848880 -> 1401570849072
	1401570848880 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 16, 28, 28)"]
	1401570848736 -> 1401570848880
	1401570848736 -> 1410754024976 [dir=none]
	1410754024976 [label="input
 (1, 258, 28, 28)" fillcolor=orange]
	1401570848736 -> 1410754024688 [dir=none]
	1410754024688 [label="weight
 (16, 258, 1, 1)" fillcolor=orange]
	1401570848736 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401570848592 -> 1401570848736
	1401570848592 [label="AddBackward0
------------
alpha: 1"]
	1401570848400 -> 1401570848592
	1401570848400 [label="CatBackward0
------------
dim: 1"]
	1410753895424 -> 1401570848400
	1401570848448 -> 1401570848592
	1401570848448 -> 1410754024784 [dir=none]
	1410754024784 [label="other
 (1, 258, 28, 28)" fillcolor=orange]
	1401570848448 -> 1410754024592 [dir=none]
	1410754024592 [label="self
 (1, 258, 28, 28)" fillcolor=orange]
	1401570848448 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1401570848256 -> 1401570848448
	1401570848256 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 516, 28, 28)
split_size    :              258"]
	1401570848016 -> 1401570848256
	1401570848016 -> 1410754024208 [dir=none]
	1410754024208 [label="input
 (1, 516, 28, 28)" fillcolor=orange]
	1401570848016 -> 1410754024304 [dir=none]
	1410754024304 [label="weight
 (516, 516, 1, 1)" fillcolor=orange]
	1401570848016 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (516,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401570847920 -> 1401570848016
	1401570847920 -> 1401323229680 [dir=none]
	1401323229680 [label="result1
 (1, 516, 28, 28)" fillcolor=orange]
	1401570847920 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1401570847728 -> 1401570847920
	1401570847728 -> 1410754024400 [dir=none]
	1410754024400 [label="self
 (1, 516, 28, 28)" fillcolor=orange]
	1401570847728 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1401570847536 -> 1401570847728
	1401570847536 [label="CatBackward0
------------
dim: 1"]
	1401570847488 -> 1401570847536
	1401570847488 -> 1410754023920 [dir=none]
	1410754023920 [label="input
 (1, 516, 28, 28)" fillcolor=orange]
	1401570847488 -> 1410754024112 [dir=none]
	1410754024112 [label="weight
 (258, 516, 1, 1)" fillcolor=orange]
	1401570847488 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (258,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401570847248 -> 1401570847488
	1401570847248 -> 1410754024016 [dir=none]
	1410754024016 [label="self
 (1, 516, 28, 28)" fillcolor=orange]
	1401570847248 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1401570847056 -> 1401570847248
	1401570847056 [label="CatBackward0
------------
dim: 1"]
	1401570848400 -> 1401570847056
	1401570846912 -> 1401570847056
	1401570846912 [label=NegBackward0]
	1401570848400 -> 1401570846912
	1401570847392 -> 1401570847488
	1401570847392 -> 1401322040080 [dir=none]
	1401322040080 [label="g
 (258, 1, 1, 1)" fillcolor=orange]
	1401570847392 -> 1410539222992 [dir=none]
	1410539222992 [label="result1
 (258, 1, 1, 1)" fillcolor=orange]
	1401570847392 -> 1401322040752 [dir=none]
	1401322040752 [label="v
 (258, 516, 1, 1)" fillcolor=orange]
	1401570847392 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570846816 -> 1401570847392
	1401322040752 [label="ul_modules.10.in_proj_q.0.conv_1.weight_v
 (258, 516, 1, 1)" fillcolor=lightblue]
	1401322040752 -> 1401570846816
	1401570846816 [label=AccumulateGrad]
	1401570846960 -> 1401570847392
	1401322040080 [label="ul_modules.10.in_proj_q.0.conv_1.weight_g
 (258, 1, 1, 1)" fillcolor=lightblue]
	1401322040080 -> 1401570846960
	1401570846960 [label=AccumulateGrad]
	1401570847440 -> 1401570847488
	1401322040560 [label="ul_modules.10.in_proj_q.0.conv_1.bias
 (258)" fillcolor=lightblue]
	1401322040560 -> 1401570847440
	1401570847440 [label=AccumulateGrad]
	1401570847584 -> 1401570847536
	1401570847584 [label=NegBackward0]
	1401570847488 -> 1401570847584
	1401570847968 -> 1401570848016
	1401570847968 -> 1401322040464 [dir=none]
	1401322040464 [label="g
 (516, 1, 1, 1)" fillcolor=orange]
	1401570847968 -> 1410539221744 [dir=none]
	1410539221744 [label="result1
 (516, 1, 1, 1)" fillcolor=orange]
	1401570847968 -> 1401322041136 [dir=none]
	1401322041136 [label="v
 (516, 516, 1, 1)" fillcolor=orange]
	1401570847968 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570847824 -> 1401570847968
	1401322041136 [label="ul_modules.10.in_proj_q.0.conv_2.weight_v
 (516, 516, 1, 1)" fillcolor=lightblue]
	1401322041136 -> 1401570847824
	1401570847824 [label=AccumulateGrad]
	1401570847680 -> 1401570847968
	1401322040464 [label="ul_modules.10.in_proj_q.0.conv_2.weight_g
 (516, 1, 1, 1)" fillcolor=lightblue]
	1401322040464 -> 1401570847680
	1401570847680 [label=AccumulateGrad]
	1401570848112 -> 1401570848016
	1401322040944 [label="ul_modules.10.in_proj_q.0.conv_2.bias
 (516)" fillcolor=lightblue]
	1401322040944 -> 1401570848112
	1401570848112 [label=AccumulateGrad]
	1401570848208 -> 1401570848448
	1401570848208 -> 1410539222224 [dir=none]
	1410539222224 [label="result
 (1, 258, 28, 28)" fillcolor=orange]
	1401570848208 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1401570848256 -> 1401570848208
	1401570848688 -> 1401570848736
	1401570848688 -> 1401322040848 [dir=none]
	1401322040848 [label="g
 (16, 1, 1, 1)" fillcolor=orange]
	1401570848688 -> 1410539646864 [dir=none]
	1410539646864 [label="result1
 (16, 1, 1, 1)" fillcolor=orange]
	1401570848688 -> 1401322041712 [dir=none]
	1401322041712 [label="v
 (16, 258, 1, 1)" fillcolor=orange]
	1401570848688 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570847152 -> 1401570848688
	1401322041712 [label="ul_modules.10.in_proj_q.1.weight_v
 (16, 258, 1, 1)" fillcolor=lightblue]
	1401322041712 -> 1401570847152
	1401570847152 [label=AccumulateGrad]
	1401570848160 -> 1401570848688
	1401322040848 [label="ul_modules.10.in_proj_q.1.weight_g
 (16, 1, 1, 1)" fillcolor=lightblue]
	1401322040848 -> 1401570848160
	1401570848160 [label=AccumulateGrad]
	1401570849648 -> 1401570848736
	1401322041520 [label="ul_modules.10.in_proj_q.1.bias
 (16)" fillcolor=lightblue]
	1401322041520 -> 1401570849648
	1401570849648 [label=AccumulateGrad]
	1401570849840 -> 1401570849888
	1401570849840 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 1, 16, 784)"]
	1401570849264 -> 1401570849840
	1401570849264 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 16, 784)"]
	1401570848928 -> 1401570849264
	1401570848928 [label="ViewBackward0
----------------------------------
self_sym_sizes: (1, 1, 16, 28, 28)"]
	1401570848544 -> 1401570848928
	1401570848544 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 16, 28, 28)"]
	1401570847872 -> 1401570848544
	1401570847872 [label="SplitWithSizesBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 144, 28, 28)
split_sizes   :        (16, 128)"]
	1401570846864 -> 1401570847872
	1401570846864 -> 1410754023632 [dir=none]
	1410754023632 [label="input
 (1, 514, 28, 28)" fillcolor=orange]
	1401570846864 -> 1410754023344 [dir=none]
	1410754023344 [label="weight
 (144, 514, 1, 1)" fillcolor=orange]
	1401570846864 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (144,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401570847200 -> 1401570846864
	1401570847200 [label="AddBackward0
------------
alpha: 1"]
	1401570846672 -> 1401570847200
	1401570846672 [label="CatBackward0
------------
dim: 1"]
	1410753889568 -> 1401570846672
	1410753895424 -> 1401570846672
	1401570846720 -> 1401570847200
	1401570846720 -> 1410754023440 [dir=none]
	1410754023440 [label="other
 (1, 514, 28, 28)" fillcolor=orange]
	1401570846720 -> 1410754023248 [dir=none]
	1410754023248 [label="self
 (1, 514, 28, 28)" fillcolor=orange]
	1401570846720 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1401570846624 -> 1401570846720
	1401570846624 [label="SplitBackward0
---------------------------------
dim           :                 1
self_sym_sizes: (1, 1028, 28, 28)
split_size    :               514"]
	1401570846288 -> 1401570846624
	1401570846288 -> 1410754022864 [dir=none]
	1410754022864 [label="input
 (1, 1028, 28, 28)" fillcolor=orange]
	1401570846288 -> 1410754022960 [dir=none]
	1410754022960 [label="weight
 (1028, 1028, 1, 1)" fillcolor=orange]
	1401570846288 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1028,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401570846192 -> 1401570846288
	1401570846192 -> 1410539642544 [dir=none]
	1410539642544 [label="result1
 (1, 1028, 28, 28)" fillcolor=orange]
	1401570846192 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1401570845904 -> 1401570846192
	1401570845904 -> 1410754023056 [dir=none]
	1410754023056 [label="self
 (1, 1028, 28, 28)" fillcolor=orange]
	1401570845904 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1401570845760 -> 1401570845904
	1401570845760 [label="CatBackward0
------------
dim: 1"]
	1401570845664 -> 1401570845760
	1401570845664 -> 1410754022480 [dir=none]
	1410754022480 [label="input
 (1, 1028, 28, 28)" fillcolor=orange]
	1401570845664 -> 1410754022672 [dir=none]
	1410754022672 [label="weight
 (514, 1028, 1, 1)" fillcolor=orange]
	1401570845664 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (514,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401570845424 -> 1401570845664
	1401570845424 -> 1410754022000 [dir=none]
	1410754022000 [label="self
 (1, 1028, 28, 28)" fillcolor=orange]
	1401570845424 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1401570845232 -> 1401570845424
	1401570845232 [label="CatBackward0
------------
dim: 1"]
	1401570846672 -> 1401570845232
	1401570845088 -> 1401570845232
	1401570845088 [label=NegBackward0]
	1401570846672 -> 1401570845088
	1401570845520 -> 1401570845664
	1401570845520 -> 1401322038544 [dir=none]
	1401322038544 [label="g
 (514, 1, 1, 1)" fillcolor=orange]
	1401570845520 -> 1410539638224 [dir=none]
	1410539638224 [label="result1
 (514, 1, 1, 1)" fillcolor=orange]
	1401570845520 -> 1401322039408 [dir=none]
	1401322039408 [label="v
 (514, 1028, 1, 1)" fillcolor=orange]
	1401570845520 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570844992 -> 1401570845520
	1401322039408 [label="ul_modules.10.in_proj_kv.0.conv_1.weight_v
 (514, 1028, 1, 1)" fillcolor=lightblue]
	1401322039408 -> 1401570844992
	1401570844992 [label=AccumulateGrad]
	1401570845184 -> 1401570845520
	1401322038544 [label="ul_modules.10.in_proj_kv.0.conv_1.weight_g
 (514, 1, 1, 1)" fillcolor=lightblue]
	1401322038544 -> 1401570845184
	1401570845184 [label=AccumulateGrad]
	1401570845568 -> 1401570845664
	1401322039216 [label="ul_modules.10.in_proj_kv.0.conv_1.bias
 (514)" fillcolor=lightblue]
	1401322039216 -> 1401570845568
	1401570845568 [label=AccumulateGrad]
	1401570845712 -> 1401570845760
	1401570845712 [label=NegBackward0]
	1401570845664 -> 1401570845712
	1401570846240 -> 1401570846288
	1401570846240 -> 1401322039120 [dir=none]
	1401322039120 [label="g
 (1028, 1, 1, 1)" fillcolor=orange]
	1401570846240 -> 1410539640912 [dir=none]
	1410539640912 [label="result1
 (1028, 1, 1, 1)" fillcolor=orange]
	1401570846240 -> 1401322039792 [dir=none]
	1401322039792 [label="v
 (1028, 1028, 1, 1)" fillcolor=orange]
	1401570846240 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570846096 -> 1401570846240
	1401322039792 [label="ul_modules.10.in_proj_kv.0.conv_2.weight_v
 (1028, 1028, 1, 1)" fillcolor=lightblue]
	1401322039792 -> 1401570846096
	1401570846096 [label=AccumulateGrad]
	1401570845856 -> 1401570846240
	1401322039120 [label="ul_modules.10.in_proj_kv.0.conv_2.weight_g
 (1028, 1, 1, 1)" fillcolor=lightblue]
	1401322039120 -> 1401570845856
	1401570845856 [label=AccumulateGrad]
	1401570846432 -> 1401570846288
	1401322039600 [label="ul_modules.10.in_proj_kv.0.conv_2.bias
 (1028)" fillcolor=lightblue]
	1401322039600 -> 1401570846432
	1401570846432 [label=AccumulateGrad]
	1401570846576 -> 1401570846720
	1401570846576 -> 1410539637936 [dir=none]
	1410539637936 [label="result
 (1, 514, 28, 28)" fillcolor=orange]
	1401570846576 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1401570846624 -> 1401570846576
	1401570847776 -> 1401570846864
	1401570847776 -> 1401322039504 [dir=none]
	1401322039504 [label="g
 (144, 1, 1, 1)" fillcolor=orange]
	1401570847776 -> 1410539637552 [dir=none]
	1410539637552 [label="result1
 (144, 1, 1, 1)" fillcolor=orange]
	1401570847776 -> 1401322040368 [dir=none]
	1401322040368 [label="v
 (144, 514, 1, 1)" fillcolor=orange]
	1401570847776 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570845328 -> 1401570847776
	1401322040368 [label="ul_modules.10.in_proj_kv.1.weight_v
 (144, 514, 1, 1)" fillcolor=lightblue]
	1401322040368 -> 1401570845328
	1401570845328 [label=AccumulateGrad]
	1401570846528 -> 1401570847776
	1401322039504 [label="ul_modules.10.in_proj_kv.1.weight_g
 (144, 1, 1, 1)" fillcolor=lightblue]
	1401322039504 -> 1401570846528
	1401570846528 [label=AccumulateGrad]
	1401570849600 -> 1401570846864
	1401322040176 [label="ul_modules.10.in_proj_kv.1.bias
 (144)" fillcolor=lightblue]
	1401322040176 -> 1401570849600
	1401570849600 [label=AccumulateGrad]
	1401570850704 -> 1401570850800
	1401570850704 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 128)"]
	1401570850128 -> 1401570850704
	1401570850128 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 128)"]
	1401570849936 -> 1401570850128
	1401570849936 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1401570849408 -> 1401570849936
	1401570849408 [label="ViewBackward0
-----------------------------------
self_sym_sizes: (1, 1, 128, 28, 28)"]
	1401570848832 -> 1401570849408
	1401570848832 [label="ViewBackward0
--------------------------------
self_sym_sizes: (1, 128, 28, 28)"]
	1401570847872 -> 1401570848832
	1401570851376 -> 1401570851424
	1401570851376 [label=NegBackward0]
	1401570851280 -> 1401570851376
	1401570852000 -> 1401570852432
	1401570852000 -> 1401322041808 [dir=none]
	1401322041808 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1401570852000 -> 1410539635632 [dir=none]
	1410539635632 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1401570852000 -> 1401322042480 [dir=none]
	1401322042480 [label="v
 (256, 256, 1, 1)" fillcolor=orange]
	1401570852000 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570850848 -> 1401570852000
	1401322042480 [label="ul_modules.10.out_proj.conv_1_sc.weight_v
 (256, 256, 1, 1)" fillcolor=lightblue]
	1401322042480 -> 1401570850848
	1401570850848 [label=AccumulateGrad]
	1401570851568 -> 1401570852000
	1401322041808 [label="ul_modules.10.out_proj.conv_1_sc.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401322041808 -> 1401570851568
	1401570851568 [label=AccumulateGrad]
	1401570852048 -> 1401570852432
	1401322042288 [label="ul_modules.10.out_proj.conv_1_sc.bias
 (256)" fillcolor=lightblue]
	1401322042288 -> 1401570852048
	1401570852048 [label=AccumulateGrad]
	1401570852624 -> 1410956648096
	1401570852624 [label=NegBackward0]
	1401570852816 -> 1401570852624
	1410956647376 -> 1410956647280
	1410956647376 -> 1401322042192 [dir=none]
	1401322042192 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1410956647376 -> 1401322688240 [dir=none]
	1401322688240 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1410956647376 -> 1401322042864 [dir=none]
	1401322042864 [label="v
 (512, 512, 1, 1)" fillcolor=orange]
	1410956647376 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570851904 -> 1410956647376
	1401322042864 [label="ul_modules.10.out_proj.conv_2.weight_v
 (512, 512, 1, 1)" fillcolor=lightblue]
	1401322042864 -> 1401570851904
	1401570851904 [label=AccumulateGrad]
	1401570851136 -> 1410956647376
	1401322042192 [label="ul_modules.10.out_proj.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401322042192 -> 1401570851136
	1401570851136 [label=AccumulateGrad]
	1401571640800 -> 1410956647280
	1401322042672 [label="ul_modules.10.out_proj.conv_2.bias
 (512)" fillcolor=lightblue]
	1401322042672 -> 1401571640800
	1401571640800 [label=AccumulateGrad]
	1410956647088 -> 1410753890864
	1410956647088 -> 1401323220304 [dir=none]
	1401323220304 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1410956647088 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1410753894560 -> 1410956647088
	1410753890720 -> 1410753890816
	1410753890720 -> 1410754026800 [dir=none]
	1410754026800 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753890720 -> 1410754021904 [dir=none]
	1410754021904 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753890720 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1401570851040 -> 1410753890720
	1401570851040 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1401570850320 -> 1401570851040
	1401570850320 -> 1410754024880 [dir=none]
	1410754024880 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1401570850320 -> 1410753920944 [dir=none]
	1410753920944 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1401570850320 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401570850512 -> 1401570850320
	1401570850512 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1401570850464 -> 1401570850512
	1401570850464 -> 1401323214640 [dir=none]
	1401323214640 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1401570850464 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1401570846336 -> 1401570850464
	1401570846336 -> 1410754025168 [dir=none]
	1410754025168 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1401570846336 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1401570846768 -> 1401570846336
	1401570846768 [label="CatBackward0
------------
dim: 1"]
	1401570846048 -> 1401570846768
	1401570846048 -> 1410754025456 [dir=none]
	1410754025456 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1401570846048 -> 1410754025648 [dir=none]
	1410754025648 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1401570846048 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401570844944 -> 1401570846048
	1401570844944 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1401570844560 -> 1401570844944
	1401570844560 -> 1410754025744 [dir=none]
	1410754025744 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1401570844560 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1401570844416 -> 1401570844560
	1401570844416 [label="CatBackward0
------------
dim: 1"]
	1410753890672 -> 1401570844416
	1401570844320 -> 1401570844416
	1401570844320 [label=NegBackward0]
	1410753890672 -> 1401570844320
	1401570845280 -> 1401570846048
	1401570845280 -> 1401322042576 [dir=none]
	1401322042576 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1401570845280 -> 1401323209552 [dir=none]
	1401323209552 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1401570845280 -> 1401322043440 [dir=none]
	1401322043440 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1401570845280 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570844368 -> 1401570845280
	1401322043440 [label="ul_modules.11.input_gated_resnet.0.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401322043440 -> 1401570844368
	1401570844368 [label=AccumulateGrad]
	1401570844512 -> 1401570845280
	1401322042576 [label="ul_modules.11.input_gated_resnet.0.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401322042576 -> 1401570844512
	1401570844512 [label=AccumulateGrad]
	1401570845376 -> 1401570846048
	1401322043248 [label="ul_modules.11.input_gated_resnet.0.conv_1.bias
 (256)" fillcolor=lightblue]
	1401322043248 -> 1401570845376
	1401570845376 [label=AccumulateGrad]
	1401570845040 -> 1401570846768
	1401570845040 [label=NegBackward0]
	1401570846048 -> 1401570845040
	1401570850032 -> 1401570850320
	1401570850032 -> 1401322043152 [dir=none]
	1401322043152 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1401570850032 -> 1410724927472 [dir=none]
	1410724927472 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1401570850032 -> 1401322043824 [dir=none]
	1401322043824 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1401570850032 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570846144 -> 1401570850032
	1401322043824 [label="ul_modules.11.input_gated_resnet.0.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401322043824 -> 1401570846144
	1401570846144 [label=AccumulateGrad]
	1401570847104 -> 1401570850032
	1401322043152 [label="ul_modules.11.input_gated_resnet.0.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401322043152 -> 1401570847104
	1401570847104 [label=AccumulateGrad]
	1401570851472 -> 1401570850320
	1401322043632 [label="ul_modules.11.input_gated_resnet.0.conv_2.bias
 (512)" fillcolor=lightblue]
	1401322043632 -> 1401570851472
	1401570851472 [label=AccumulateGrad]
	1401570852528 -> 1410753890720
	1401570852528 -> 1410724933040 [dir=none]
	1410724933040 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1401570852528 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1401570851040 -> 1401570852528
	1410753895808 -> 1410753893744
	1410753895808 -> 1410754027952 [dir=none]
	1410754027952 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753895808 -> 1410754027760 [dir=none]
	1410754027760 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753895808 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1401570849024 -> 1410753895808
	1401570849024 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1401570848064 -> 1401570849024
	1401570848064 -> 1410754027568 [dir=none]
	1410754027568 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1401570848064 -> 1410754022768 [dir=none]
	1410754022768 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1401570848064 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401570844848 -> 1401570848064
	1401570844848 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1401570844032 -> 1401570844848
	1401570844032 -> 1410724932464 [dir=none]
	1410724932464 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1401570844032 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1401570843888 -> 1401570844032
	1401570843888 -> 1410754027472 [dir=none]
	1410754027472 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1401570843888 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1401570843744 -> 1401570843888
	1401570843744 [label="CatBackward0
------------
dim: 1"]
	1401570843600 -> 1401570843744
	1401570843600 -> 1410754027280 [dir=none]
	1410754027280 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1401570843600 -> 1410754026992 [dir=none]
	1410754026992 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1401570843600 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401570843408 -> 1401570843600
	1401570843408 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1401570843120 -> 1401570843408
	1401570843120 -> 1410754025264 [dir=none]
	1410754025264 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1401570843120 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1401570843024 -> 1401570843120
	1401570843024 [label="CatBackward0
------------
dim: 1"]
	1410753890816 -> 1401570843024
	1401570842832 -> 1401570843024
	1401570842832 [label=NegBackward0]
	1410753890816 -> 1401570842832
	1401570843456 -> 1401570843600
	1401570843456 -> 1401322043536 [dir=none]
	1401322043536 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1401570843456 -> 1410724933520 [dir=none]
	1410724933520 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1401570843456 -> 1401322044400 [dir=none]
	1401322044400 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1401570843456 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570842928 -> 1401570843456
	1401322044400 [label="ul_modules.11.input_gated_resnet.1.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401322044400 -> 1401570842928
	1401570842928 [label=AccumulateGrad]
	1401570843072 -> 1401570843456
	1401322043536 [label="ul_modules.11.input_gated_resnet.1.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401322043536 -> 1401570843072
	1401570843072 [label=AccumulateGrad]
	1401570843552 -> 1401570843600
	1401322044208 [label="ul_modules.11.input_gated_resnet.1.conv_1.bias
 (256)" fillcolor=lightblue]
	1401322044208 -> 1401570843552
	1401570843552 [label=AccumulateGrad]
	1401570843696 -> 1401570843744
	1401570843696 [label=NegBackward0]
	1401570843600 -> 1401570843696
	1401570844224 -> 1401570848064
	1401570844224 -> 1401322044112 [dir=none]
	1401322044112 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1401570844224 -> 1410724930160 [dir=none]
	1410724930160 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1401570844224 -> 1401322044784 [dir=none]
	1401322044784 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1401570844224 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570843792 -> 1401570844224
	1401322044784 [label="ul_modules.11.input_gated_resnet.1.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401322044784 -> 1401570843792
	1401570843792 [label=AccumulateGrad]
	1401570843984 -> 1401570844224
	1401322044112 [label="ul_modules.11.input_gated_resnet.1.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401322044112 -> 1401570843984
	1401570843984 [label=AccumulateGrad]
	1401570851184 -> 1401570848064
	1401322044592 [label="ul_modules.11.input_gated_resnet.1.conv_2.bias
 (512)" fillcolor=lightblue]
	1401322044592 -> 1401570851184
	1401570851184 [label=AccumulateGrad]
	1401570844128 -> 1410753895808
	1401570844128 -> 1410724936112 [dir=none]
	1410724936112 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1401570844128 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1401570849024 -> 1401570844128
	1410753893792 -> 1410539319312
	1410753893792 -> 1410754028816 [dir=none]
	1410754028816 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410753893792 -> 1410754028624 [dir=none]
	1410754028624 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410753893792 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1401570844656 -> 1410753893792
	1401570844656 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1401570844080 -> 1401570844656
	1401570844080 -> 1410754028432 [dir=none]
	1410754028432 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1401570844080 -> 1410754027184 [dir=none]
	1410754027184 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1401570844080 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401570843360 -> 1401570844080
	1401570843360 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1401570842592 -> 1401570843360
	1401570842592 -> 1410724937072 [dir=none]
	1410724937072 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1401570842592 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1401570842448 -> 1401570842592
	1401570842448 -> 1410754028336 [dir=none]
	1410754028336 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1401570842448 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1401570842304 -> 1401570842448
	1401570842304 [label="CatBackward0
------------
dim: 1"]
	1401570842160 -> 1401570842304
	1401570842160 -> 1410754028048 [dir=none]
	1410754028048 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1401570842160 -> 1410754027376 [dir=none]
	1410754027376 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1401570842160 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401570841968 -> 1401570842160
	1401570841968 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1401570841680 -> 1401570841968
	1401570841680 -> 1410754027856 [dir=none]
	1410754027856 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1401570841680 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1401570841536 -> 1401570841680
	1401570841536 [label="CatBackward0
------------
dim: 1"]
	1410753893744 -> 1401570841536
	1401570841440 -> 1401570841536
	1401570841440 [label=NegBackward0]
	1410753893744 -> 1401570841440
	1401570842064 -> 1401570842160
	1401570842064 -> 1401322044496 [dir=none]
	1401322044496 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1401570842064 -> 1401323116912 [dir=none]
	1401323116912 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1401570842064 -> 1401322045360 [dir=none]
	1401322045360 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1401570842064 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570841488 -> 1401570842064
	1401322045360 [label="ul_modules.11.input_gated_resnet.2.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401322045360 -> 1401570841488
	1401570841488 [label=AccumulateGrad]
	1401570841632 -> 1401570842064
	1401322044496 [label="ul_modules.11.input_gated_resnet.2.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401322044496 -> 1401570841632
	1401570841632 [label=AccumulateGrad]
	1401570842112 -> 1401570842160
	1401322045168 [label="ul_modules.11.input_gated_resnet.2.conv_1.bias
 (256)" fillcolor=lightblue]
	1401322045168 -> 1401570842112
	1401570842112 [label=AccumulateGrad]
	1401570842208 -> 1401570842304
	1401570842208 [label=NegBackward0]
	1401570842160 -> 1401570842208
	1401570842784 -> 1401570844080
	1401570842784 -> 1401322045072 [dir=none]
	1401322045072 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1401570842784 -> 1401323116816 [dir=none]
	1401323116816 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1401570842784 -> 1401322324336 [dir=none]
	1401322324336 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1401570842784 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570842400 -> 1401570842784
	1401322324336 [label="ul_modules.11.input_gated_resnet.2.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401322324336 -> 1401570842400
	1401570842400 [label=AccumulateGrad]
	1401570842496 -> 1401570842784
	1401322045072 [label="ul_modules.11.input_gated_resnet.2.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401322045072 -> 1401570842496
	1401570842496 [label=AccumulateGrad]
	1401570848496 -> 1401570844080
	1401322324144 [label="ul_modules.11.input_gated_resnet.2.conv_2.bias
 (512)" fillcolor=lightblue]
	1401322324144 -> 1401570848496
	1401570848496 [label=AccumulateGrad]
	1401570842736 -> 1410753893792
	1401570842736 -> 1401323112400 [dir=none]
	1401323112400 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1401570842736 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1401570844656 -> 1401570842736
	1410539320560 -> 1410539321712
	1410539320560 -> 1410754029680 [dir=none]
	1410754029680 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410539320560 -> 1410754029488 [dir=none]
	1410754029488 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410539320560 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1401570843168 -> 1410539320560
	1401570843168 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1401570842640 -> 1401570843168
	1401570842640 -> 1410754029296 [dir=none]
	1410754029296 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1401570842640 -> 1410754028144 [dir=none]
	1410754028144 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1401570842640 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401570841872 -> 1401570842640
	1401570841872 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1401570841104 -> 1401570841872
	1401570841104 -> 1401323051952 [dir=none]
	1401323051952 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1401570841104 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1401570840912 -> 1401570841104
	1401570840912 -> 1410754029200 [dir=none]
	1410754029200 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1401570840912 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1401570840768 -> 1401570840912
	1401570840768 [label="CatBackward0
------------
dim: 1"]
	1401570840576 -> 1401570840768
	1401570840576 -> 1410754028912 [dir=none]
	1410754028912 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1401570840576 -> 1410754028240 [dir=none]
	1410754028240 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1401570840576 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401570840384 -> 1401570840576
	1401570840384 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1401570840096 -> 1401570840384
	1401570840096 -> 1410754028720 [dir=none]
	1410754028720 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1401570840096 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1401570840000 -> 1401570840096
	1401570840000 [label="CatBackward0
------------
dim: 1"]
	1410539319312 -> 1401570840000
	1401570839904 -> 1401570840000
	1401570839904 [label=NegBackward0]
	1410539319312 -> 1401570839904
	1401570840432 -> 1401570840576
	1401570840432 -> 1401322324048 [dir=none]
	1401322324048 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1401570840432 -> 1401323053008 [dir=none]
	1401323053008 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1401570840432 -> 1401322324912 [dir=none]
	1401322324912 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1401570840432 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570839952 -> 1401570840432
	1401322324912 [label="ul_modules.11.input_gated_resnet.3.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401322324912 -> 1401570839952
	1401570839952 [label=AccumulateGrad]
	1401570840048 -> 1401570840432
	1401322324048 [label="ul_modules.11.input_gated_resnet.3.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401322324048 -> 1401570840048
	1401570840048 [label=AccumulateGrad]
	1401570840528 -> 1401570840576
	1401322324720 [label="ul_modules.11.input_gated_resnet.3.conv_1.bias
 (256)" fillcolor=lightblue]
	1401322324720 -> 1401570840528
	1401570840528 [label=AccumulateGrad]
	1401570840624 -> 1401570840768
	1401570840624 [label=NegBackward0]
	1401570840576 -> 1401570840624
	1401570841344 -> 1401570842640
	1401570841344 -> 1401322324624 [dir=none]
	1401322324624 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1401570841344 -> 1401323053584 [dir=none]
	1401323053584 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1401570841344 -> 1401322325296 [dir=none]
	1401322325296 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1401570841344 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570840864 -> 1401570841344
	1401322325296 [label="ul_modules.11.input_gated_resnet.3.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401322325296 -> 1401570840864
	1401570840864 [label=AccumulateGrad]
	1401570841008 -> 1401570841344
	1401322324624 [label="ul_modules.11.input_gated_resnet.3.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401322324624 -> 1401570841008
	1401570841008 [label=AccumulateGrad]
	1401570844752 -> 1401570842640
	1401322325104 [label="ul_modules.11.input_gated_resnet.3.conv_2.bias
 (512)" fillcolor=lightblue]
	1401322325104 -> 1401570844752
	1401570844752 [label=AccumulateGrad]
	1401570841248 -> 1410539320560
	1401570841248 -> 1401323020336 [dir=none]
	1401323020336 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1401570841248 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1401570843168 -> 1401570841248
	1410539320416 -> 1410539321424
	1410539320416 -> 1410754030544 [dir=none]
	1410754030544 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410539320416 -> 1410754030352 [dir=none]
	1410754030352 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410539320416 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1401570841776 -> 1410539320416
	1401570841776 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1401570841152 -> 1401570841776
	1401570841152 -> 1410754030160 [dir=none]
	1410754030160 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1401570841152 -> 1410754029008 [dir=none]
	1410754029008 [label="weight
 (512, 512, 2, 2)" fillcolor=orange]
	1401570841152 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401570840336 -> 1401570841152
	1401570840336 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1401570839616 -> 1401570840336
	1401570839616 -> 1401323108400 [dir=none]
	1401323108400 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1401570839616 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1401570839424 -> 1401570839616
	1401570839424 -> 1410754030064 [dir=none]
	1410754030064 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1401570839424 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1401570839184 -> 1401570839424
	1401570839184 [label="CatBackward0
------------
dim: 1"]
	1401570839088 -> 1401570839184
	1401570839088 -> 1410754029776 [dir=none]
	1410754029776 [label="input
 (1, 512, 29, 29)" fillcolor=orange]
	1401570839088 -> 1410754029104 [dir=none]
	1410754029104 [label="weight
 (256, 512, 2, 2)" fillcolor=orange]
	1401570839088 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401570838896 -> 1401570839088
	1401570838896 [label="ConstantPadNdBackward0
----------------------
pad: (1, 0, 1, 0)"]
	1401570838608 -> 1401570838896
	1401570838608 -> 1410754029584 [dir=none]
	1410754029584 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1401570838608 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1401570838512 -> 1401570838608
	1401570838512 [label="CatBackward0
------------
dim: 1"]
	1410539321712 -> 1401570838512
	1401570838320 -> 1401570838512
	1401570838320 [label=NegBackward0]
	1410539321712 -> 1401570838320
	1401570838944 -> 1401570839088
	1401570838944 -> 1401322325008 [dir=none]
	1401322325008 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1401570838944 -> 1401323108016 [dir=none]
	1401323108016 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1401570838944 -> 1401322325872 [dir=none]
	1401322325872 [label="v
 (256, 512, 2, 2)" fillcolor=orange]
	1401570838944 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570838464 -> 1401570838944
	1401322325872 [label="ul_modules.11.input_gated_resnet.4.conv_1.weight_v
 (256, 512, 2, 2)" fillcolor=lightblue]
	1401322325872 -> 1401570838464
	1401570838464 [label=AccumulateGrad]
	1401570838560 -> 1401570838944
	1401322325008 [label="ul_modules.11.input_gated_resnet.4.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401322325008 -> 1401570838560
	1401570838560 [label=AccumulateGrad]
	1401570838992 -> 1401570839088
	1401322325680 [label="ul_modules.11.input_gated_resnet.4.conv_1.bias
 (256)" fillcolor=lightblue]
	1401322325680 -> 1401570838992
	1401570838992 [label=AccumulateGrad]
	1401570839136 -> 1401570839184
	1401570839136 [label=NegBackward0]
	1401570839088 -> 1401570839136
	1401570839760 -> 1401570841152
	1401570839760 -> 1401322325584 [dir=none]
	1401322325584 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1401570839760 -> 1410753906256 [dir=none]
	1410753906256 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1401570839760 -> 1401322326256 [dir=none]
	1401322326256 [label="v
 (512, 512, 2, 2)" fillcolor=orange]
	1401570839760 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570839328 -> 1401570839760
	1401322326256 [label="ul_modules.11.input_gated_resnet.4.conv_2.weight_v
 (512, 512, 2, 2)" fillcolor=lightblue]
	1401322326256 -> 1401570839328
	1401570839328 [label=AccumulateGrad]
	1401570839472 -> 1401570839760
	1401322325584 [label="ul_modules.11.input_gated_resnet.4.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401322325584 -> 1401570839472
	1401570839472 [label=AccumulateGrad]
	1401570843264 -> 1401570841152
	1401322326064 [label="ul_modules.11.input_gated_resnet.4.conv_2.bias
 (512)" fillcolor=lightblue]
	1401322326064 -> 1401570843264
	1401570843264 [label=AccumulateGrad]
	1401570839712 -> 1410539320416
	1401570839712 -> 1410753907408 [dir=none]
	1410753907408 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1401570839712 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1401570841776 -> 1401570839712
	1410539320272 -> 1401709432592
	1410539320272 -> 1410754034864 [dir=none]
	1410754034864 [label="other
 (1, 256, 28, 28)" fillcolor=orange]
	1410539320272 -> 1410754034672 [dir=none]
	1410754034672 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1410539320272 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1401570840192 -> 1410539320272
	1401570840192 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 512, 28, 28)
split_size    :              256"]
	1401570839664 -> 1401570840192
	1401570839664 -> 1410754034480 [dir=none]
	1410754034480 [label="input
 (1, 512, 28, 28)" fillcolor=orange]
	1401570839664 -> 1410754034000 [dir=none]
	1410754034000 [label="weight
 (512, 512, 1, 1)" fillcolor=orange]
	1401570839664 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401570838800 -> 1401570839664
	1401570838800 -> 1410753908464 [dir=none]
	1410753908464 [label="result1
 (1, 512, 28, 28)" fillcolor=orange]
	1401570838800 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1401570838128 -> 1401570838800
	1401570838128 -> 1410754034384 [dir=none]
	1410754034384 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1401570838128 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1401570838032 -> 1401570838128
	1401570838032 [label="CatBackward0
------------
dim: 1"]
	1401570837936 -> 1401570838032
	1401570837936 [label="AddBackward0
------------
alpha: 1"]
	1401570837792 -> 1401570837936
	1401570837792 -> 1410754033520 [dir=none]
	1410754033520 [label="input
 (1, 512, 28, 28)" fillcolor=orange]
	1401570837792 -> 1410754033904 [dir=none]
	1410754033904 [label="weight
 (256, 512, 1, 1)" fillcolor=orange]
	1401570837792 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401570837600 -> 1401570837792
	1401570837600 -> 1410754033808 [dir=none]
	1410754033808 [label="self
 (1, 512, 28, 28)" fillcolor=orange]
	1401570837600 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1401570837312 -> 1401570837600
	1401570837312 [label="CatBackward0
------------
dim: 1"]
	1410539321424 -> 1401570837312
	1401570837216 -> 1401570837312
	1401570837216 [label=NegBackward0]
	1410539321424 -> 1401570837216
	1401570837648 -> 1401570837792
	1401570837648 -> 1401322328848 [dir=none]
	1401322328848 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1401570837648 -> 1410753910480 [dir=none]
	1410753910480 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1401570837648 -> 1401322329520 [dir=none]
	1401322329520 [label="v
 (256, 512, 1, 1)" fillcolor=orange]
	1401570837648 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570837120 -> 1401570837648
	1401322329520 [label="ul_modules.11.out_proj.conv_1.weight_v
 (256, 512, 1, 1)" fillcolor=lightblue]
	1401322329520 -> 1401570837120
	1401570837120 [label=AccumulateGrad]
	1401570837264 -> 1401570837648
	1401322328848 [label="ul_modules.11.out_proj.conv_1.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401322328848 -> 1401570837264
	1401570837264 [label=AccumulateGrad]
	1401570837744 -> 1401570837792
	1401322329328 [label="ul_modules.11.out_proj.conv_1.bias
 (256)" fillcolor=lightblue]
	1401322329328 -> 1401570837744
	1401570837744 [label=AccumulateGrad]
	1401570837840 -> 1401570837936
	1401570837840 -> 1410754034096 [dir=none]
	1410754034096 [label="input
 (1, 256, 28, 28)" fillcolor=orange]
	1401570837840 -> 1410754034288 [dir=none]
	1410754034288 [label="weight
 (256, 256, 1, 1)" fillcolor=orange]
	1401570837840 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401570837168 -> 1401570837840
	1401570837168 -> 1410754034192 [dir=none]
	1410754034192 [label="self
 (1, 256, 28, 28)" fillcolor=orange]
	1401570837168 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1401570836928 -> 1401570837168
	1401570836928 [label="CatBackward0
------------
dim: 1"]
	1401570836832 -> 1401570836928
	1401570836832 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (1, 1, 128, 784)"]
	1401570836688 -> 1401570836832
	1401570836688 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1401570836592 -> 1401570836688
	1401570836592 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (1, 784, 128)"]
	1401570852768 -> 1401570836592
	1401570852768 -> 1410753912592 [dir=none]
	1410753912592 [label="mat2
 (1, 784, 128)" fillcolor=orange]
	1401570852768 -> 1410753912880 [dir=none]
	1410753912880 [label="self
 (1, 784, 784)" fillcolor=orange]
	1401570852768 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	1401570852096 -> 1401570852768
	1401570852096 [label="ViewBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 784)"]
	1401571229504 -> 1401570852096
	1401571229504 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 784)"]
	1401571229312 -> 1401571229504
	1401571229312 -> 1410753913264 [dir=none]
	1410753913264 [label="result
 (1, 1, 784, 784)" fillcolor=orange]
	1401571229312 [label="SoftmaxBackward0
----------------------
dim   :     4294967295
result: [saved tensor]"]
	1401571229216 -> 1401571229312
	1401571229216 -> 1410754033040 [dir=none]
	1410754033040 [label="mask
 (1, 1, 784, 784)" fillcolor=orange]
	1401571229216 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	1401571229120 -> 1401571229216
	1401571229120 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (1, 784, 784)"]
	1401571228976 -> 1401571229120
	1401571228976 -> 1410753914032 [dir=none]
	1410753914032 [label="mat2
 (1, 16, 784)" fillcolor=orange]
	1401571228976 -> 1410753914320 [dir=none]
	1410753914320 [label="self
 (1, 784, 16)" fillcolor=orange]
	1401571228976 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	1401571226384 -> 1401571228976
	1401571226384 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (1, 1, 784, 16)"]
	1401571228688 -> 1401571226384
	1401571228688 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 784, 16)"]
	1401571228592 -> 1401571228688
	1401571228592 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1401571228496 -> 1401571228592
	1401571228496 -> 1410753914608 [dir=none]
	1410753914608 [label="other
 ()" fillcolor=orange]
	1401571228496 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	1401571228304 -> 1401571228496
	1401571228304 [label="ViewBackward0
----------------------------------
self_sym_sizes: (1, 1, 16, 28, 28)"]
	1401571228160 -> 1401571228304
	1401571228160 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 16, 28, 28)"]
	1401571228064 -> 1401571228160
	1401571228064 -> 1410754032944 [dir=none]
	1410754032944 [label="input
 (1, 258, 28, 28)" fillcolor=orange]
	1401571228064 -> 1410754032656 [dir=none]
	1410754032656 [label="weight
 (16, 258, 1, 1)" fillcolor=orange]
	1401571228064 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401571227920 -> 1401571228064
	1401571227920 [label="AddBackward0
------------
alpha: 1"]
	1401571227632 -> 1401571227920
	1401571227632 [label="CatBackward0
------------
dim: 1"]
	1410539321424 -> 1401571227632
	1401571227680 -> 1401571227920
	1401571227680 -> 1410754032752 [dir=none]
	1410754032752 [label="other
 (1, 258, 28, 28)" fillcolor=orange]
	1401571227680 -> 1410754032560 [dir=none]
	1410754032560 [label="self
 (1, 258, 28, 28)" fillcolor=orange]
	1401571227680 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1401571227536 -> 1401571227680
	1401571227536 [label="SplitBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 516, 28, 28)
split_size    :              258"]
	1401571227200 -> 1401571227536
	1401571227200 -> 1410754032176 [dir=none]
	1410754032176 [label="input
 (1, 516, 28, 28)" fillcolor=orange]
	1401571227200 -> 1410754032272 [dir=none]
	1410754032272 [label="weight
 (516, 516, 1, 1)" fillcolor=orange]
	1401571227200 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (516,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401571227056 -> 1401571227200
	1401571227056 -> 1410753916624 [dir=none]
	1410753916624 [label="result1
 (1, 516, 28, 28)" fillcolor=orange]
	1401571227056 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1401571226768 -> 1401571227056
	1401571226768 -> 1410754032368 [dir=none]
	1410754032368 [label="self
 (1, 516, 28, 28)" fillcolor=orange]
	1401571226768 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1401571226672 -> 1401571226768
	1401571226672 [label="CatBackward0
------------
dim: 1"]
	1401571226528 -> 1401571226672
	1401571226528 -> 1410754031888 [dir=none]
	1410754031888 [label="input
 (1, 516, 28, 28)" fillcolor=orange]
	1401571226528 -> 1410754032080 [dir=none]
	1410754032080 [label="weight
 (258, 516, 1, 1)" fillcolor=orange]
	1401571226528 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (258,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401571226240 -> 1401571226528
	1401571226240 -> 1410754031984 [dir=none]
	1410754031984 [label="self
 (1, 516, 28, 28)" fillcolor=orange]
	1401571226240 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1401571226000 -> 1401571226240
	1401571226000 [label="CatBackward0
------------
dim: 1"]
	1401571227632 -> 1401571226000
	1401571225808 -> 1401571226000
	1401571225808 [label=NegBackward0]
	1401571227632 -> 1401571225808
	1401571226336 -> 1401571226528
	1401571226336 -> 1401322327504 [dir=none]
	1401322327504 [label="g
 (258, 1, 1, 1)" fillcolor=orange]
	1401571226336 -> 1410753918448 [dir=none]
	1410753918448 [label="result1
 (258, 1, 1, 1)" fillcolor=orange]
	1401571226336 -> 1401322328176 [dir=none]
	1401322328176 [label="v
 (258, 516, 1, 1)" fillcolor=orange]
	1401571226336 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571225712 -> 1401571226336
	1401322328176 [label="ul_modules.11.in_proj_q.0.conv_1.weight_v
 (258, 516, 1, 1)" fillcolor=lightblue]
	1401322328176 -> 1401571225712
	1401571225712 [label=AccumulateGrad]
	1401571225904 -> 1401571226336
	1401322327504 [label="ul_modules.11.in_proj_q.0.conv_1.weight_g
 (258, 1, 1, 1)" fillcolor=lightblue]
	1401322327504 -> 1401571225904
	1401571225904 [label=AccumulateGrad]
	1401571226432 -> 1401571226528
	1401322327984 [label="ul_modules.11.in_proj_q.0.conv_1.bias
 (258)" fillcolor=lightblue]
	1401322327984 -> 1401571226432
	1401571226432 [label=AccumulateGrad]
	1401571226576 -> 1401571226672
	1401571226576 [label=NegBackward0]
	1401571226528 -> 1401571226576
	1401571227104 -> 1401571227200
	1401571227104 -> 1401322327888 [dir=none]
	1401322327888 [label="g
 (516, 1, 1, 1)" fillcolor=orange]
	1401571227104 -> 1410753920272 [dir=none]
	1410753920272 [label="result1
 (516, 1, 1, 1)" fillcolor=orange]
	1401571227104 -> 1401322328560 [dir=none]
	1401322328560 [label="v
 (516, 516, 1, 1)" fillcolor=orange]
	1401571227104 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571226960 -> 1401571227104
	1401322328560 [label="ul_modules.11.in_proj_q.0.conv_2.weight_v
 (516, 516, 1, 1)" fillcolor=lightblue]
	1401322328560 -> 1401571226960
	1401571226960 [label=AccumulateGrad]
	1401571226720 -> 1401571227104
	1401322327888 [label="ul_modules.11.in_proj_q.0.conv_2.weight_g
 (516, 1, 1, 1)" fillcolor=lightblue]
	1401322327888 -> 1401571226720
	1401571226720 [label=AccumulateGrad]
	1401571227344 -> 1401571227200
	1401322328368 [label="ul_modules.11.in_proj_q.0.conv_2.bias
 (516)" fillcolor=lightblue]
	1401322328368 -> 1401571227344
	1401571227344 [label=AccumulateGrad]
	1401571227488 -> 1401571227680
	1401571227488 -> 1410753913072 [dir=none]
	1410753913072 [label="result
 (1, 258, 28, 28)" fillcolor=orange]
	1401571227488 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1401571227536 -> 1401571227488
	1401571227968 -> 1401571228064
	1401571227968 -> 1401322328272 [dir=none]
	1401322328272 [label="g
 (16, 1, 1, 1)" fillcolor=orange]
	1401571227968 -> 1410956756368 [dir=none]
	1410956756368 [label="result1
 (16, 1, 1, 1)" fillcolor=orange]
	1401571227968 -> 1401322329136 [dir=none]
	1401322329136 [label="v
 (16, 258, 1, 1)" fillcolor=orange]
	1401571227968 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571226096 -> 1401571227968
	1401322329136 [label="ul_modules.11.in_proj_q.1.weight_v
 (16, 258, 1, 1)" fillcolor=lightblue]
	1401322329136 -> 1401571226096
	1401571226096 [label=AccumulateGrad]
	1401571227392 -> 1401571227968
	1401322328272 [label="ul_modules.11.in_proj_q.1.weight_g
 (16, 1, 1, 1)" fillcolor=lightblue]
	1401322328272 -> 1401571227392
	1401571227392 [label=AccumulateGrad]
	1401571228880 -> 1401571228064
	1401322328944 [label="ul_modules.11.in_proj_q.1.bias
 (16)" fillcolor=lightblue]
	1401322328944 -> 1401571228880
	1401571228880 [label=AccumulateGrad]
	1401571228928 -> 1401571228976
	1401571228928 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 1, 16, 784)"]
	1401571228448 -> 1401571228928
	1401571228448 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 16, 784)"]
	1401571228256 -> 1401571228448
	1401571228256 [label="ViewBackward0
----------------------------------
self_sym_sizes: (1, 1, 16, 28, 28)"]
	1401571227824 -> 1401571228256
	1401571227824 [label="ViewBackward0
-------------------------------
self_sym_sizes: (1, 16, 28, 28)"]
	1401571227008 -> 1401571227824
	1401571227008 [label="SplitWithSizesBackward0
--------------------------------
dim           :                1
self_sym_sizes: (1, 144, 28, 28)
split_sizes   :        (16, 128)"]
	1401571225760 -> 1401571227008
	1401571225760 -> 1410754031600 [dir=none]
	1410754031600 [label="input
 (1, 514, 28, 28)" fillcolor=orange]
	1401571225760 -> 1410754031312 [dir=none]
	1410754031312 [label="weight
 (144, 514, 1, 1)" fillcolor=orange]
	1401571225760 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (144,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401571226144 -> 1401571225760
	1401571226144 [label="AddBackward0
------------
alpha: 1"]
	1401571225424 -> 1401571226144
	1401571225424 [label="CatBackward0
------------
dim: 1"]
	1410753890672 -> 1401571225424
	1410539321424 -> 1401571225424
	1401571225568 -> 1401571226144
	1401571225568 -> 1410754031408 [dir=none]
	1410754031408 [label="other
 (1, 514, 28, 28)" fillcolor=orange]
	1401571225568 -> 1410754031216 [dir=none]
	1410754031216 [label="self
 (1, 514, 28, 28)" fillcolor=orange]
	1401571225568 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	1401571225472 -> 1401571225568
	1401571225472 [label="SplitBackward0
---------------------------------
dim           :                 1
self_sym_sizes: (1, 1028, 28, 28)
split_size    :               514"]
	1401571225088 -> 1401571225472
	1401571225088 -> 1410754030832 [dir=none]
	1410754030832 [label="input
 (1, 1028, 28, 28)" fillcolor=orange]
	1401571225088 -> 1410754030928 [dir=none]
	1410754030928 [label="weight
 (1028, 1028, 1, 1)" fillcolor=orange]
	1401571225088 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1028,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401571224848 -> 1401571225088
	1401571224848 -> 1410956759344 [dir=none]
	1410956759344 [label="result1
 (1, 1028, 28, 28)" fillcolor=orange]
	1401571224848 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	1401571224560 -> 1401571224848
	1401571224560 -> 1410754031024 [dir=none]
	1410754031024 [label="self
 (1, 1028, 28, 28)" fillcolor=orange]
	1401571224560 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1401571224464 -> 1401571224560
	1401571224464 [label="CatBackward0
------------
dim: 1"]
	1401571224320 -> 1401571224464
	1401571224320 -> 1410754030448 [dir=none]
	1410754030448 [label="input
 (1, 1028, 28, 28)" fillcolor=orange]
	1401571224320 -> 1410754030640 [dir=none]
	1410754030640 [label="weight
 (514, 1028, 1, 1)" fillcolor=orange]
	1401571224320 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (514,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (0, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	1401571224128 -> 1401571224320
	1401571224128 -> 1410754029968 [dir=none]
	1410754029968 [label="self
 (1, 1028, 28, 28)" fillcolor=orange]
	1401571224128 [label="EluBackward0
---------------------------
alpha      :            1.0
input_scale:              1
scale      :              1
self       : [saved tensor]"]
	1401571223888 -> 1401571224128
	1401571223888 [label="CatBackward0
------------
dim: 1"]
	1401571225424 -> 1401571223888
	1401571223648 -> 1401571223888
	1401571223648 [label=NegBackward0]
	1401571225424 -> 1401571223648
	1401571224176 -> 1401571224320
	1401571224176 -> 1401322325968 [dir=none]
	1401322325968 [label="g
 (514, 1, 1, 1)" fillcolor=orange]
	1401571224176 -> 1410956761072 [dir=none]
	1410956761072 [label="result1
 (514, 1, 1, 1)" fillcolor=orange]
	1401571224176 -> 1401322326832 [dir=none]
	1401322326832 [label="v
 (514, 1028, 1, 1)" fillcolor=orange]
	1401571224176 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571223552 -> 1401571224176
	1401322326832 [label="ul_modules.11.in_proj_kv.0.conv_1.weight_v
 (514, 1028, 1, 1)" fillcolor=lightblue]
	1401322326832 -> 1401571223552
	1401571223552 [label=AccumulateGrad]
	1401571223696 -> 1401571224176
	1401322325968 [label="ul_modules.11.in_proj_kv.0.conv_1.weight_g
 (514, 1, 1, 1)" fillcolor=lightblue]
	1401322325968 -> 1401571223696
	1401571223696 [label=AccumulateGrad]
	1401571224272 -> 1401571224320
	1401322326640 [label="ul_modules.11.in_proj_kv.0.conv_1.bias
 (514)" fillcolor=lightblue]
	1401322326640 -> 1401571224272
	1401571224272 [label=AccumulateGrad]
	1401571224368 -> 1401571224464
	1401571224368 [label=NegBackward0]
	1401571224320 -> 1401571224368
	1401571225040 -> 1401571225088
	1401571225040 -> 1401322326544 [dir=none]
	1401322326544 [label="g
 (1028, 1, 1, 1)" fillcolor=orange]
	1401571225040 -> 1410956762704 [dir=none]
	1410956762704 [label="result1
 (1028, 1, 1, 1)" fillcolor=orange]
	1401571225040 -> 1401322327216 [dir=none]
	1401322327216 [label="v
 (1028, 1028, 1, 1)" fillcolor=orange]
	1401571225040 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571224800 -> 1401571225040
	1401322327216 [label="ul_modules.11.in_proj_kv.0.conv_2.weight_v
 (1028, 1028, 1, 1)" fillcolor=lightblue]
	1401322327216 -> 1401571224800
	1401571224800 [label=AccumulateGrad]
	1401571224512 -> 1401571225040
	1401322326544 [label="ul_modules.11.in_proj_kv.0.conv_2.weight_g
 (1028, 1, 1, 1)" fillcolor=lightblue]
	1401322326544 -> 1401571224512
	1401571224512 [label=AccumulateGrad]
	1401571225184 -> 1401571225088
	1401322327024 [label="ul_modules.11.in_proj_kv.0.conv_2.bias
 (1028)" fillcolor=lightblue]
	1401322327024 -> 1401571225184
	1401571225184 [label=AccumulateGrad]
	1401571225376 -> 1401571225568
	1401571225376 -> 1410956763856 [dir=none]
	1410956763856 [label="result
 (1, 514, 28, 28)" fillcolor=orange]
	1401571225376 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1401571225472 -> 1401571225376
	1401571226864 -> 1401571225760
	1401571226864 -> 1401322326928 [dir=none]
	1401322326928 [label="g
 (144, 1, 1, 1)" fillcolor=orange]
	1401571226864 -> 1410956764432 [dir=none]
	1410956764432 [label="result1
 (144, 1, 1, 1)" fillcolor=orange]
	1401571226864 -> 1401322327792 [dir=none]
	1401322327792 [label="v
 (144, 514, 1, 1)" fillcolor=orange]
	1401571226864 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401571223936 -> 1401571226864
	1401322327792 [label="ul_modules.11.in_proj_kv.1.weight_v
 (144, 514, 1, 1)" fillcolor=lightblue]
	1401322327792 -> 1401571223936
	1401571223936 [label=AccumulateGrad]
	1401571225280 -> 1401571226864
	1401322326928 [label="ul_modules.11.in_proj_kv.1.weight_g
 (144, 1, 1, 1)" fillcolor=lightblue]
	1401322326928 -> 1401571225280
	1401571225280 [label=AccumulateGrad]
	1401571228784 -> 1401571225760
	1401322327600 [label="ul_modules.11.in_proj_kv.1.bias
 (144)" fillcolor=lightblue]
	1401322327600 -> 1401571228784
	1401571228784 [label=AccumulateGrad]
	1401570836784 -> 1401570852768
	1401570836784 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 128)"]
	1401571229264 -> 1401570836784
	1401571229264 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (1, 1, 784, 128)"]
	1401571229024 -> 1401571229264
	1401571229024 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	1401571228544 -> 1401571229024
	1401571228544 [label="ViewBackward0
-----------------------------------
self_sym_sizes: (1, 1, 128, 28, 28)"]
	1401571228016 -> 1401571228544
	1401571228016 [label="ViewBackward0
--------------------------------
self_sym_sizes: (1, 128, 28, 28)"]
	1401571227008 -> 1401571228016
	1401570836880 -> 1401570836928
	1401570836880 [label=NegBackward0]
	1401570836832 -> 1401570836880
	1401570837504 -> 1401570837840
	1401570837504 -> 1401322329232 [dir=none]
	1401322329232 [label="g
 (256, 1, 1, 1)" fillcolor=orange]
	1401570837504 -> 1410956766640 [dir=none]
	1410956766640 [label="result1
 (256, 1, 1, 1)" fillcolor=orange]
	1401570837504 -> 1401322329904 [dir=none]
	1401322329904 [label="v
 (256, 256, 1, 1)" fillcolor=orange]
	1401570837504 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570836544 -> 1401570837504
	1401322329904 [label="ul_modules.11.out_proj.conv_1_sc.weight_v
 (256, 256, 1, 1)" fillcolor=lightblue]
	1401322329904 -> 1401570836544
	1401570836544 [label=AccumulateGrad]
	1401570837024 -> 1401570837504
	1401322329232 [label="ul_modules.11.out_proj.conv_1_sc.weight_g
 (256, 1, 1, 1)" fillcolor=lightblue]
	1401322329232 -> 1401570837024
	1401570837024 [label=AccumulateGrad]
	1401570837552 -> 1401570837840
	1401322329712 [label="ul_modules.11.out_proj.conv_1_sc.bias
 (256)" fillcolor=lightblue]
	1401322329712 -> 1401570837552
	1401570837552 [label=AccumulateGrad]
	1401570837984 -> 1401570838032
	1401570837984 [label=NegBackward0]
	1401570837936 -> 1401570837984
	1401570838272 -> 1401570839664
	1401570838272 -> 1401322329616 [dir=none]
	1401322329616 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	1401570838272 -> 1410956768272 [dir=none]
	1410956768272 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	1401570838272 -> 1401322330288 [dir=none]
	1401322330288 [label="v
 (512, 512, 1, 1)" fillcolor=orange]
	1401570838272 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570838752 -> 1401570838272
	1401322330288 [label="ul_modules.11.out_proj.conv_2.weight_v
 (512, 512, 1, 1)" fillcolor=lightblue]
	1401322330288 -> 1401570838752
	1401570838752 [label=AccumulateGrad]
	1401570838080 -> 1401570838272
	1401322329616 [label="ul_modules.11.out_proj.conv_2.weight_g
 (512, 1, 1, 1)" fillcolor=lightblue]
	1401322329616 -> 1401570838080
	1401570838080 [label=AccumulateGrad]
	1401570841824 -> 1401570839664
	1401322330096 [label="ul_modules.11.out_proj.conv_2.bias
 (512)" fillcolor=lightblue]
	1401322330096 -> 1401570841824
	1401570841824 [label=AccumulateGrad]
	1401570838224 -> 1410539320272
	1401570838224 -> 1410956769424 [dir=none]
	1410956769424 [label="result
 (1, 256, 28, 28)" fillcolor=orange]
	1401570838224 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	1401570840192 -> 1401570838224
	1401709432832 -> 1401571653904
	1401709432832 -> 1401322330000 [dir=none]
	1401322330000 [label="g
 (40, 1, 1, 1)" fillcolor=orange]
	1401709432832 -> 1410956770000 [dir=none]
	1410956770000 [label="result1
 (40, 1, 1, 1)" fillcolor=orange]
	1401709432832 -> 1401322330864 [dir=none]
	1401322330864 [label="v
 (40, 256, 1, 1)" fillcolor=orange]
	1401709432832 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	1401570838656 -> 1401709432832
	1401322330864 [label="output_conv.weight_v
 (40, 256, 1, 1)" fillcolor=lightblue]
	1401322330864 -> 1401570838656
	1401570838656 [label=AccumulateGrad]
	1401570837456 -> 1401709432832
	1401322330000 [label="output_conv.weight_g
 (40, 1, 1, 1)" fillcolor=lightblue]
	1401322330000 -> 1401570837456
	1401570837456 [label=AccumulateGrad]
	1410539320368 -> 1401571653904
	1401322330672 [label="output_conv.bias
 (40)" fillcolor=lightblue]
	1401322330672 -> 1410539320368
	1410539320368 [label=AccumulateGrad]
	1401571653904 -> 1410754033712
}
