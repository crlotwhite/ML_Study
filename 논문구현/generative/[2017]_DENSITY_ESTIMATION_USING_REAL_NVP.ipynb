{"cells":[{"cell_type":"code","execution_count":37,"id":"eb2b36f8-5b36-48ca-b5c9-b6292e4071fc","metadata":{},"outputs":[],"source":["import os\n","import tqdm\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","import matplotlib.pyplot as plt\n","from enum import IntEnum\n","from dataclasses import dataclass\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms"]},{"cell_type":"code","execution_count":38,"id":"821a5b44-e539-4d00-a175-c5e81fac967d","metadata":{},"outputs":[],"source":["@dataclass\n","class Hyperparameters:\n","    learning_rate: float = 0.001\n","    batch_size: int = 32\n","    num_epochs: int = 10\n","    dropout_rate: float = 0.5\n","    num_workers: int = 4\n","    device: str = 'cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu'\n","\n","hp = Hyperparameters()\n"]},{"cell_type":"code","execution_count":39,"id":"9dbf70d8-4787-48a8-bdef-e4e60a0f6347","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]}],"source":["transform_train = transforms.Compose([\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor()\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor()\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform_train)\n","trainloader = DataLoader(trainset, batch_size=hp.batch_size, shuffle=True, num_workers=hp.num_workers)\n","\n","testset = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform_test)\n","testloader = DataLoader(testset, batch_size=hp.batch_size, shuffle=False, num_workers=hp.num_workers)\n"]},{"cell_type":"code","execution_count":40,"id":"c34ec483-7233-46bb-9ede-c01d6c777983","metadata":{},"outputs":[],"source":["class ResNet(nn.Module):\n","    class ResidualBlock(nn.Module):\n","        def __init__(self, in_channels, out_channels):\n","            super(ResNet.ResidualBlock, self).__init__()\n","            \n","            self.model = nn.Sequential(*[\n","                nn.BatchNorm2d(in_channels),\n","                nn.ReLU(),  \n","                nn.utils.weight_norm(nn.Conv2d(in_channels,\n","                                               out_channels, \n","                                               kernel_size=3, \n","                                               padding=1, \n","                                               bias=False)),\n","                nn.BatchNorm2d(out_channels),\n","                nn.ReLU(),\n","                nn.utils.weight_norm(nn.Conv2d(out_channels, \n","                                               out_channels,\n","                                               kernel_size=3, \n","                                               padding=1, \n","                                               bias=True))\n","            ])\n","            \n","        def forward(self, x):\n","            return x + self.model(x)\n","    \n","    def __init__(self, in_channels, mid_channels, out_channels, num_blocks,\n","                 kernel_size, padding, double_after_norm):\n","        super(ResNet, self).__init__()\n","        \n","        self.head_norm = nn.BatchNorm2d(in_channels)\n","        self.double_after_norm = double_after_norm\n","\n","        self.head_conv = nn.utils.weight_norm(nn.Conv2d(2*in_channels, \n","                                                        mid_channels, \n","                                                        kernel_size, \n","                                                        padding=padding, \n","                                                        bias=True))\n","        self.head_skip = nn.utils.weight_norm(nn.Conv2d(mid_channels,\n","                                                        mid_channels,\n","                                                        kernel_size=1,\n","                                                        padding=0,\n","                                                        bias=True))\n","        \n","        self.blocks = nn.ModuleList([self.ResidualBlock(mid_channels, mid_channels)\n","                                     for _ in range(num_blocks)])\n","        self.skips = nn.ModuleList([nn.utils.weight_norm(nn.Conv2d(mid_channels,\n","                                                                   mid_channels,\n","                                                                   kernel_size=1,\n","                                                                   padding=0,\n","                                                                   bias=True))\n","                                   for _ in range(num_blocks)])\n","        \n","        \n","        self.tail_norm = nn.BatchNorm2d(mid_channels)\n","        self.tail_conv = nn.utils.weight_norm(nn.Conv2d(mid_channels, \n","                                                        out_channels, \n","                                                        kernel_size=1, \n","                                                        padding=0, \n","                                                        bias=True))\n","        \n","    def forward(self, x):\n","        x = self.head_norm(x)\n","        if self.double_after_norm:\n","            x *= 2.0\n","\n","        x = torch.cat((x, -x), dim=1) # Unet의 디코딩 과정과 비슷해보임\n","        x = F.relu(x)\n","        \n","        x = self.head_conv(x)\n","        x_skip = self.head_skip(x)\n","        \n","        for block, skip in zip(self.blocks, self.skips):\n","            x = block(x)\n","            x_skip += skip(x)\n","        \n","        x = self.tail_norm(x_skip)\n","        x = F.relu(x)\n","        x = self.tail_conv(x)\n","        \n","        return x\n"]},{"cell_type":"markdown","id":"0e6b1902-5e77-4c3d-822d-f7b92a2e63d2","metadata":{},"source":["![](https://raw.githubusercontent.com/crlotwhite/ML_Study/main/%EB%85%BC%EB%AC%B8%EA%B5%AC%ED%98%84/generative/assets/Cap%202024-08-15%2001-45-54-178.png)"]},{"cell_type":"markdown","id":"79a46b8a-9e19-45e8-9e58-45c9b4c0b585","metadata":{},"source":["![](https://raw.githubusercontent.com/crlotwhite/ML_Study/main/%EB%85%BC%EB%AC%B8%EA%B5%AC%ED%98%84/generative/assets/Cap%202024-08-15%2003-25-01-809.png)\n","\n","왼쪽: 체크 무늬  \n","오른쪽: Channel-wise"]},{"cell_type":"code","execution_count":41,"id":"5553550d-0ea3-4986-aec0-e567df93a55a","metadata":{},"outputs":[],"source":["class CouplingLayer(nn.Module):\n","    class MaskType(IntEnum):\n","        CHECKERBOARD = 0\n","        CHANNEL_WISE = 1\n","        \n","    class Rescale(nn.Module):\n","        def __init__(self, num_channels):\n","            super(CouplingLayer.Rescale, self).__init__()\n","            \n","            self.weight = nn.Parameter(torch.ones(num_channels, 1, 1))\n","        \n","        def forward(self, x):\n","            return x * self.weight\n","        \n","    def __init__(self, in_channels, mid_channels, \n","                 num_blocks, mask_type, reverse_mask):\n","        super(CouplingLayer, self).__init__()\n","        \n","        self.mask_type = mask_type\n","        self.reverse_mask = reverse_mask\n","        \n","        if self.mask_type == self.MaskType.CHANNEL_WISE:\n","            in_channels //= 2\n","        \n","        self.st_net = ResNet(in_channels, mid_channels, 2*in_channels, \n","                             num_blocks=num_blocks, kernel_size=3, padding=1,\n","                             double_after_norm=(self.mask_type == self.MaskType.CHECKERBOARD))\n","        \n","        self.rescale = nn.utils.weight_norm(self.Rescale(in_channels))\n","        \n","    def forward(self, x, sldj=None, reverse=True):\n","        if self.mask_type == self.MaskType.CHECKERBOARD:\n","            b = self.checkerboard_mask(x.size(2), x.size(3), self.reverse_mask, device=x.device)\n","            x_b = x * b\n","            st = self.st_net(x_b)\n","            s, t = st.chunk(2, dim=1)\n","            s = self.rescale(torch.tanh(s))\n","            s = s * (1 - b)\n","            t = t * (1 - b)\n","            \n","            if reverse:\n","                inv_exp_s = s.mul(-1).exp()\n","                x = x * inv_exp_s - t\n","            else:\n","                exp_s = s.exp()\n","                x = (x + t) * exp_s\n","                \n","                sldj += s.contiguous().view(s.size(0), -1).sum(-1) # sldj = log|det(dz/dx)|\n","        else:\n","            if self.reverse_mask:\n","                x_id, x_change = x.chunk(2, dim=1)\n","            else:\n","                x_change, x_id = x.chunk(2, dim=1)\n","            \n","            st = self.st_net(x_id)\n","            s, t = st.chunk(2, dim=1)\n","            s = self.rescale(torch.tanh(s))\n","            \n","            if reverse:\n","                inv_exp_s = s.mul(-1).exp()\n","                x_change = x_change * inv_exp_s - t\n","            else:\n","                exp_s = s.exp()\n","                x_change = (x_change + t) * exp_s\n","                \n","                sldj += s.contiguous().view(s.size(0), -1).sum(-1)\n","            \n","            if self.reverse_mask:\n","                x = torch.cat((x_id, x_change), dim=1)\n","            else:\n","                x = torch.cat((x_change, x_id), dim=1)\n","                \n","        return x, sldj\n","        \n","    def checkerboard_mask(self, height, width, reverse=False, \n","                          dtype=torch.float32, device=None, \n","                          requires_grad=False):\n","        checkerboard = [[((i % 2) + j) % 2 for j in range(width)] for i in range(height)]\n","        mask = torch.tensor(checkerboard, dtype=dtype, device=device, requires_grad=requires_grad)\n","\n","        if reverse:\n","            mask = 1 - mask\n","\n","        mask = mask.view(1, 1, height, width)\n","        return mask"]},{"cell_type":"code","execution_count":42,"id":"e187f058-2366-48d8-8965-5aae225734e8","metadata":{},"outputs":[],"source":["# ref: https://github.com/tensorflow/models/blob/master/research/real_nvp/real_nvp_utils.py\n","def squeeze_2x2(x, reverse=False, alt_order=False):\n","    block_size = 2\n","    if alt_order:\n","        n, c, h, w = x.size()\n","\n","        if reverse:\n","            c //= 4\n","        \n","        squeeze_matrix = torch.tensor([[[[1., 0.], [0., 0.]]],\n","                                       [[[0., 0.], [0., 1.]]],\n","                                       [[[0., 1.], [0., 0.]]],\n","                                       [[[0., 0.], [1., 0.]]]],\n","                                      dtype=x.dtype,\n","                                      device=x.device)\n","        perm_weight = torch.zeros((4 * c, c, 2, 2), dtype=x.dtype, device=x.device)\n","        for c_idx in range(c):\n","            slice_0 = slice(c_idx * 4, (c_idx + 1) * 4)\n","            slice_1 = slice(c_idx, c_idx + 1)\n","            perm_weight[slice_0, slice_1, :, :] = squeeze_matrix\n","        shuffle_channels = torch.tensor([c_idx * 4 for c_idx in range(c)]\n","                                        + [c_idx * 4 + 1 for c_idx in range(c)]\n","                                        + [c_idx * 4 + 2 for c_idx in range(c)]\n","                                        + [c_idx * 4 + 3 for c_idx in range(c)])\n","        perm_weight = perm_weight[shuffle_channels, :, :, :]\n","\n","        if reverse:\n","            x = F.conv_transpose2d(x, perm_weight, stride=2)\n","        else:\n","            x = F.conv2d(x, perm_weight, stride=2)\n","    else:\n","        b, c, h, w = x.size()\n","        x = x.permute(0, 2, 3, 1)\n","\n","        if reverse:\n","            x = x.view(b, h, w, c // 4, 2, 2)\n","            x = x.permute(0, 1, 4, 2, 5, 3)\n","            x = x.contiguous().view(b, 2 * h, 2 * w, c // 4)\n","        else:\n","            x = x.view(b, h // 2, 2, w // 2, 2, c)\n","            x = x.permute(0, 1, 3, 5, 2, 4)\n","            x = x.contiguous().view(b, h // 2, w // 2, c * 4)\n","\n","        x = x.permute(0, 3, 1, 2)\n","\n","    return x"]},{"cell_type":"code","execution_count":43,"id":"f8ef1fef-181b-40ca-ad59-3e84e7559342","metadata":{},"outputs":[],"source":["class RealNVP(nn.Module):\n","    class RecursiveBlock(nn.Module):\n","        def __init__(self, scale_idx, num_scales, in_channels, mid_channels, num_blocks):\n","            super(RealNVP.RecursiveBlock, self).__init__()\n","            \n","            self.is_last_block = scale_idx == num_scales - 1\n","            self.in_couplings = nn.ModuleList([\n","                CouplingLayer(in_channels, mid_channels, num_blocks, CouplingLayer.MaskType.CHECKERBOARD, False),\n","                CouplingLayer(in_channels, mid_channels, num_blocks, CouplingLayer.MaskType.CHECKERBOARD, True),\n","                CouplingLayer(in_channels, mid_channels, num_blocks, CouplingLayer.MaskType.CHECKERBOARD, False)\n","            ])\n","            \n","            if self.is_last_block:\n","                self.in_couplings.append(\n","                    CouplingLayer(in_channels, mid_channels, num_blocks, CouplingLayer.MaskType.CHECKERBOARD, True))\n","            else:\n","                self.out_couplings = nn.ModuleList([\n","                    CouplingLayer(4 * in_channels, 2 * mid_channels, num_blocks, CouplingLayer.MaskType.CHANNEL_WISE, False),\n","                    CouplingLayer(4 * in_channels, 2 * mid_channels, num_blocks, CouplingLayer.MaskType.CHANNEL_WISE, True),\n","                    CouplingLayer(4 * in_channels, 2 * mid_channels, num_blocks, CouplingLayer.MaskType.CHANNEL_WISE, False)\n","                ])\n","                self.next_block = RealNVP.RecursiveBlock(scale_idx + 1, num_scales, 2 * in_channels, 2 * mid_channels, num_blocks)\n","                \n","        def forward(self, x, sldj=None, reverse=False):\n","            if reverse:\n","                if not self.is_last_block:\n","                    x = squeeze_2x2(x, reverse=False, alt_order=True)\n","                    x, x_split = x.chunk(2, dim=1)\n","                    x, sldj = self.next_block(x, sldj, reverse)\n","                    x = torch.cat((x, x_split), dim=1)\n","                    x = squeeze_2x2(x, reverse=True, alt_order=True)\n","                    \n","                    x = squeeze_2x2(x, reverse=False)\n","                    for coupling in reversed(self.out_couplings):\n","                        x, sldj = coupling(x, sldj, reverse)\n","                    x = squeeze_2x2(x, reverse=True)\n","                    \n","                for coupling in reversed(self.in_couplings):\n","                    x, sldj = coupling(x, sldj, reverse)\n","            else:\n","                for coupling in self.in_couplings:\n","                    x, sldj = coupling(x, sldj, reverse)\n","                \n","                if not self.is_last_block:\n","                    x = squeeze_2x2(x, reverse=False)\n","                    for coupling in self.out_couplings:\n","                        x, sldj = coupling(x, sldj, reverse)\n","                    x = squeeze_2x2(x, reverse=True)\n","                    \n","                    x = squeeze_2x2(x, reverse=False, alt_order=True)\n","                    x, x_split = x.chunk(2, dim=1)\n","                    x, sldj = self.next_block(x, sldj, reverse)\n","                    x = torch.cat((x, x_split), dim=1)\n","                    x = squeeze_2x2(x, reverse=True, alt_order=True)\n","            \n","            return x, sldj\n","                    \n","    def __init__(self, num_scales=2, in_channels=3, mid_channels=64, num_blocks=8):\n","        super(RealNVP, self).__init__()\n","        \n","        self.register_buffer('data_constraint', torch.tensor([0.9], dtype=torch.float32))\n","        \n","        self.flows = self.RecursiveBlock(0, num_scales, in_channels, mid_channels, num_blocks)\n","        \n","    def forward(self, x, reverse=False):\n","        sldj = None\n","        if not reverse:\n","            x, sldj = self.pre_process(x)\n","        \n","        x, sldj = self.flows(x, sldj, reverse)\n","        \n","        return x, sldj\n","            \n","    def pre_process(self, x):\n","        y = (x * 255. + torch.rand_like(x)) / 256.\n","        y = (y * 2 - 1) * self.data_constraint\n","        y = (y + 1) / 2\n","        y = y.log() - (1 - y).log()\n","        \n","        ldj = F.softplus(y) + F.softplus(-y) \\\n","            - F.softplus((1 - self.data_constraint).log() - self.data_constraint.log())\n","        sldj = ldj.view(ldj.size(0), -1).sum(-1)\n","        \n","        return y, sldj"]},{"cell_type":"markdown","id":"f02b08b6-fbff-4b7e-85f5-4124ce34b1fe","metadata":{},"source":["경계 효과를 줄이기 위해 특수한 방식으로 밀도를 모델링  \n","\n","밀도 함수 변환: 밀도를 모델링할 때, 다음과 같은 변환을 사용합니다\n","$$ logit(\\alpha + (1 - \\alpha)\\frac{x}{256}) $$\n","\n","- a: 0.05\n","- x: 원본 픽셀\n","\n","최적화: \n","- ADAM 최적화 방법을 사용하며 기본 하이퍼파라미터를 그대로 설정\n","- L2 정규화를 사용하여 가중치의 크기 매개변수를 조정\n","- 사전 분포 $P_z$는 등방성 유닛 노름 가우시안으로 설정"]},{"cell_type":"code","execution_count":44,"id":"6e15f042-24cf-47f0-9ba9-01e2adc561ac","metadata":{},"outputs":[],"source":["class RealNVPLoss(nn.Module):\n","    def __init__(self, k=256):\n","        super(RealNVPLoss, self).__init__()\n","        self.k = k\n","        \n","    def forward(self, z, sldj):\n","        prior_ll = -0.5 * (z ** 2 + np.log(2 * np.pi))\n","        prior_ll = prior_ll.contiguous().view(z.size(0), -1).sum(-1) - np.log(self.k) * np.prod(z.size()[1:])\n","        ll = prior_ll + sldj\n","        nll = -ll.mean()\n","        \n","        return nll\n","        "]},{"cell_type":"code","execution_count":45,"id":"b3fef631-8ad2-4306-93d5-900f27ab60f8","metadata":{},"outputs":[],"source":["model = RealNVP(num_scales=2, in_channels=3, mid_channels=64, num_blocks=8).to(hp.device)\n","loss_fn = RealNVPLoss()\n","\n","norm_params = []\n","unnorm_params = []\n","for n, p in model.named_parameters():\n","    if n.endswith('weight_g'):\n","        norm_params.append(p)\n","    else:\n","        unnorm_params.append(p)\n","\n","param_groups = [{'name': 'normalized', 'params': norm_params, 'weight_decay': 5e-5},\n","                {'name': 'unnormalized', 'params': unnorm_params}]\n","\n","\n","optimizer = optim.Adam(param_groups, lr=1e-3)"]},{"cell_type":"code","execution_count":46,"id":"8fe261b2-061c-4cbc-a5a4-2c8cfd8eaf36","metadata":{},"outputs":[],"source":["class AverageMeter(object):\n","    \"\"\"Computes and stores the average and current value.\n","\n","    Adapted from: https://github.com/pytorch/examples/blob/master/imagenet/train.py\n","    \"\"\"\n","    def __init__(self):\n","        self.val = 0.\n","        self.avg = 0.\n","        self.sum = 0.\n","        self.count = 0.\n","\n","    def reset(self):\n","        self.val = 0.\n","        self.avg = 0.\n","        self.sum = 0.\n","        self.count = 0.\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"]},{"cell_type":"code","execution_count":47,"id":"ec63fb95-9df1-48b1-96a6-9dd339060128","metadata":{},"outputs":[],"source":["def sample(model, batch_size, device):\n","    z = torch.randn((batch_size, 3, 32, 32), dtype=torch.float32, device=device)\n","    x, _ = model(z, reverse=True)\n","    x = torch.sigmoid(x)\n","    \n","    return x"]},{"cell_type":"code","execution_count":48,"id":"ea32622b-1e26-445c-8b68-395530dc574b","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 50000/50000 [06:31<00:00, 127.74it/s, bpd=4.66, loss=9.93e+3]\n","100%|██████████| 10000/10000 [00:27<00:00, 360.47it/s, bpd=4.43, loss=9.44e+3]\n","100%|██████████| 50000/50000 [06:27<00:00, 129.07it/s, bpd=4.18, loss=8.91e+3]\n","100%|██████████| 10000/10000 [00:26<00:00, 371.12it/s, bpd=6.93, loss=1.48e+4]\n","100%|██████████| 50000/50000 [06:25<00:00, 129.61it/s, bpd=4.04, loss=8.61e+3]\n","100%|██████████| 10000/10000 [00:26<00:00, 375.10it/s, bpd=10.1, loss=2.14e+4]\n","100%|██████████| 50000/50000 [06:23<00:00, 130.30it/s, bpd=3.95, loss=8.42e+3]\n","100%|██████████| 10000/10000 [00:26<00:00, 374.17it/s, bpd=8.24, loss=1.75e+4]\n","100%|██████████| 50000/50000 [06:27<00:00, 129.01it/s, bpd=3.91, loss=8.32e+3]\n","100%|██████████| 10000/10000 [00:27<00:00, 369.35it/s, bpd=4.13, loss=8.8e+3]\n","100%|██████████| 50000/50000 [06:29<00:00, 128.53it/s, bpd=3.86, loss=8.22e+3]\n","100%|██████████| 10000/10000 [00:26<00:00, 372.33it/s, bpd=4.01, loss=8.55e+3]\n","100%|██████████| 50000/50000 [06:27<00:00, 129.19it/s, bpd=3.83, loss=8.16e+3]\n","100%|██████████| 10000/10000 [00:27<00:00, 369.51it/s, bpd=3.78, loss=8.04e+3]\n","100%|██████████| 50000/50000 [06:26<00:00, 129.27it/s, bpd=3.81, loss=8.1e+3] \n","100%|██████████| 10000/10000 [00:26<00:00, 372.12it/s, bpd=3.73, loss=7.94e+3]\n","100%|██████████| 50000/50000 [06:26<00:00, 129.44it/s, bpd=3.76, loss=8.01e+3]\n","100%|██████████| 10000/10000 [00:26<00:00, 371.62it/s, bpd=3.87, loss=8.24e+3]\n","100%|██████████| 50000/50000 [06:26<00:00, 129.27it/s, bpd=3.75, loss=7.99e+3]\n","100%|██████████| 10000/10000 [00:27<00:00, 367.85it/s, bpd=3.73, loss=7.95e+3]\n"]}],"source":["best_loss = 0.0\n","\n","for epoch in range(hp.num_epochs):\n","    model.train()\n","    loss_meter = AverageMeter()\n","    with tqdm.tqdm(total=len(trainloader.dataset)) as pbar:\n","        for x, _ in trainloader:\n","            x = x.to(hp.device)\n","            optimizer.zero_grad()\n","            \n","            z, sldj = model(x, reverse=False)\n","            loss = loss_fn(z, sldj)\n","            loss_meter.update(loss.item(), x.size(0))\n","            loss.backward()\n","            for group in optimizer.param_groups:\n","                torch.nn.utils.clip_grad_norm_(group['params'], 100.0, 2)\n","            \n","            optimizer.step()\n","            \n","            pbar.set_postfix(loss=loss_meter.avg, \n","                             bpd=loss_meter.avg / (np.log(2) * np.prod(x.size()[1:])))\n","            pbar.update(x.size(0))\n","            \n","    model.eval()\n","    loss_meter = AverageMeter()\n","    with torch.no_grad():\n","        with tqdm.tqdm(total=len(testloader.dataset)) as pbar:\n","            for x, _ in testloader:\n","                x = x.to(hp.device)\n","                \n","                z, sldj = model(x, reverse=False)\n","                loss = loss_fn(z, sldj)\n","                loss_meter.update(loss.item(), x.size(0))\n","                \n","                pbar.set_postfix(loss=loss_meter.avg, \n","                                 bpd=loss_meter.avg / (np.log(2) * np.prod(x.size()[1:])))\n","                pbar.update(x.size(0))\n","        \n","        if loss_meter.avg < best_loss:\n","            best_loss = loss_meter.avg\n","        \n","        images = sample(model, 64, hp.device)\n","        os.makedirs('samples', exist_ok=True)\n","        images_concat = torchvision.utils.make_grid(images, nrow=8, padding=2, pad_value=255)\n","        torchvision.utils.save_image(images_concat, f'samples/{str(epoch).zfill(3)}.png')\n","    "]},{"cell_type":"markdown","id":"807a8a9f-68b9-4d3c-8417-cc79ea126412","metadata":{},"source":["** result **\n","\n","![](https://raw.githubusercontent.com/crlotwhite/ML_Study/main/%EB%85%BC%EB%AC%B8%EA%B5%AC%ED%98%84/generative/assets/realnvp_result.gif)"]}],"metadata":{"kernelspec":{"display_name":"dl","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":5}
