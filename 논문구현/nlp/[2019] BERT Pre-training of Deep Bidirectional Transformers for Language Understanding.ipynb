{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, DataCollatorForLanguageModeling\n",
    "import numpy as np\n",
    "from random import randrange, randint\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 로드\n",
    "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train[:1%]')\n",
    "\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "train_set = train_test_split['train']\n",
    "test_set = train_test_split['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "856fc60ae31048c99ffd02192e8ddb8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/293 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc684ee532f4e609a2711a03bc33e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/74 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 토크나이즈\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenized_train_set = train_set.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_test_set = test_set.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성\n",
    "def create_mlm_and_nsp_dataset(dataset, tokenizer, max_length=512, nsp_probability=0.5):\n",
    "    examples = []\n",
    "    for i in range(len(dataset['input_ids']) - 1):\n",
    "        # Next Sentence Prediction (NSP)\n",
    "        # 두 문장이 실제로 이어지는지 여부는 50% 비율로 참인 문장과 랜덤하게 추출되어 거짓인 문장의 비율로 구성\n",
    "        if random.random() < nsp_probability:\n",
    "            is_next = 1\n",
    "            next_sentence = dataset['input_ids'][i + 1]\n",
    "        else:\n",
    "            is_next = 0\n",
    "            next_sentence = random.choice(dataset['input_ids'])\n",
    "        \n",
    "        # current + next sentence\n",
    "        input_ids = dataset['input_ids'][i] + next_sentence\n",
    "        attention_mask = dataset['attention_mask'][i] + dataset['attention_mask'][i + 1]\n",
    "        token_type_ids = [0] * len(dataset['input_ids'][i]) + [1] * len(next_sentence)\n",
    "\n",
    "        input_ids = input_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        token_type_ids = token_type_ids[:max_length]\n",
    "\n",
    "        # Masked Language Modeling (MLM)\n",
    "        # 마스킹은 전체 단어의 15% 정도만 진행\n",
    "        # 모든 토큰을 마스킹 하는게 아니라 80% 정도만 <MASK>로 처리\n",
    "        # 10%는 랜덤한 단어, 나머지 10%는 정상적인 단어를 그대로 둔다.\n",
    "        labels = input_ids.copy()\n",
    "        for j in range(len(input_ids)):\n",
    "            if random.random() < 0.15:\n",
    "                if random.random() < 0.8:\n",
    "                    input_ids[j] = tokenizer.mask_token_id\n",
    "                elif random.random() < 0.5:\n",
    "                    input_ids[j] = random.randint(0, tokenizer.vocab_size - 1)\n",
    "            else:\n",
    "                labels[j] = -100\n",
    "\n",
    "        examples.append({\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'token_type_ids': token_type_ids,\n",
    "            'labels': labels,\n",
    "            'next_sentence_label': is_next\n",
    "        })\n",
    "\n",
    "    return examples\n",
    "\n",
    "mlm_nsp_train_set = create_mlm_and_nsp_dataset(tokenized_train_set, tokenizer)\n",
    "mlm_nsp_test_set = create_mlm_and_nsp_dataset(tokenized_test_set, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset:\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: np.array(value) for key, value in self.examples[idx].items()}\n",
    "\n",
    "# 데이터셋 생성\n",
    "train_dataset = BertDataset(mlm_nsp_train_set)\n",
    "test_dataset = BertDataset(mlm_nsp_test_set)\n",
    "\n",
    "# 데이터 로더 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    vocab_size: int\n",
    "    hidden_size: int\n",
    "    max_length: int\n",
    "    type_vocab_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.word_embeddings = nn.Embed(self.vocab_size, self.hidden_size)\n",
    "        self.position_embeddings = nn.Embed(self.max_length, self.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embed(self.type_vocab_size, self.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm()\n",
    "        self.dropout = nn.Dropout(0.1, deterministic=False)\n",
    "\n",
    "    def __call__(self, input_ids, token_type_ids):\n",
    "        seq_length = input_ids.shape[1]\n",
    "        position_ids = jnp.arange(seq_length)[None, :]\n",
    "        word_embeddings = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "\n",
    "        embeddings = word_embeddings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Module):\n",
    "    num_heads: int\n",
    "    hidden_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        assert self.hidden_size % self.num_heads == 0\n",
    "        self.attention_head_size = int(self.hidden_size / self.num_heads)\n",
    "        self.all_head_size = self.num_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Dense(self.all_head_size)\n",
    "        self.key = nn.Dense(self.all_head_size)\n",
    "        self.value = nn.Dense(self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1, deterministic=False)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.shape[:-1] + (self.num_heads, self.attention_head_size)\n",
    "        x = x.reshape(new_x_shape)\n",
    "        return x.transpose((0, 2, 1, 3))\n",
    "\n",
    "    def __call__(self, hidden_states, attention_mask):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        attention_scores = jnp.einsum(\"...qhd,...khd->...hqk\", query_layer, key_layer)\n",
    "        attention_scores = attention_scores / jnp.sqrt(self.attention_head_size)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask[:, :, None, None]\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        attention_probs = nn.softmax(attention_scores, axis=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = jnp.einsum(\"...hqk,...khd->...qhd\", attention_probs, value_layer)\n",
    "        context_layer = context_layer.transpose((0, 2, 1, 3))\n",
    "        new_context_layer_shape = context_layer.shape[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.reshape(new_context_layer_shape)\n",
    "        return context_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfOutput(nn.Module):\n",
    "    hidden_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.dense = nn.Dense(self.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm()\n",
    "        self.dropout = nn.Dropout(0.1, deterministic=False)\n",
    "\n",
    "    def __call__(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertAttention(nn.Module):\n",
    "    num_heads: int\n",
    "    hidden_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.self = BertSelfAttention(self.num_heads, self.hidden_size)\n",
    "        self.output = BertSelfOutput(self.hidden_size)\n",
    "\n",
    "    def __call__(self, hidden_states, attention_mask):\n",
    "        self_outputs = self.self(hidden_states, attention_mask)\n",
    "        attention_output = self.output(self_outputs, hidden_states)\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertIntermediate(nn.Module):\n",
    "    hidden_size: int\n",
    "    intermediate_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.dense = nn.Dense(self.intermediate_size)\n",
    "        self.intermediate_act_fn = nn.gelu\n",
    "\n",
    "    def __call__(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertOutput(nn.Module):\n",
    "    hidden_size: int\n",
    "    intermediate_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.dense = nn.Dense(self.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm()\n",
    "        self.dropout = nn.Dropout(0.1, deterministic=False)\n",
    "\n",
    "    def __call__(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    num_heads: int\n",
    "    hidden_size: int\n",
    "    intermediate_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.attention = BertAttention(self.num_heads, self.hidden_size)\n",
    "        self.intermediate = BertIntermediate(self.hidden_size, self.intermediate_size)\n",
    "        self.output = BertOutput(self.hidden_size, self.intermediate_size)\n",
    "\n",
    "    def __call__(self, hidden_states, attention_mask):\n",
    "        attention_output = self.attention(hidden_states, attention_mask)\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    hidden_size: int\n",
    "    num_heads: int\n",
    "    num_layers: int\n",
    "    intermediate_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.layers = [BertLayer(self.num_heads, self.hidden_size, self.intermediate_size) for _ in range(self.num_layers)]\n",
    "\n",
    "    def __call__(self, hidden_states, attention_mask):\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states, attention_mask)  # Simplified\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    hidden_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.dense = nn.Dense(self.hidden_size)\n",
    "        self.activation = nn.tanh\n",
    "\n",
    "    def __call__(self, hidden_states):\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForPreTraining(nn.Module):\n",
    "    vocab_size: int\n",
    "    hidden_size: int\n",
    "    max_length: int\n",
    "    num_heads: int\n",
    "    num_layers: int\n",
    "    intermediate_size: int\n",
    "    type_vocab_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.embeddings = BertEmbeddings(self.vocab_size, self.hidden_size, self.max_length, self.type_vocab_size)\n",
    "        self.encoder = BertEncoder(self.hidden_size, self.num_heads, self.num_layers, self.intermediate_size)\n",
    "        self.pooler = BertPooler(self.hidden_size)\n",
    "        self.cls = nn.Dense(self.vocab_size)\n",
    "        self.seq_relationship = nn.Dense(2)\n",
    "\n",
    "    def __call__(self, input_ids, attention_mask, token_type_ids):\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
    "        encoder_output = self.encoder(embedding_output, attention_mask)\n",
    "        pooled_output = self.pooler(encoder_output)\n",
    "        prediction_scores = self.cls(encoder_output)\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return prediction_scores, seq_relationship_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForPreTraining(nn.Module):\n",
    "    vocab_size: int\n",
    "    hidden_size: int\n",
    "    max_length: int\n",
    "    num_heads: int\n",
    "    num_layers: int\n",
    "    intermediate_size: int\n",
    "    type_vocab_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.embeddings = BertEmbeddings(self.vocab_size, self.hidden_size, self.max_length, self.type_vocab_size)\n",
    "        self.encoder = BertEncoder(self.hidden_size, self.num_heads, self.num_layers, self.intermediate_size)\n",
    "        self.pooler = BertPooler(self.hidden_size)\n",
    "        self.cls = nn.Dense(self.vocab_size)\n",
    "        self.seq_relationship = nn.Dense(2)\n",
    "\n",
    "    def __call__(self, input_ids, attention_mask, token_type_ids):\n",
    "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
    "        encoder_output = self.encoder(embedding_output, attention_mask)\n",
    "        pooled_output = self.pooler(encoder_output)\n",
    "        prediction_scores = self.cls(encoder_output)\n",
    "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
    "        return prediction_scores, seq_relationship_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_state(rng, model, learning_rate):\n",
    "    params = model.init(rng, jnp.ones((1, 512), jnp.int32), jnp.ones((1, 512), jnp.int32), jnp.zeros((1, 512), jnp.int32))\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss 함수 정의\n",
    "def compute_loss(prediction_scores, seq_relationship_score, masked_lm_labels, next_sentence_labels):\n",
    "    masked_lm_loss = optax.softmax_cross_entropy(prediction_scores, jax.nn.one_hot(masked_lm_labels, num_classes=30522))\n",
    "    next_sentence_loss = optax.softmax_cross_entropy(seq_relationship_score, jax.nn.one_hot(next_sentence_labels, num_classes=2))\n",
    "    return jnp.mean(masked_lm_loss) + jnp.mean(next_sentence_loss)\n",
    "\n",
    "# 평가 메트릭 정의\n",
    "def compute_metrics(prediction_scores, seq_relationship_score, masked_lm_labels, next_sentence_labels):\n",
    "    masked_lm_accuracy = jnp.mean(jnp.argmax(prediction_scores, -1) == masked_lm_labels)\n",
    "    next_sentence_accuracy = jnp.mean(jnp.argmax(seq_relationship_score, -1) == next_sentence_labels)\n",
    "    return {'masked_lm_accuracy': masked_lm_accuracy, 'next_sentence_accuracy': next_sentence_accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 루프 정의\n",
    "@jax.jit\n",
    "def train_step(state, batch, dropout_key):\n",
    "    dropout_train_key = jax.random.fold_in(key=dropout_key, data=state.step)\n",
    "    def loss_fn(params):\n",
    "        prediction_scores, seq_relationship_score = state.apply_fn(params, batch['input_ids'], batch['attention_mask'], batch['token_type_ids'], rngs={'dropout': dropout_train_key})\n",
    "        loss = compute_loss(prediction_scores, seq_relationship_score, batch['labels'], batch['next_sentence_label'])\n",
    "        return loss, (prediction_scores, seq_relationship_score)\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, (prediction_scores, seq_relationship_score)), grads = grad_fn(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics = compute_metrics(prediction_scores, seq_relationship_score, batch['labels'], batch['next_sentence_label'])\n",
    "    return state, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def eval_step(state, batch, dropout_key):\n",
    "    dropout_train_key = jax.random.fold_in(key=dropout_key, data=state.step)\n",
    "    prediction_scores, seq_relationship_score = state.apply_fn(state.params, batch['input_ids'], batch['attention_mask'], batch['token_type_ids'], rngs={'dropout': dropout_train_key})\n",
    "    return compute_metrics(prediction_scores, seq_relationship_score, batch['labels'], batch['next_sentence_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 3\n",
    "learning_rate = 1e-4\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "main_key, dropout_key = jax.random.split(rng)\n",
    "\n",
    "model = BertForPreTraining(\n",
    "    vocab_size=30522, hidden_size=768, max_length=512,\n",
    "    num_heads=12, num_layers=12, intermediate_size=3072,\n",
    "    type_vocab_size=2\n",
    ")\n",
    "state = create_train_state(main_key, model, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_numpy(batch):\n",
    "  return jax.tree_util.tree_map(lambda x: x.numpy(), batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Metrics: {'masked_lm_accuracy': Array(0.13867188, dtype=float32), 'next_sentence_accuracy': Array(1., dtype=float32)}\n",
      "Epoch 1, Eval Metrics: {'masked_lm_accuracy': np.float32(0.13005371), 'next_sentence_accuracy': np.float32(0.575)}\n",
      "Epoch 2, Train Metrics: {'masked_lm_accuracy': Array(0.15917969, dtype=float32), 'next_sentence_accuracy': Array(0.25, dtype=float32)}\n",
      "Epoch 2, Eval Metrics: {'masked_lm_accuracy': np.float32(0.13261719), 'next_sentence_accuracy': np.float32(0.575)}\n",
      "Epoch 3, Train Metrics: {'masked_lm_accuracy': Array(0.15332031, dtype=float32), 'next_sentence_accuracy': Array(0.75, dtype=float32)}\n",
      "Epoch 3, Eval Metrics: {'masked_lm_accuracy': np.float32(0.13398437), 'next_sentence_accuracy': np.float32(0.4875)}\n"
     ]
    }
   ],
   "source": [
    "# 학습 루프\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    for batch in train_loader:\n",
    "        batch = batch_to_numpy(batch)\n",
    "        state, train_metrics = train_step(state, batch, dropout_key)\n",
    "    print(f\"Epoch {epoch + 1}, Train Metrics: {train_metrics}\")\n",
    "\n",
    "    # Evaluation\n",
    "    eval_metrics = []\n",
    "    for batch in test_loader:\n",
    "        batch = batch_to_numpy(batch)\n",
    "        metrics = eval_step(state, batch, dropout_key)\n",
    "        eval_metrics.append(metrics)\n",
    "    eval_metrics = jax.device_get(eval_metrics)\n",
    "    eval_metrics = {k: np.mean([metrics[k] for metrics in eval_metrics]) for k in eval_metrics[0].keys()}\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Eval Metrics: {eval_metrics}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
